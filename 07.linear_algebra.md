---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

````{admonition} Announcements & Reminders
:class: note
:class: dropdown

1. Quiz 2

```{figure} _static/misc/quiz2.png
:width: 80%

Distribution of results for Quiz 2
```

2. The midterm is scheduled at **18:30 to 19:45** (1 hour + 15 min reading time) on **Monday, April 15** in **Melville Hall**

3. Practice problems for the midterm [to be published](ps_practice_midterm)

````

% ```
% Linear algebra I : 
% - vector spaces, 
% - linear operators, 
% - eigenvalues and eigenvectors, 
% - change of bases,
% - determinants?
% 
% 3blue1brown:
% 1. vectors and vector spaces
% 2. linear combinations, span and basis vectors
% 3. linear transformations and matrices
% 4. matrix multiplication as compositions
% 5. inverse matrices, column spaces and null space
% 6. non-square matrices as transformations between dimensions
% 7. dot products
% 8. cross products, also in light of linear transformations
% 9. Cramer's rule
% 10. Chance of bases
% 11. Eigenvectors and eigenvalues
% 12. Computing eigenvalues and eigenvectors
% 13. Abstract vector spaces
% 
% ```

# üìñ Elements of linear algebra

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>

> **Important note**
>
> This lecture covers the fundamental concepts of linear algebra, and covers more material than the time permits to cover in a single lecture
> - It is strongly advised to study the details of this lecture note is greater details in preparation for the midterm and final exams
> - I also encourage you to watch some or all of the videos in the [3Blue1Brown: Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) series at your own time to get deeper understanding of the linear algebra concepts through visualizations and excellent explanations
> 
> The lecture covers the following main concepts:
> 1. Vector space
> 2. Linear combination
> 3. Span
> 4. Linear independence
> 5. Basis and dimension
> 6. Matrices as linear transformations
> 7. Column spaces and null space
> 8. Rank of a matrix
> 9. Singular matrices
> 10. Matrix determinant
> 11. Eigenvectors and eigenvalues
> 12. Diagonalization of matrices
>
> See references [in the bottom of this note](references_reading)
>
> Midterm and final exam may have some but not much of the material from this lecture because it is mainly left for self-study. Open book quiz, however, will contain questions for this material.



## Motivating example

Linear algebra is most useful in economics for solving systems of linear equations

```{admonition} Definition
:class: caution

A general system of linear equations can be written as
%
$$
\begin{array}{c}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1K} x_K = b_1 \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2K} x_K = b_2 \\
\vdots \\
a_{N1} x_1 + a_{N2} x_2 + \cdots + a_{NK} x_K = b_N 
\end{array}
$$
%
```

More often we write the system in **matrix form**

$$
\left(
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1K} \\
a_{21} & a_{22} & \cdots & a_{2K} \\
\vdots & \vdots & & \vdots \\
a_{N1} & a_{N2} & \cdots & a_{NK} 
\end{array}
\right)
\left(
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_K
\end{array}
\right)
=
\left(
\begin{array}{c}
b_1 \\
b_2 \\
\vdots \\
b_K
\end{array}
\right)
%
$$

$$
%
A x = b
%
$$


Let's solve this system on a computer:

```{code-cell} python3
import numpy as np
from scipy.linalg import solve
A = [[0, 2, 4],
     [1, 4, 8],
     [0, 3, 7]]
b = (1, 2, 0)
A, b = np.asarray(A), np.asarray(b)
x=solve(A, b)
print(f'Solutions is x={x}')
```

This tells us that the solution is $x = (0. , 3.5, -1.5)$

That is, 
%
$$
%
x =
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
=
\left(
\begin{array}{c}
0 \\
3.5 \\
-1.5
\end{array}
\right)
%
$$
%

The usual question: if computers can do it so easily --- what do we need to study for?

But now let's try this similar looking problem

**Question:** what changed?

```{code-cell} python3
import numpy as np
from scipy.linalg import solve
A = [[0, 2, 4],
     [1, 4, 8],
     [0, 3, 6]]
b = (1, 2, 0)
A, b = np.asarray(A), np.asarray(b)
x=solve(A, b)
print(f'Solutions is x={x}')
```

This is the output that we get: 
`LinAlgWarning: Ill-conditioned matrix`

- What does this mean? 
- How can we fix it?

We still need to understand the concepts after all!




## Vector Space

```{admonition} Definition
:class: caution

A field (of numbers) is a set $\mathbb{F}$ of numbers with the property that if $a,b \in \mathbb{F}$, then $a+b$, $a-b$, $ab$ and $a/b$ (provided that $b \ne 0$) are also in $\mathbb{F}$.
```

```{admonition} Example
:class: tip

$\mathbb{Q}$ and $\mathbb{R}$ are fields, but $\mathbb{N}$, $\mathbb{Z}$ is not (why?)
```


```{admonition} Definition
:class: caution

A *vector space* over a field $\mathbb{F}$ is a set $V$ of objects called vectors, together with two operations:
1. vector addition that takes two vectors $v,w \in V$ and produces a vector $v+w \in V$, and
2. scalar multiplication that takes a scalar $\alpha \in \mathbb{F}$ and a vector $v \in V$ and produces a vector $\alpha v \in V$,

which satisfy the following properties:

1. Associativity of vector addition: $(u+v)+w = u+ (v+w)$ for all $u,v,w \in V$
2. Existance of zero vector ${\bf 0}$ such that $v + {\bf 0} = v$ for all $v \in V$
3. Existance of negatives: for each $v \in V$ there exists a vector $-v \in V$ such that $v + (-v) = {\bf 0}$
4. Associativity of multiplication: $\alpha (\beta v) = (\alpha \beta) v$ for all $\alpha, \beta \in \mathbb{F}$ and $v \in V$
5. Distributivity: $(\alpha + \beta) v = \alpha v + \beta v$ and $\alpha(v+w) = \alpha v + \alpha w$ for all $\alpha, \beta \in \mathbb{F}$ and $v, w \in V$
6. Unitarity: $1u = u$ for all $u \in V$

```

- may seem like a bunch of abstract nonsense, but it's actually describes something intuitive
- what we already know about operations with vectors in $\mathbb{R}^N$ put in formal definition


```{admonition} Example
:class: tip

%
$$
%
\left(
\begin{array}{c}
1 \\
2 \\
3 \\
4
\end{array}
\right)
+
\left(
\begin{array}{c}
2 \\
4 \\
6 \\
8
\end{array}
\right)
=
\left(
\begin{array}{c}
3 \\
6 \\
9 \\
12
\end{array}
\right)
%
$$
%

```

````{admonition} Example
:class: tip


%
$$
%
-1
\left(
\begin{array}{c}
1 \\
2 \\
3 \\
4
\end{array}
\right)
=
\left(
\begin{array}{c}
-1 \\
-2 \\
-3 \\
-4
\end{array}
\right)
%
$$
%


```{figure} _static/plots/vec_scalar.png
:width: 50%
:name: f:vec_scalar
:width: 50%

Scalar multiplication by a negative number
```
````

### Dot product and norm

```{admonition} Definition
:class: caution

The **dot product** of two vectors $x, y \in \mathbb{R}^N$ is
%
$$
%
x \cdot y = x^T y =
\sum_{n=1}^N x_n y_n
%
$$
%

```

The notation $\square^T$ flips the vector from column-vector to row-vector, this will make sense when we talk later about matrices for which vectors are special case.


```{admonition} Fact: Properties of dot product
:class: important

For any $\alpha, \beta \in \mathbb{R}$ and any $x, y \in \mathbb{R}^N$, the following statements are true:
%
1. $x^T y = y^T x$
2. $(\alpha x)' (\beta y) = \alpha \beta (x^T y)$
9. $x^T (y + z) = x^T y + x^T z$
%
```


```{admonition} Definition
:class: caution

The (Euclidean) **norm** of $x \in \mathbb{R}^N$ is defined as
%
$$
%
\| x \| 
= \sqrt{x^T x } 
= \left( \sum_{n=1}^N x_n^2 \right)^{1/2}
%
$$
%

```

- $\| x \|$ represents the ``length'' of $x$
- $\| x - y \|$ represents distance between $x$ and $y$

```{admonition} Fact
:class: important

For any $\alpha \in \mathbb{R}$ and any $x, y \in \mathbb{R}^N$, the following statements are true:
%
1. $\| x \| \geq 0$ and $\| x \| = 0$ if and only if
$x = 0$

- $\| \alpha x \| = |\alpha| \| x \|$

- $\| x + y \| \leq \| x \| + \| y \|$
(**triangle inequality**)

- $| x^T y | \leq \| x \| \| y \|$
(**Cauchy-Schwarz inequality**)
```

## Linear combinations

```{admonition} Definition
:class: caution

A **linear combination** of vectors $x_1,\ldots, x_K$ in $\mathbb{R}^N$ 
is a vector 
%
$$
%
y = \sum_{k=1}^K \alpha_k x_k 
= \alpha_1 x_1 + \cdots + \alpha_K x_K 
%
$$
%
where $\alpha_1,\ldots, \alpha_K$ are scalars

```

```{admonition} Example
:class: tip

%
$$
%
0.5 \left(
\begin{array}{c}
6.0 \\
2.0 \\
8.0
\end{array}
\right)
+
3.0 \left(
\begin{array}{c}
0 \\
1.0 \\
-1.0
\end{array}
\right)
=
\left(
\begin{array}{c}
3.0 \\
4.0 \\
1.0
\end{array}
\right)
%
$$
%

```

```{admonition} Fact
:class: important

Inner products of linear combinations satisfy 
%
$$
%
\left(
\sum_{k=1}^K \alpha_k x_k
\right)' 
\left(
\sum_{j=1}^J \beta_j y_j 
\right)
=
\sum_{k=1}^K \sum_{j=1}^J \alpha_k \beta_j x_k' y_j 
%
$$
%
```

````{admonition} Proof
:class: dropdown

Follows from the properties of the dot product after some simple algebra
````


## Span

Let $X \subset \mathbb{R}^N$ be any nonempty set (of points in $\mathbb{R}^N$, i.e. vectors)

```{admonition} Definition
:class: caution

Set of all possible linear combinations of elements of $X$ is 
called the **span** of $X$, denoted by $\mathrm{span}(X)$

```

For finite $X = \{x_1,\ldots, x_K\}$ the span can be expressed
as 
%
$$
\mathrm{span}(X)= \left\{ \text{ all } \sum_{k=1}^K \alpha_k x_k 
\text{ such that }
(\alpha_1,\ldots, \alpha_K) \in \mathbb{R}^K \right\}
$$
%


```{admonition} Example
:class: tip

Let's start with the span of a singleton

Let $X = \{ (1,1) \} \subset \mathbb{R}^2$

The span of $X$ is all vectors of the form 

%
$$
%
\alpha 1 
=
\left(
\begin{array}{c}
\alpha \\
\alpha
\end{array}
\right)
\quad \text{ with } \quad \alpha \in \mathbb{R} 
%
$$
%

Constitutes a line in the plane that passes through

- the vector $(1,1)$ (set $\alpha = 1$)
- the origin $(0,0)$ (set $\alpha = 0$)

```

```{figure} _static/plots/span_of_one_vec.png
:width: 50%


The span of $(1,1)$ in $\mathbb{R}^2$
```

```{admonition} Example
:class: tip

Let $x_1 = (3, 4, 2)$ and let $x_2 = (3, -4, 0.4)$

By definition, the span is all vectors of the form

%
$$
%
y = 
\alpha \left(
\begin{array}{c}
3 \\
4 \\
2
\end{array}
\right)
+
\beta \left(
\begin{array}{c}
3 \\
-4 \\
0.4
\end{array}
\right)
\quad \text{where} \quad
\alpha, \beta \in \mathbb{R}
%
$$
%

It turns out to be a plane that passes through

- the vector $x_1$
- the vector $x_2$
- the origin $0$

```

```{figure} _static/plots/span_plane.png
:width: 80%
:name: f:span_plane

Span of $x_1, x_2$
```


```{admonition} Definition
:class: caution

Consider the vectors $\{e_1, \ldots, e_N\} \subset \mathbb{R}^N$, where

%
$$
%
e_1 = 
\left(
\begin{array}{c}
1 \\
0 \\
\vdots \\
0
\end{array}
\right),
\quad 
e_2 = 
\left(
\begin{array}{c}
0 \\
1 \\
\vdots \\
0
\end{array}
\right),
\; 
\cdots,
\;
e_N = 
\left(
\begin{array}{c}
0 \\
0 \\
\vdots \\
1
\end{array}
\right)
%
$$
%

That is, $e_n$ has all zeros except for a $1$ as the $n$-th element

Vectors $e_1, \ldots, e_N$ are called the **canonical basis vectors** of $\mathbb{R}^N$

```

```{figure} _static/plots/vec_canon.png
:width: 50%
:name: f:vec_canon

Canonical basis vectors in $\mathbb{R}^2$
```

```{admonition} Fact
:class: important

The span of $\{e_1, \ldots, e_N\}$ is equal to all of $\mathbb{R}^N$
```

````{admonition} Proof
:class: dropdown

Proof for $N=2$: 

Pick any $y \in \mathbb{R}^2$ 

We have
%
$$
%
y 
=
\left(
\begin{array}{c}
y_1 \\
y_2
\end{array}
\right)
=
\left(
\begin{array}{c}
y_1 \\
0
\end{array}
\right)
+
\left(
\begin{array}{c}
0 \\
y_1
\end{array}
\right)
\\
=
y_1
\left(
\begin{array}{c}
1 \\
0
\end{array}
\right)
+
y_2
\left(
\begin{array}{c}
0 \\
1
\end{array}
\right)
= y_1 e_1 + y_2 e_2
%
$$
%
Thus, $y \in \mathrm{span} \{e_1, e_2\}$ 

Since $y$ arbitrary, we have shown that $\mathrm{span} \{e_1,
e_2\} = \mathbb{R}^2$

````

```{figure} _static/plots/vec_canon_x.png
:width: 50%
:name: f:vec_canon2

Canonical basis vectors in $\mathbb{R}^2$
```

### Linear subspace

```{admonition} Definition
:class: caution

A nonempty $S \subset \mathbb{R}^N$ called a **linear
subspace** of $\mathbb{R}^N$ if
%
$$
%
x, y \in S \; \text{ and } \;\alpha, \beta \in \mathbb{R}
\quad \implies \quad
\alpha x + \beta y \in S 
%
$$
%

```

In other words, $S \subset \mathbb{R}^N$ is "closed" under vector addition
and scalar multiplication

Note: Sometimes we just say **subspace** and drop "linear"

```{admonition} Example
:class: tip

$\mathbb{R}^N$ itself is a linear subspace of $\mathbb{R}^N$
```

```{admonition} Fact
:class: important

Fix $a \in \mathbb{R}^N$ and let $A = \{ x \in \mathbb{R}^N \colon a 'x = 0 \}$

The set $A$ is a linear subspace of $\mathbb{R}^N$
```

````{admonition} Proof
:class: dropdown

Let $x, y \in A$ and let $\alpha, \beta \in \mathbb{R}$

We must show that $z = \alpha x + \beta y \in A$ 

Equivalently, that $a^T z = 0$

True because
%
$$
%
a^T z =
a^T (\alpha x + \beta y) = \alpha
a^T x + \beta a^T y = 0 + 0 = 0
%
$$
%

````

## Linear independence

```{admonition} Definition
:class: caution

A nonempty collection of vectors $X = \{x_1,\ldots, x_K\}
\subset \mathbb{R}^N$ is called **linearly independent** if
%
$$
%
\sum_{k=1}^K \alpha_k x_k
= 0 
\; \implies \;
\alpha_1 = \cdots = \alpha_K = 0
%
$$
%

```

- linear independence of a set of vectors determines how large
a space they span
- loosely speaking, linearly independent sets span large spaces

```{admonition} Example
:class: tip

Let $x = (1, 2)$ and $y = (-5, 3)$

The set $X = \{x, y\}$ is linearly independent in $\mathbb{R}^2$

Indeed, suppose $\alpha_1$ and $\alpha_2$ are scalars with

%
$$
%
\alpha_1
\left(
\begin{array}{c}
1 \\
2
\end{array}
\right)
+ 
\alpha_2
\left(
\begin{array}{c}
-5 \\
3
\end{array}
\right)
=
0
%
$$
%
Equivalently,
%
$$
%
\alpha_1 = 5 \alpha_2
\\
2 \alpha_1 = -3 \alpha_2
%
$$
%

Then $2(5\alpha_2) = 10 \alpha_2 = -3 \alpha_2$, implying $\alpha_2 = 0$
and hence $\alpha_1 = 0$ 
```


```{admonition} Fact
:class: important

The set of canonical basis vectors $\{e_1, \ldots, e_N\}$
is linearly independent in $\mathbb{R}^N$
```

````{admonition} Proof
:class: dropdown

Let $\alpha_1, \ldots, \alpha_N$ be coefficients such that
$\sum_{k=1}^N \alpha_k e_k = 0$

Then
%
$$
\left(
\begin{array}{c}
0 \\
0 \\
\vdots \\
0
\end{array}
\right)
=
\big(
\alpha_1, \alpha_2, \dots, \alpha_N
\big)
\left(
\begin{array}{cccc}
1,&0,&\dots&0\\
0,&1,&\dots&0\\
\vdots&\vdots&\ddots&\vdots\\
0,&0,&\dots&1
\end{array}
\right)
=
\left(
\begin{array}{c}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\alpha_N
\end{array}
\right)
%
$$

In particular, $\alpha_k =  $ for all $k$

Hence $\{e_1, \ldots, e_N\}$ linearly independent

$\blacksquare$
````

```{admonition} Fact
:class: important

Take $X = \{x_1,\ldots, x_K\} \subset \mathbb{R}^N$.
For $K > 1$ all of following statements are equivalent
%
1. $X$ is linearly independent

2. No $x_i \in X$ can be written as linear combination of the others

9. $X_0 \subset X \implies \mathrm{span}(X_0) \subset \mathrm{span}(X)$
%
```


````{admonition} Example
:class: tip

As another visual example of linear independence, consider the pair

$$
%
x_1 =
\begin{pmatrix}
3 \\
4 \\
2
\end{pmatrix}
\quad \text{and} \quad
x_2 =
\begin{pmatrix}
3 \\
-4 \\
1
\end{pmatrix}
%
$$
%

The span of this pair is a plane in $\mathbb{R}^3$

But if we drop either one the span reduces to a line

```{figure} _static/plots/nonredundant1.png
:width: 50%
:name: 

The span of $\{x_1, x_2\}$ is a plane
```

```{figure} _static/plots/nonredundant2.png
:width: 50%
:name: 

The span of $\{x_1\}$ alone is a line
```

```{figure} _static/plots/nonredundant3.png
:width: 50%
:name: 

The span of $\{x_2\}$ alone is a line
```
````

```{admonition} Fact
:class: important

If $X$ is linearly independent, then $X$ does not contain $0$
```

````{admonition} Proof
:class: dropdown

Exercise
````

```{admonition} Fact
:class: important

If $X$ is linearly independent, then every subset of $X$ is linearly independent 
```

````{admonition} Proof
:class: dropdown

Sketch of proof: Suppose for example that $\{x_1,\ldots,
x_{K-1}\} \subset X$ is linearly dependent

Then $\exists \; \alpha_1, \ldots, \alpha_{K-1}$ not all zero with 
$\sum_{k=1}^{K-1} \alpha_k x_k = 0$

Setting $\alpha_K =0$ we can write this as $\sum_{k=1}^K \alpha_k x_k = 0$

Not all scalars zero so contradicts linear independence of $X$ 

````

```{admonition} Fact
:class: important

If $X= \{x_1,\ldots, x_K\} \subset \mathbb{R}^N$ is linearly independent and $z$ is an $N$-vector not in $\mathrm{span}(X)$, then $X \cup \{ z \}$ is linearly independent 
```

````{admonition} Proof
:class: dropdown

Suppose to the contrary that $X \cup \{ z \}$ is linearly
dependent:
%
$$
%
\exists \; \alpha_1, \ldots, \alpha_K, \beta
\text{ not all zero with }
\sum_{k=1}^K \alpha_k x_k + \beta z = 0
%
$$ (eq:m)
%

If $\beta=0$, then by {eq}`eq:m` we have $\sum_{k=1}^K \alpha_k x_k = 0$ and 
$\alpha_k \ne 0$ for some $k$, a contradiction

If $\beta \ne0$, then by {eq}`eq:m` we have 
%
$$
%
z = \sum_{k=1}^K \frac{-\alpha_k}{\beta} x_k 
%
$$
%
Hence $z \in \mathrm{span}(X)$ --- contradiction

````

```{admonition} Fact
:class: important

If $X = \{x_1,\ldots, x_K\} \subset \mathbb{R}^N$ is linearly independent and
$y \in \mathbb{R}^N$, then there is at most one set of scalars $\alpha_1,\ldots,\alpha_K$ such
that $y = \sum_{k=1}^K \alpha_k x_k$
```

````{admonition} Proof
:class: dropdown

Suppose there are two such sets of scalars

That is,
%
$$
%
\exists \;
\alpha_1, \ldots, \alpha_K
\text{ and } \beta_1, \ldots, \beta_K
\; \text{ such that } \; 
y 
= \sum_{k=1}^K \alpha_k x_k
= \sum_{k=1}^K \beta_k x_k
%
$$
%
$$
%
\implies \sum_{k=1}^K (\alpha_k - \beta_k) x_k = 0
%
$$ 
%

%
$$
%
\implies
\alpha_k = \beta_k 
\quad \text{for all} \quad k
%
$$
%

````

Here's one of the most fundamental results in linear algebra

```{admonition} Exchange Lemma
:class: important

Let $S$ be a linear subspace of $\mathbb{R}^N$, and be spanned by $K$ vectors. \
Then any linearly independent subset of $S$ has at most $K$
vectors
```

````{admonition} Proof
:class: dropdown

Omitted

````

````{admonition} Example
:class: tip

If $X = \{x_1, x_2, x_3\} \subset \mathbb{R}^2$,
then $X$ is linearly dependent

- because $\mathbb{R}^2$ is spanned by the two vectors $e_1, e_2$

```{figure} _static/plots/vecs.png
:width: 50%

Must be linearly dependent
```
````

```{admonition} Fact
:class: important

Let $X = \{ x_1, \ldots, x_N \}$ be any $N$ vectors in $\mathbb{R}^N$

$\mathrm{span}(X) = \mathbb{R}^N$ if and only if $X$ is linearly independent
```

```{admonition} Example
:class: tip

The vectors $x = (1, 2)$ and $y = (-5, 3)$ span $\mathbb{R}^2$
```

````{admonition} Proof
:class: dropdown

Let prove that 

$X= \{ x_1, \ldots, x_N \}$ linearly independent $\implies$ $\mathrm{span}(X) = \mathbb{R}^N$

Seeking a contradiction, suppose that 

1. $X $ is linearly independent 
9. and yet $\exists \, z \in \mathbb{R}^N$ with $z \notin \mathrm{span}(X)$ 

But then $X \cup \{z\} \subset \mathbb{R}^N$ is linearly independent (why?)

This set has $N+1$ elements 

And yet $\mathbb{R}^N$ is spanned by the $N$ canonical basis vectors

Contradiction (of what?)

Next let's show the converse

$\mathrm{span}(X) = \mathbb{R}^N$
$\implies$ 
$X= \{ x_1, \ldots, x_N \}$ linearly independent

Seeking a contradiction, suppose that 

1. $\mathrm{span}(X) = \mathbb{R}^N$ 
9. and yet $X$ is linearly dependent 

Since $X$ not independent, $\exists X_0 \subsetneq X$ with $\mathrm{span}(X_0) =
\mathrm{span}(X)$

But by 1 this implies that $\mathbb{R}^N$ is spanned by $K < N$ vectors

But then the $N$ canonical basis vectors must be linearly dependent

Contradiction 

````

## Bases and dimension

```{admonition} Definition
:class: caution

Let $S$ be a linear subspace of $\mathbb{R}^N$ 

A set of vectors $B = \{b_1, \ldots, b_K\} \subset S$ is
called a **basis of $S$** if
%
1. $B$ is linearly independent
9. $\mathrm{span}(B) = S$

```

```{admonition} Example
:class: tip

Canonical basis vectors form a basis of $\mathbb{R}^N$
```

````{admonition} Example
:class: tip

Recall the plane
%
$$
%
P = \{ (x_1, x_2, 0) \in \mathbb{R}^3 \colon x_1, x_2 \in \mathbb{R \}}
%
$$
%

We showed before that $\mathrm{span}\{e_1, e_2\} = P$ for 

%
$$
%
e_1 = 
\left(
\begin{array}{c}
1 \\
0 \\
0
\end{array}
\right),
\quad 
e_2 = 
\left(
\begin{array}{c}
0 \\
1 \\
0
\end{array}
\right)
%
$$
%

Moreover, $\{e_1, e_2\}$ is linearly independent (why?)

Hence $\{e_1, e_2\}$ is a basis for $P$

```{figure} _static/plots/flat_plane_e_vecs.png
:width: 50%
:name: 

The pair $\{e_1, e_2\}$ form a basis for $P$
```
`````

What are the implications of $B$ being a basis of $S$?

- every element of $S$ can be represented uniquely from the smaller set $B$

In more detail:

- $B$ spans $S$ and, by linear independence, every element is needed to span $S$ --- a "minimal" spanning set

- Since $B$ spans $S$, every $y$ in $S$ can be represented as
a linear combination of the basis vectors

- By independence, this representation is unique


```{admonition} Fact
:class: important

If $B \subset \mathbb{R}^N$ is linearly independent, then $B$ is a basis of $\mathrm{span}(B)$

```

````{admonition} Proof
:class: dropdown

Follows from the definitions
````


```{admonition} Fact: Fundamental Properties of Bases
:class: important

If $S$ is a linear subspace of $\mathbb{R}^N$ distinct from $\{0\}$, then 
%
1. $S$ has at least one basis, and
9. every basis of $S$ has the same number of elements
```

````{admonition} Proof
:class: dropdown

Proof of part 2: Let $B_i$ be a basis of $S$ with $K_i$ elements, $i=1, 2$

By definition, $B_2$ is a linearly independent subset of $S$

Moreover, $S$ is spanned by the set $B_1$, which has $K_1$ elements

Hence $K_2 \leq K_1$ 

Reversing the roles of $B_1$ and $B_2$ gives $K_1 \leq K_2$

````

```{admonition} Definition
:class: caution

Let $S$ be a linear subspace of $\mathbb{R}^N$

We now know that every basis of $S$ has the same number of elements

This common number is called the **dimension** of $S$

```

```{admonition} Example
:class: tip

$\mathbb{R}^N$ is $N$ dimensional because the $N$ canonical basis vectors form a basis
```

```{admonition} Example
:class: tip

$P = \{ (x_1, x_2, 0) \in \mathbb{R}^3 \colon x_1, x_2 \in \mathbb{R \}}$ is
two dimensional because the first two canonical basis vectors of $\mathbb{R}^3$ form a basis 
```










## Linear Mappings

- Linear functions are in one-to-one correspondence with matrices
- Nonlinear functions are closely connected to linear functions through Taylor approximation


```{admonition} Definition
:class: caution

A function $T \colon \mathbb{R}^K \to \mathbb{R}^N$ is called
**linear** if 
%
$$
%
T(\alpha x + \beta y) = \alpha Tx + \beta Ty
\qquad
\forall \, 
x, y \in \mathbb{R}^K, \;
\forall \,
\alpha, \beta \in \mathbb{R}
%
$$
%

```

- Linear functions often written with upper case letters

- Typically omit parenthesis around arguments when convenient

```{admonition} Example
:class: tip

$T \colon \mathbb{R} \to \mathbb{R}$ defined by $Tx = 2x$ is linear 
```

````{admonition} Proof
:class: dropdown

Take any $\alpha, \beta, x, y$ in $\mathbb{R}$ and observe that
%
$$
%
T(\alpha x + \beta y)
= 2(\alpha x + \beta y)
= \alpha 2 x + \beta 2 y
= \alpha Tx + \beta Ty 
%
$$
%

````

```{admonition} Example
:class: tip

The function $f \colon \mathbb{R} \to \mathbb{R}$ defined by $f(x) = x^2$ is
**non**linear
```

````{admonition} Proof
:class: dropdown

Set $\alpha = \beta = x = y = 1$

Then 
%
- $f(\alpha x + \beta y) = f(2) = 4$
- But $\alpha f(x) + \beta f(y) = 1 + 1 = 2$

````

```{admonition} Example
:class: tip

Given constants $c_1$ and $c_2$, the 
function $T \colon \mathbb{R}^2 \to \mathbb{R}$ defined by 
%
$$
%
T x = T (x_1, x_2) = c_1 x_1 + c_2 x_2 
%
$$
%
is linear 

```

````{admonition} Proof
:class: dropdown

If we take any $\alpha, \beta$ in $\mathbb{R}$ and
$x, y$ in $\mathbb{R}^2$, then 
%
$$
%
T(\alpha x + \beta y)
= c_1 [\alpha x_1 + \beta y_1] + c_2 [\alpha x_2 + \beta y_2]
\\
= \alpha [c_1 x_1 + c_2 x_2] + \beta [c_1 y_1 + c_2 y_2]
\\
= \alpha T x + \beta T y 
%
$$
%

````

```{figure} _static/plots/linfunc.png
:width: 75%
:name: 

The graph of $T x = c_1 x_1 + c_2 x_2$ is a plane
through the origin
```

```{note}

Thinking of linear functions as those whose graph is a straight line is not correct!
```


```{admonition} Example
:class: tip

Function $f \colon \mathbb{R} \to \mathbb{R}$ defined by $f(x) = 1 + 2x$ is **nonlinear**
```

````{admonition} Proof
:class: dropdown

Take $\alpha = \beta = x = y = 1$

Then 

- $f(\alpha x + \beta y) = f(2) = 5$
- But $\alpha f(x) + \beta f(y) = 3 + 3 = 6$

````

This kind of function is called an **affine** function

```{admonition} Fact
:class: important

If $T \colon \mathbb{R}^K \to \mathbb{R}^N$ is a linear mapping and $x_1,\ldots, x_J$ are vectors in $\mathbb{R}^K$, then for any linear combination we have
%
$$
T
\left[ \alpha_1 x_1 + \cdots + \alpha_J x_J \right]
= \alpha_1 T x_1 + \cdots + \alpha_J T x_J
$$
%
```

````{admonition} Proof
:class: dropdown

Proof for $J=3$: Applying the def of linearity twice, 
%
$$
%
T
\left[ \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 \right]
= T\left[ (\alpha_1 x_1 + \alpha_2 x_2) + \alpha_3 x_3 \right]
\\
= T\left[ \alpha_1 x_1 + \alpha_2 x_2 \right] + \alpha_3 T x_3 
\\
= \alpha_1 T x_1 + \alpha_2 T x_2 + \alpha_3 T x_3 
%
$$
%

````

```{admonition} Fact
:class: important

If $T \colon \mathbb{R}^K \to \mathbb{R}^N$ is a linear mapping, then 
%
$$
%
\mathrm{rng}(T) = \mathrm{span}(V) 
\quad \text{where} \quad
V = \{Te_1, \ldots, Te_K\}
%
$$
%

Here $e_k$ is the $k$-th canonical basis vector in $\mathbb{R}^K$
```
- note that $V = \{Te_1, \ldots, Te_K\}$ is the set of columns of the matrix representation of $T$

````{admonition} Proof
:class: dropdown

Any $x \in \mathbb{R}^K$ can be expressed as $\sum_{k=1}^K \alpha_k e_k$

Hence $\mathrm{rng}(T)$ is the set of all points of the form
%
$$
%
Tx
= T \left[ \sum_{k=1}^K \alpha_k e_k \right]
= \sum_{k=1}^K \alpha_k T e_k 
%
$$
%
as we vary $\alpha_1, \ldots, \alpha_K$ over all combinations

This coincides with the definition of $\mathrm{span}(V)$

````

```{admonition} Example
:class: tip

Let $T \colon \mathbb{R}^2 \to \mathbb{R}^2$ be defined by 
%
$$
%
Tx 
=
T(x_1, x_2)
=
x_1 
\begin{pmatrix}
1 \\
2
\end{pmatrix}
+
x_2 
\begin{pmatrix}
0 \\
-2
\end{pmatrix}
%
$$
%

Then 
%
$$
%
Te_1
=
\begin{pmatrix}
1 \\
2
\end{pmatrix}
\quad \text{and} \quad
Te_2
=
\begin{pmatrix}
0 \\
-2
\end{pmatrix}
%
$$
%

**Exercise:** Show that $V = \{Te_1, Te_2\}$ is linearly independent

We conclude that the range of $T$ is all of $\mathbb{R}^2$ (why?)

```

```{admonition} Definition
:class: caution

The **null space** or **kernel** of linear mapping $T \colon \mathbb{R}^K \to
\mathbb{R}^N$ is
%
$$
%
\mathrm{kernel}(T) = \{ x \in \mathbb{R}^K \colon Tx = 0\}
%
$$
%

```


### Linearity and Bijections

Recall that an arbitrary function can be 

- one-to-one (injections)
- onto (surjections)
- both (bijections)
- neither 

For linear functions from $\mathbb{R}^N$ to $\mathbb{R}^N$, the first three are all
equivalent!

In particular, 
%
$$
%
\text{onto } \iff \text{ one-to-one } \iff \text{ bijection}
%
$$
%

```{admonition} Fact
:class: important

If $T$ is a linear function from $\mathbb{R}^N$ to $\mathbb{R}^N$ then all of the following are equivalent:
%
1. $T$ is a bijection
2. $T$ is onto/surjective
3. $T$ is one-to-one/injective
4. $\mathrm{kernel}(T) = \{ 0 \}$
5. The set of vectors $V = \{Te_1, \ldots, Te_N\}$ is linearly independent
```

```{admonition} Definition
:class: caution

If any one of the above equivalent conditions is true, then $T$ is called **nonsingular**

```


````{admonition} Proof
:class: dropdown

Proof that $T$ onto $\iff$ $V = \{Te_1, \ldots, Te_N\}$ is
linearly independent

Recall that for any linear mapping $T$ we have $\mathrm{rng}(T) = \mathrm{span}(V)$

Using this fact and the definitions,
%
$$
%
T \text{ is onto/surjective } 
\iff \mathrm{rng}(T) = \mathbb{R}^N
\\
\iff \mathrm{span}(V) = \mathbb{R}^N
\\
\iff V \text{ is linearly indepenent}
%
$$
%

(We saw that $N$ vectors span $\mathbb{R}^N$ iff linearly indepenent)

**Exercise:** rest of proof

````


```{admonition} Fact
:class: important

If $T \colon \mathbb{R}^N \to \mathbb{R}^N$ is nonsingular then so is $T^{-1}$. 
```

````{admonition} Proof
:class: dropdown

**Exercise:** prove this statement

````

### Mappings Across Different Dimensions 

Remember that the above results apply to mappings from $\mathbb{R}^N$ to $\mathbb{R}^N$

Things change when we look at linear mappings across dimensions

The general rules for linear mappings are 

- Mappings from lower to higher dimensions cannot be onto
- Mappings from higher to lower dimensions cannot be one-to-one
- In either case they cannot be bijections

```{admonition} Fact
:class: important

For a linear mapping $T$ from $\mathbb{R}^K \to \mathbb{R}^N$, the following statements are true:
%
1. If $K < N$ then $T$ is not onto
9. If $K > N$ then $T$ is not one-to-one
%
```

````{admonition} Proof
:class: dropdown

Proof of part 1: Let $K < N$ and let $T \colon \mathbb{R}^K \to \mathbb{R}^N$ be linear 

Letting $V = \{Te_1, \ldots, Te_K\}$, we have
%
$$
%
\dim(\mathrm{rng}(T)) = \dim(\mathrm{span}(V)) \leq K < N
%
$$
%
$$
%
\implies 
\mathrm{rng}(T) \ne \mathbb{R}^N
%
$$
%

Hence $T$ is not onto 

Proof of part 2: $K > N$ $\implies$ $T$ is not one-to-one

Suppose to the contrary that $T$ is one-to-one

Let $\alpha_1, \ldots, \alpha_K$ be a collection of vectors such that 
%
$$
%
\alpha_1 T e_1 + \cdots + \alpha_K T e_K = 0
%
$$
%
$$
%
\implies 
T (\alpha_1 e_1 + \cdots + \alpha_K e_K) = 0
\qquad (\text{by linearity})
%
$$
%
$$
%
\implies
\alpha_1 e_1 + \cdots + \alpha_K e_K = 0
\qquad (\text{since $\ker(T) = \{0\}$})
%
$$
%
$$
%
\implies 
\alpha_1 = \cdots = \alpha_K = 0
\qquad (\text{by independence of $\{e_1, \ldots e_K\}$)}
%
$$
%

We have shown that $\{Te_1, \ldots, Te_K\}$ is linearly
independent

But then $\mathbb{R}^N$ contains a linearly independent set
with $K > N$ vectors --- contradiction

````

````{admonition} Example
:class: tip

Cost function $c(k, \ell) = rk + w\ell$ cannot be one-to-one

```{figure} _static/plots/cost_min_2.png
:width: 50%
:name: 
```
````


### Matrices as Mappings

Any $N \times K$ matrix $A$ can be thought of as a function
$x \mapsto A x$
%
- In $A x$ the $x$ is understood to be
a column vector

It turns out that every such mapping is linear!

To see this fix $N \times K$ matrix $A$ and let $T$ be defined by
%
$$
%
T \colon \mathbb{R}^K \to \mathbb{R}^N, 
\qquad
Tx = A x
%
$$
%

Pick any $x$, $y$ in $\mathbb{R}^K$, and any scalars $\alpha$ and $\beta$

The rules of matrix arithmetic tell us that
%
$$
%
T(\alpha x + \beta y) 
= A (\alpha x + \beta y)
= \alpha A x + \beta A y
=: \alpha Tx + \beta Ty 
%
$$
%

So matrices make linear functions

How about examples of linear functions that don't involve matrices? 

There are none!

```{admonition} Fact
:class: important

If $T \colon \mathbb{R}^K \to \mathbb{R}^N$ then 
%
$$
T \text{ is linear }
\; \iff \;
\exists \; N \times K \text{ matrix } A \text{ such that } Tx = A x, 
\;\forall \, x \in \mathbb{R}^K
$$
%
```

````{admonition} Proof
:class: dropdown

- Just above we showed the $\Longleftarrow$ part
- Let's show the $\implies$ part

Let $T \colon \mathbb{R}^K \to \mathbb{R}^N$ be linear

We aim to construct an $N \times K$ matrix $A$ such that
%
$$ Tx = A x, \qquad \forall \, x \in \mathbb{R}^K $$
%

As usual, let
$e_k$ be the $k$-th canonical basis vector in $\mathbb{R}^K$

Define a matrix $A$ by $\mathrm{col}_k(A) = Te_k$

Pick any $x = (x_1, \ldots, x_K) \in \mathbb{R}^K$

By linearity we have 
%
$$
%
Tx 
= T \left[\sum_{k=1}^K x_k e_k \right]
= \sum_{k=1}^K x_k T e_k
= \sum_{k=1}^K x_k \mathrm{col}_k(A)
= A x
%
$$
%

````

### Matrix Product as Composition

```{admonition} Fact
:class: important

Let

- $A$ be $N \times K$ and $B$ be $K \times M$
- $T \colon \mathbb{R}^K \to \mathbb{R}^N$ be the linear mapping $Tx = Ax$ 
- $U \colon \mathbb{R}^M \to \mathbb{R}^K$ be the linear mapping $Ux = Bx$ 

The matrix product $A B$ corresponds exactly to the
**composition** of $T$ and $U$ 

```

````{admonition} Proof
:class: dropdown

%
$$
%
(T \circ U) (x)
= T( Ux)
= T( B x)
= A B x
%
$$
%

````

This helps us understand a few things

For example, let
%
- $A$ be $N \times K$ and $B$ be $J \times M$
- $T \colon \mathbb{R}^K \to \mathbb{R}^N$ be the linear mapping $Tx = Ax$ 
- $U \colon \mathbb{R}^M \to \mathbb{R}^J$ be the linear mapping $Ux = Bx$ 

Then $A B$ is only defined when $K = J$

This is because $A B$ corresponds to $T \circ U$

But for $T \circ U$ to be well defined we need $K = J$

Then $U$ mappings $\mathbb{R}^M$ to $\mathbb{R}^K$ and $T$ mappings $\mathbb{R}^K$ to $\mathbb{R}^N$

### Column Space


```{admonition} Definition
:class: caution

Let $A$ be an $N \times K$ matrix. \
The **column space** of $A$ is defined as the span of its columns
%
$$
%
\mathrm{span}(A) 
= \mathrm{span} \{ \mathrm{col}_1 (A), \ldots, \mathrm{col}_K(A) \}
\\
= \text{all vectors of the form } \sum_{k=1}^K x_k \mathrm{col}_k(A)
%
$$
%

```

Equivalently,
%
$$
%
\mathrm{span}(A) = \{ Ax  \colon x \in \mathbb{R}^K \}
%
$$
%

This is exactly the range of the associated linear mapping
%

$T \colon \mathbb{R}^K \to \mathbb{R}^N$ defined by $T x = A x$

```{admonition} Example
:class: tip

If
%
$$
%
A =
\begin{pmatrix}
1 & -5 \\
2 & 3
\end{pmatrix}
%
$$
%
then the span is all linear combinations
%
$$
%
x_1
\left(
\begin{array}{c}
1 \\
2
\end{array}
\right)
+ 
x_2
\left(
\begin{array}{c}
-5 \\
3
\end{array}
\right)
\quad
\text{where}
\quad
(x_1, x_2) \in \mathbb{R}^2
%
$$
%

These columns are linearly independent (shown earlier)

Hence the column space is all of $\mathbb{R}^2$ (why?)
```

**Exercise:** Show that the column space of any $N \times K$ matrix is a linear
subspace of $\mathbb{R}^N$

### Rank

Equivalent questions
%
- How large is the range of the linear mapping $T x = A x$?
- How large is the column space of $A$?

The obvious measure of size for a linear subspace is its dimension

```{admonition} Definition
:class: caution

The dimension of $\mathrm{span}(A)$ is known as the **rank** of $A$ 
%
$$
%
\mathrm{rank}(A) = \mathrm{dim}(\mathrm{span}(A))
%
$$

```
%

Because $\mathrm{span}(A)$ is the span of $K$ vectors, we have
%
$$
%
\mathrm{rank}(A) = \mathrm{dim}(\mathrm{span}(A)) \leq K
%
$$
%

```{admonition} Definition
:class: caution

$A$ is said to have **full column rank** if 
%
$$
%
\mathrm{rank}(A) = \text{ number of columns of } A
%
$$
%

```

```{admonition} Fact
:class: important

For any matrix $A$, the following statements are equivalent:
%
1. $A$ is of full column rank

2. The columns of $A$ are linearly independent

9. If $A x = 0$, then $x = 0$
%
```

**Exercise:** Check this, recalling that
%
$$
%
\dim(\mathrm{span}\{a_1, \ldots, a_K\}) = K
\, \iff \,
\{a_1, \ldots, a_K\} \text{ linearly indepenent}
%
$$
%


```{code-cell} python3

import numpy as np
A = [[2.0, 1.0],
     [6.5, 3.0]]
print(f'Rank = {np.linalg.matrix_rank(A)}')
A = [[2.0, 1.0],
     [6.0, 3.0]]
print(f'Rank = {np.linalg.matrix_rank(A)}')
```

### Systems of Linear Equations

Let's look at solving linear equations such as $A x = b$ where $A$ is a square matrix

- Take $N \times N$ matrix $A$ and $N \times 1$ vector $b$ as given 
- Search for an $N \times 1$ solution $x$

But does such a solution exist? If so is it unique?

The best way to think about this is to consider 
the corresponding linear mapping
%
$$
%
T \colon \mathbb{R}^N \to \mathbb{R}^N,
\qquad Tx = A x
%
$$
%

Equivalent:
%
1. $A x = b$ has a unique solution $x$ for any
given $b$
2. $T x = b$ has a unique solution $x$ for any
given $b$
9. $T$ is a bijection

We already have conditions for linear mappings to be bijections

Just need to translate these into the matrix setting

Recall that $T$ called nonsingular if $T$ is a linear bijection

```{admonition} Definition
:class: caution

We say that $A$ is **nonsingular** if $T: x \to Ax$ is nonsingular

```

Equivalent:

- $x \mapsto A x$ is a bijection from $\mathbb{R}^N$ to $\mathbb{R}^N$

We now list equivalent conditions for nonsingularity

```{admonition} Fact
:class: important

Let $A$ be an $N \times N$ matrix \
All of the following conditions are equivalent

%
1. $A$ is nonsingular

2. The columns of $A$ are linearly independent

3. $\mathrm{rank}(A) = N$

4. $\mathrm{span}(A) = \mathbb{R}^N$

5. If $A x = A y$, then $x = y$

6. If $A x = 0$, then $x = 0$

7. For each $b \in \mathbb{R}^N$, the equation $A x = b$ has a solution

8. For each $b \in \mathbb{R}^N$, the equation $A x = b$ has a unique solution

```

All equivalent ways of saying that $Tx = A x$ is a bijection!

```{admonition} Example
:class: tip

For condition 5 the equivalence is: \
$A x = A y$, then $x = y$
$\iff$
$T x = T y$, then $x = y$
$\iff$
$T$ is one-to-one
$\iff$
Since $T$ is a linear mapping from $\mathbb{R}^N$ to $\mathbb{R}^N$,
$T$ is a bijection

```

```{admonition} Example
:class: tip

For condition 6 the equivalence is: \
if $A x = 0$, then $x = 0$
$\iff$
$\{x: Ax = 0\} = \{0\}$
$\iff$
$\{x: Tx = 0\} = \{0\}$
$\iff$
$\ker{T}=\{0\}$
$\iff$
Since $T$ is a linear mapping from $\mathbb{R}^N$ to $\mathbb{R}^N$,
$T$ is a bijection
```
% 

```{admonition} Example
:class: tip

For condition 7 the equivalence is: \
for each $b\in\mathbb{R}^N$, the equation $Ax = b$ has a solution
$\iff$
every $b\in\mathbb{R}^N$ has an $x$ such that $Ax = b$
$\iff$
every $b\in\mathbb{R}^N$ has an $x$ such that $Tx = b$
$\iff$
$T is onto/surjection$
$\iff$
Since $T$ is a linear mapping from $\mathbb{R}^N$ to $\mathbb{R}^N$,
$T$ is a bijection
```

```{admonition} Example
:class: tip

Now consider condition 2: \

The columns of $A$ are linearly independent.

Let $e_j$ be the $j$-th canonical basis vector in $\mathbb{R}^N$.

Observe that $Ae_j = \mathrm{col}_j(A)$
$\implies$
$Te_j = \mathrm{col}_j(A)$
$\implies$
$V = \{T e_1, \ldots, Te_N\}
=$ columns of $A$, and $V$ is linearly independent if and only if $T$ is a bijection
```

````{admonition} Example
:class: tip

Consider a one good linear market system 
%
$$
%
q = a - b p \qquad (\text{demand}) \\
q = c + d p \qquad (\text{supply})
%
$$
%

Treating $q$ and $p$ as the unknowns, let's write in matrix form as 
%
$$
%
\begin{pmatrix}
1 & b \\
1 & -d
\end{pmatrix}
\begin{pmatrix}
q\\
p
\end{pmatrix}
=
\begin{pmatrix}
a \\
c 
\end{pmatrix}
%
$$
%

A unique solution exists whenever the columns are linearly independent

- so $(b, -d)$ is not a scalar multiple of $1$ $\iff$ $b \ne -d$ 

```{figure} _static/plots/not_multiple_of_one.png
:width: 50%
:name: 

$(b, -d)$ is not a scalar multiple of $1$
```
````

```{admonition} Example
:class: tip

Recall in the introduction we try to solve the system $A x = b$
of this form

$$
A = \left[\begin{array}{cccc}
1 & 2 & 4 \\
1 & 4 & 8 \\
0 & 3 & 6 \\
\end{array}
\right]
$$

The problem is that $A$ is singular (not nonsingular)

- In particular, $\mathrm{col}_3(A) = 2 \mathrm{col}_2(A)$
```

```{admonition} Definition
:class: caution

Given square matrix $A$, suppose $\exists$ square matrix $B$ such that 
%
$$A B = B A = I$$ 
%
Then
%
- $B$ is called the **inverse** of $A$, and written $A^{-1}$
- $A$ is called **invertible**

```

```{admonition} Fact
:class: important

A square matrix $A$ is nonsingular if and only if it is invertible
```

- $A^{-1}$ is just the matrix corresponding to the linear mapping $T^{-1}$

```{admonition} Fact
:class: important

Given nonsingular $N \times N$ matrix $A$ and $b \in \mathbb{R}^N$, the unique solution to $A x = b$ is given by 
%
$$ x_b = A^{-1} b $$
%
```

````{admonition} Proof
:class: dropdown

Since $A$ is nonsingular we already know any solution is
unique

- $T$ is a bijection, and hence one-to-one
- if $A x = A y = b$ then $x = y$

To show that $x_b$ is indeed a solution we need to show that 
$A x_b = b$ 

To see this, observe that
%
$$
%
A x_b = A A^{-1} b = I b = b
%
$$
%

````


```{admonition} Fact
:class: important

In the $2 \times 2$ case, the inverse has the form

%
$$
%
\left(
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right)^{-1} = 
\frac{1}{ad - bc}
\left(
\begin{array}{cc}
d & -b \\
-c & a \\
\end{array}
\right)
%
$$
%
```


```{admonition} Example
:class: tip

%
$$
%
A = 
\left(
\begin{array}{cc}
1 & 2 \\
1 & -1.5 \\
\end{array}
\right)
\quad \implies \quad
A^{-1} = 
\frac{1}{-3.5}
\left(
\begin{array}{cc}
-1.5 & -2 \\
-1 & 1 \\
\end{array}
\right)
%
$$
%
```


```{admonition} Fact
:class: important

If $A$ is nonsingular and $\alpha \ne 0$, then
%
1. $A^{-1}$ is nonsingular and $(A^{-1})^{-1} = A$
9. $\alpha A$ is nonsingular and $(\alpha A)^{-1} = \alpha^{-1} A^{-1}$
```

````{admonition} Proof
:class: dropdown

Proof of part 2: 

It suffices to show that $\alpha^{-1} A^{-1}$ is the right inverse of 
$\alpha A$

This is true because
%
$$
%
\alpha A \alpha^{-1} A^{-1} 
=
\alpha \alpha^{-1} A A^{-1} 
= I
%
$$
%

````

```{admonition} Fact
:class: important

If $A$ and $B$ are $N \times N$ and nonsingular then
%
1. $A B$ is also nonsingular
9. $(A B)^{-1} = B^{-1} A^{-1}$
```

````{admonition} Proof
:class: dropdown

Proof I: Let $T$ and $U$ be the linear mappings corresponding to $A$ and
$B$

Recall that

- $T \circ U$ is the linear mapping corresponding to $A B$
- Compositions of linear mappings are linear
- Compositions of bijections are bijections

Hence $T \circ U$ is a linear bijection with $(T \circ U)^{-1} = U^{-1} \circ T^{-1}$

That is, $AB$ is nonsingular with inverse $B^{-1} A^{-1}$

Proof II: 

A different proof that $A B$ is nonsingular with inverse 
$B^{-1} A^{-1}$

Suffices to show that $B^{-1} A^{-1}$ is the right inverse of $A B$

To see this, observe that 
%
$$
%
A B B^{-1} A^{-1}
= A A^{-1}
= I
%
$$
%

Hence $B^{-1} A^{-1}$ is a right inverse as claimed

````

### Singular case

````{admonition} Example
:class: tip

The matrix $A$ with columns
%
$$
%
a_1 =
\begin{pmatrix}
3 \\
4 \\
2
\end{pmatrix},
\quad
a_2 =
\begin{pmatrix}
3 \\
-4 \\
1
\end{pmatrix}
\quad \text{and} \quad
a_3 =
\begin{pmatrix}
-3 \\
4 \\
-1
\end{pmatrix}
%
$$
%
is singular ($a_3 = - a_2$)

Its column space $\mathrm{span}(A)$ is just a plane in $\mathbb{R}^2$

Recall $b \in \mathrm{span}(A)$ 
%
- $\iff$ $\exists \, x_1, \ldots, x_N$ such that $\sum_{k=1}^N
x_k \mathrm{col}_k(A) = b$

- $\iff$ $\exists \, x$ such that $A x = b$

Thus if $b$ is not in this plane then $A x = b$ has no
solution

```{figure} _static/plots/not_in_span.png
:width: 50%
:name: 

The vector $b$ is not in $\mathrm{span}(A)$
```
````

```{admonition} Fact
:class: important

If $A$ is a singular matrix and $A x = b$ has a
solution then it has an infinity (in fact a continuum) of solutions
```

````{admonition} Proof
:class: dropdown

Let $A$ be singular and let $x$ be a solution

Since $A$ is singular there exists a nonzero $y$ with $A
y = 0$

But then $\alpha y + x$ is also a solution for any $\alpha \in
\mathbb{R}$ because
%
$$ 
%
A(\alpha y + x) 
= \alpha A y + A x
= A x = b
%
$$
%

````









## Determinants


```{admonition} Fact
:class: important
:name: det2by2

In the $N = 2$ case this definition reduces to

%
$$
%
\det 
\left(
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right)
= ad - bc
%
$$
%
```

```{admonition} Example
:class: tip

%
$$
%
\det 
\left(
\begin{array}{cc}
2 & 0 \\
7 & -1 \\
\end{array}
\right)
= (2 \times -1) - (7 \times 0) = -2
%
$$
%

```

````{admonition} Fact
:class: important
:name: det3by3

Determinant of $3 \times 3$ matrix can be computed by the **triangles rule**:
%
$$
\mathrm{det}
\left(
\begin{array}{ccc}
a_{11},& a_{12},& a_{13} \\
a_{21},& a_{22},& a_{23} \\
a_{31},& a_{32},& a_{33}
\end{array}
\right) =
\begin{array}{l}
+ a_{11}a_{22}a_{33} \\
+ a_{12}a_{23}a_{31} \\
+ a_{13}a_{21}a_{32} \\
- a_{13}a_{22}a_{31} \\
- a_{12}a_{21}a_{33} \\
- a_{11}a_{23}a_{32}
\end{array}
$$
%

```{figure} _static/img/det33.png
:scale: 100%
```

````

Important facts concerning the determinants

```{admonition} Fact
:class: important

If $I$ is the $N \times N$ identity, $A$ and $B$ are $N \times N$ matrices and $\alpha \in \mathbb{R}$, then
%
1. $\det(I) = 1$ 

2. $A$ is nonsingular if and only if $\det(A)
\ne 0$

3. $\det(AB) = \det(A)
\det(B)$

4. $\det(\alpha A) = \alpha^N \det(A)$

9. $\det(A^{-1}) = (\det(A))^{-1}$
```





## Eigenvalues and Eigenvectors

Let $A$ be $N \times N$ 

In general $A$ mappings $x$ to some arbitrary new location $A x$

But sometimes $x$ will only be **scaled**:
%
$$
%
A x = \lambda x
\quad \text{for some scalar $\lambda$}
%
$$ (eq:eiei)
%

```{admonition} Definition
:class: caution

If {eq}`eq:eiei` holds and $x$ is nonzero, then 
%
1. $x$ is called an **eigenvector** of $A$
and $\lambda$ is called an **eigenvalue**

9. $(x, \lambda)$ is called an **eigenpair**

```

Clearly $(x, \lambda)$ is an eigenpair of $A$ $\implies$
$(\alpha x, \lambda)$ is an eigenpair of $A$ for any nonzero $\alpha$

```{admonition} Example
:class: tip

Let
%
$$
%
A =
\begin{pmatrix}
1 & -1 \\
3 & 5
\end{pmatrix}
%
$$
%

Then

%
$$
%
\lambda = 2 
\quad \text{ and } \quad
x
=
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
%
$$
%

form an eigenpair because $x \ne 0$ and

%
$$
%
A x =
\begin{pmatrix}
1 & -1 \\
3 & 5
\end{pmatrix}
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
=
\begin{pmatrix}
2 \\
-2
\end{pmatrix}
= 2
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
=
\lambda x 
%
$$
%
```

```{code-cell} python3
import numpy as np
A = [[1, 2],
     [2, 1]]
eigvals, eigvecs = np.linalg.eig(A)
for i in range(eigvals.size):
  x = eigvecs[:,i]
  lm = eigvals[i]
  print(f'Eigenpair {i}:\n{lm:.5f} --> {x}')
  print(f'Check Ax=lm*x: {np.dot(A, x)} = {lm * x}')
```

```{figure} _static/plots/eigenvecs.png
:width: 50%
:name: 

The eigenvectors of $A$
```

Consider the matrix 
%
$$
%
R = 
\left(
\begin{array}{cc}
0 & -1 \\
1 & 0 \\
\end{array}
\right)
%
$$
%

Induces counter-clockwise rotation on any point by $90^{\circ}$

```{hint}
The rows of the matrix show where the classic basis vectors are translated to.
```

Hence no point $x$ is scaled

Hence there exists **no** pair $\lambda \in \mathbb{R}$ and $x \ne
0$
such that
%
$$R x = \lambda x$$ 
%

In other words, no **real-valued** eigenpairs exist

```{figure} _static/plots/rotation_1.png
:width: 50%
:name: 

The matrix $R$ rotates points by $90^{\circ}$
```

```{figure} _static/plots/rotation_2.png
:width: 50%
:name: 

The matrix $R$ rotates points by $90^{\circ}$
```

But $R x = \lambda x$ can hold **if** we allow
complex values

```{admonition} Example
:class: tip
%
$$
%
\left(
\begin{array}{cc}
0 & -1 \\
1 & 0 \\
\end{array}
\right)
\begin{pmatrix}
1 \\
-i
\end{pmatrix}
=
\begin{pmatrix}
i \\
1
\end{pmatrix}
=
i
\begin{pmatrix}
1 \\
-i
\end{pmatrix}
%
$$
%
```

That is,
%
$$
%
R x = \lambda x
\quad \text{for} \quad
\lambda = i
\quad \text{and} \quad
x = 
\begin{pmatrix}
1 \\
-i
\end{pmatrix}
%
$$
%

Hence $(x, \lambda)$ is an eigenpair provided we admit complex values 

```{note}
We do, since this is standard, but will not go into details in this course
```

```{admonition} Fact
:class: important

For any square matrix $A$ 
%
$$
%
\lambda \text{ is an eigenvalue of } A \; \iff \;
\det(A - \lambda I) = 0
%
$$
%
```

````{admonition} Proof
:class: dropdown


Let $A$ by $N \times N$ and let $I$ be the $N \times N$
identity

We have
%
$$
%
\det(A - \lambda I) = 0
\iff A - \lambda I \text{ is singular}
\\
\iff \exists \, x \ne 0 \text{ such that }
(A - \lambda I) x = 0
\\
\iff \exists \, x \ne 0 \text{ such that }
A x = \lambda x
\\
\iff \lambda 
\text{ is an eigenvalue of } A
%
$$
%
````


```{admonition} Example
:class: tip

In the $2 \times 2$ case,

%
$$
%
A =
\left(
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right)
\quad \implies \quad
A - \lambda I =
\left(
\begin{array}{cc}
a - \lambda & b \\
c & d - \lambda 
\end{array}
\right)
%
$$
%
$$
%
\implies
\det(A - \lambda I) 
= (a - \lambda)(d - \lambda) - bc
\\
= \lambda^2 - (a + d) \lambda + (ad - bc)
%
$$
%

Hence the eigenvalues of $A$ are given by the two roots of 
%
$$
%
\lambda^2 - (a + d) \lambda + (ad - bc) = 0
%
$$
%

Equivalently,
%
$$
%
\lambda^2 - \mathrm{trace}(A) \lambda + \det(A) = 0
%
$$
%
```

<!-- ### Existence of Eigenvalues

Fix $N \times N$ matrix $A$ 

```{admonition} Fact
:class: important

There exist complex numbers $\lambda_1, \ldots, \lambda_N$ such that
```
%
$$
%
\det(A - \lambda I) = \prod_{n=1}^N (\lambda_n - \lambda)
%
$$
%

Each such $\lambda_i$ is an eigenvalue of $A$ because
%
$$
%
\det(A - \lambda_i I) 
= \prod_{n=1}^N (\lambda_n - \lambda_i) 
= 0
%
$$
%

Important: Not all are necessarily distinct --- there can be repeats

```{admonition} Fact
:class: important

Given $N \times N$ matrix $A$ with eigenvalues $\lambda_1, \ldots, \lambda_N$
```
we have
%

- $\det(A) = \prod_{n=1}^N \lambda_n$

- $\mathrm{trace}(A) = \sum_{n=1}^N \lambda_n$

- If $A$ is symmetric, then $\lambda_n \in \mathbb{R}$ for all $n$

9. If $A = \mathrm{diag}(d_1, \ldots, d_N)$, then $\lambda_n = d_n$ for all $n$

Hence $A$ is nonsingular $\iff$ all eigenvalues are nonzero (why?)

```{admonition} Fact
:class: important

If $A$ is nonsingular, then 
```
%
$$
%
\text{eigenvalues of } A^{-1}
= 1/\lambda_1, \ldots, 1/\lambda_N
%
$$
% 
-->



## Diagonalization

Consider a square $N \times N$ matrix $A$

$$
%
A = 
\left(
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1N} \\
a_{21} & a_{22} & \cdots & a_{2N} \\
\vdots & \vdots & & \vdots \\
a_{N1} & a_{N2} & \cdots & a_{NN} \\
\end{array}
\right)
%
$$


```{admonition} Definition
:class: caution

The $N$ elements of the form $a_{nn}$ are called the **principal diagonal**

```

```{admonition} Definition
:class: caution

A square matrix $D$ is called **diagonal** if all entries off the
principal diagonal are zero

%
$$
%
D = 
\left(
\begin{array}{cccc}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & d_N \\
\end{array}
\right)
%
$$
%

Often written as

%
$$
%
D = \mathrm{diag}(d_1, \ldots, d_N) 
%
$$
%

```



```{admonition} Fact
:class: important

If $C = \mathrm{diag}(c_1, \ldots, c_N)$ and $D = \mathrm{diag}(d_1, \ldots,d_N)$ then
%
1. $C + D = \mathrm{diag}(c_1 + d_1, \ldots, c_N + d_N)$

2. $C D = \mathrm{diag}(c_1 d_1, \ldots, c_N d_N)$

3. $D^k = \mathrm{diag}(d^k_1, \ldots, d^k_N)$ for any $k \in \mathbb{N}$

4. $d_n \geq 0$ for all $n$ $\implies$ $D^{1/2}$ exists and equals
%
$$\mathrm{diag}(\sqrt{d_1}, \ldots, \sqrt{d_N})$$
%

5. $d_n \ne 0$ for all $n$ $\implies$ $D$ is nonsingular and 
%
$$D^{-1} = \mathrm{diag}(d_1^{-1}, \ldots, d_N^{-1})$$
%
```



```{admonition} Definition
:class: caution

Square matrix $A$ is said to be **similar** to square matrix $B$ if 
%
$$
%
\exists \text{ invertible matrix $P$ such that }
A = P B P^{-1}
%
$$ 
%

```

```{figure} _static/plots/diagonalize.png
:width: 50%
:name: 

```

```{admonition} Fact
:class: important

If $A$ is similar to $B$, then $A^t$ is similar to
$B^t$ for all $t \in \mathbb{N}$
```

````{admonition} Proof
:class: dropdown

Proof for case $t=2$: 
%
$$
%
A^2
= A A \\
= P B P^{-1} P B P^{-1} \\
= P B B P^{-1} \\
= P B^2 P^{-1} 
%
$$
%

````

```{admonition} Definition
:class: caution

If $A$ is similar to a diagonal matrix, then $A$ is
called **diagonalizable**

```

```{admonition} Fact
:class: important

Let $A$ be diagonalizable with $A = P D
P^{-1}$ and let
%
1. $D = \mathrm{diag}(\lambda_1, \ldots, \lambda_N)$
9. $p_n = \mathrm{col}_n(P)$
%
Then $(p_n, \lambda_n)$ is an eigenpair of $A$ for each $n$
```

````{admonition} Proof
:class: dropdown

From $A = P D P^{-1}$ we get $A P = P D$

Equating $n$-th column on each side gives 
%
$$
%
A p_n = \lambda_n p_n
%
$$
%

Moreover $p_n \ne 0$ because $P$ is invertible (which facts?)

````

```{admonition} Fact
:class: important

If $N \times N$ matrix $A$ has $N$ distinct eigenvalues
$\lambda_1, \ldots, \lambda_N$, then
$A$ is diagonalizable as $A = P D P^{-1}$ where
%
1. $D = \mathrm{diag}(\lambda_1, \ldots, \lambda_N)$
9. $\mathrm{col}_n(P)$ is an eigenvector for $\lambda_n$
```

```{admonition} Example
:class: tip

Let
%
$$
%
A =
\begin{pmatrix}
1 & -1 \\
3 & 5
\end{pmatrix}
%
$$
%

The eigenvalues of $A$ are 2 and 4, while the eigenvectors are
%
$$
%
p_1 =
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\quad \text{and} \quad
p_2 =
\begin{pmatrix}
1 \\
-3
\end{pmatrix}
%
$$
%

Hence
%
$$
%
A = P \mathrm{diag}(2, 4) P^{-1}
%
$$
%
```


```{code-cell} python3
import numpy as np
from numpy.linalg import inv
A = np.array([[1, -1],
              [3, 5]])
eigvals, eigvecs = np.linalg.eig(A)
D = np.diag(eigvals)
P = eigvecs
print('A =',A,sep='\n')
print('D =',D,sep='\n')
print('P =',P,sep='\n')
print('P^-1 =',inv(P),sep='\n')
print('P*D*P^-1 =',P@D@inv(P),sep='\n')
```


(references_reading)=
## References and reading

```{dropdown} References
- {cite:ps}`simon1994`: 6.1, 6.2, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 11, 16, 23.7, 23.8
- {cite:ps}`sundaram1996`: Appendix C.1, C.2
```

```{dropdown} Further reading and self-learning

- Excellent visualizations of concepts covered in this lecture, strongly recommended for further study
[3Blue1Brown: Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

```
