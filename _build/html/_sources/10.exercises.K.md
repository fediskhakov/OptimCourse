---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Exercise set K

Please, see the 
[**general comment on the tutorial exercises**](02.exercises.A.md)

## Question K.1

Consider an unconstrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Prove the following statement

```{admonition} Statement
:class: important

Let $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ be a differentiable function, and $x^\star(\theta)$ be the maximizer of $f(x,\theta)$ for every $\theta$.
Suppose that $x^\star(\theta)$ is differentiable function itself.
Then the value function of the problem $V(\theta) = f\big(x^\star(\theta),\theta)$ is differentiable w.r.t. $\theta$ and
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial f}{\partial \theta_j} \big(x^\star(\theta),\theta\big),
\forall j
$$

```

```{hint}
Total derivative + first order conditions
```

## Question K.2

Consider an equality constrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to}
\\
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}\\
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Prove the following statement

```{admonition} Statement
:class: important

Assume that the maximizer correspondence in the problem above is *single-valued* and can be represented by the function $x^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^N$, with the corresponding Lagrange multipliers $\lambda^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^K$.

Assume that both $x^\star(\theta)$ and $\lambda^\star(\theta)$ are differentiable, and that the constraint qualification assumption holds.
Then 
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial \mathcal{L}}{\partial \theta_j} \big(x^\star(\theta),\lambda^\star(\theta),\theta\big),
\forall j
$$
%
where $\mathcal{L}(x,\lambda,\theta)$ is the Lagrangian of the problem.
```

## Question K.3

```{admonition} Roy's identity
:class: important

Consider the choice problem of a consumer endowed with strictly concave and differentiable utility function $u \colon \mathbb{R}^N_{++} \to \mathbb{R}$ where $\mathbb{R}^N_{++}$ denotes the set of vector in $\mathbb{R}^N$ with strictly positive elements.

The budget constraint is given by ${\bf p}\cdot{\bf x} \le m$ where ${\bf p} \in \mathbb{R}^N_{++}$ are prices and $m>0$ is income.

Then the demand function $x^\star({\bf p},m)$ and the indirect utility function (value function of the problem) satisfy the equations
%
$$
x_i^\star({\bf p},m) = -\frac{\partial v}{\partial p_i}({\bf p},m) \Big/ \frac{\partial v}{\partial m}({\bf p},m), \; \forall i \in \{1,\dots,N\}
$$
```

1. Prove the statement

2. Verify the statement by direct calculation (i.e. by expressing the indirect utility and plugging its partials into the identity) using the following specification of utility
%
$$
u({\bf x}) = \prod_{i=1}^N x_i^{\alpha_i}, \; \alpha_i > 0
$$

```{hint}
Envelope theorem should be useful
```

````{admonition} Solutions
:class: dropdown

**Question K.1**

See Theorem 19.4 in Simon and Blume (1994), pp. 453-454

**Question K.2**

Here is a version of proof. The Lagrangian is
%
$$
\mathcal{L}(x^\star(\theta),\lambda^\star(\theta), \theta) = 
f(x^\star(\theta),\theta)- \sum_{i=1}^{I}\lambda_{i}^\star(\theta) g_{i}(x^\star(\theta), \theta )
$$
%
For any $j=1,\dots,K$ the partial derivative with respect to $\theta_{j}$ is 
%
$$
\begin{array}{rl}
\frac{\partial\mathcal{L}(x^\star(\theta), \lambda^\star(\theta), \theta)}{\partial{\theta_{j}}}
&= 
\frac{\partial f(x^\star(\theta), \theta)}{\partial{\theta_{j}}} 
- \sum_{i=1}^{I} \left[ \frac{\partial\lambda_{i}^\star(\theta)}{\partial \theta_{j}}g_{i}(x^\star(\theta), \theta) + \lambda_{i}^\star(\theta) \frac{\partial g_{i}(x^\star(\theta), \theta) }{\partial \theta_{j}}\right] \\
&= \frac{\partial f(x^\star(\theta), \theta)}{\partial{\theta_{j}}} 
- \sum_{i=1}^{I} \lambda_{i}^\star(\theta) \frac{\partial g_{i}(x^\star(\theta), \theta) }{\partial \theta_{j}} 
\end{array}

$$
%
where the last equality follows from the equality constraints $g_{i}(x^\star(\theta), \theta)=0$ for all $i$.
The partial derivative of value function is
%
$$
\begin{array}{rl}
\frac{\partial V(\theta)}{\partial{\theta_{j}}}
&= 
\frac{d f(x^\star(\theta),\theta)}{d \theta_{j}} \\ 
&= \sum_{n=1}^{N} \frac{\partial f(x^\star(\theta), \theta)}{\partial x_{n}} \frac{\partial x_n(\theta)}{\partial \theta_{j}} 
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}} \\
&= \sum_{n=1}^{N} \left(\sum_{i=1}^{I}\lambda_{i} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial x_{n}} \right)\frac{\partial x_{n}(\theta)}{ \partial \theta_{j}} 
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}} \\
&= \sum_{i=1}^{I}\lambda_{i} \sum_{n=1}^{N} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial x_{n}}\frac{\partial x_{n}(\theta)}{\partial\theta_{j}} 
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}} \\
&= - \sum_{i=1}^{I}\lambda_{i} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial \theta_j}
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}}\\
&= \frac{\partial\mathcal{L}(x^\star(\theta), \lambda^\star(\theta), \theta)}{\partial{\theta_{j}}}
\end{array}

$$
%
where the third equality follows from the first-order conditions,
%
$$
\begin{array}{rl}
0 = \frac{\partial\mathcal{L}(x^\star(\theta), \lambda^\star(\theta), \theta)}{\partial{x_n}}
&= 
\frac{\partial f(x^\star(\theta), \theta)}{\partial{x_n}} 
- \sum_{i=1}^{I} \lambda_{i}^\star(\theta) \frac{\partial g_{i}(x^\star(\theta), \theta) }{\partial x_{n}}, \\
\end{array}
$$
%
and the fifth equality follows from the constraint and its total derivative
%
$$
g_{i}(x^\star(\theta), \theta)=0\\ \Longrightarrow 
\sum_{n=1}^{N} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial x_{n}}\frac{\partial x_{n}(\theta)}{ \partial\theta_{j}} 
+ \frac{\partial g_i(x^\star(\theta), \theta)}{\partial \theta_{j}}= 0
$$
%


**Question K.3**

We first show the Roy's identity.
The value function is $V(p, m)=\max\{u(x): p \cdot x \leq m\}$ where $u\colon \mathbb{R}^{N}_{++}\to \mathbb{R}$.
The Lagrangian of the maximization problem is $\mathcal{L}(x, \lambda, p, m) = u(x) - \lambda (\sum_{i=1}^{N}p_{i}x_{i}- m)$. 
The Envelope Theorem implies
%
$$

\begin{align*}
&\frac{\partial V}{\partial p_{j}}=\frac{\partial\mathcal{L}}{\partial{p_{j}}}=-\lambda x_{j} \qquad (j=1,\dots, N)\\
&\frac{\partial{V}}{\partial{m}}= \frac{\partial{\mathcal{L}}}{\partial{m}}=\lambda
\end{align*}

$$
%
It follows from the previous equations that 
%
$$
x_{j}=- \frac{1}{\lambda} \frac{\partial V}{\partial p_{j}} = - \frac{\partial V}{\partial p_{j}} \bigg/ \frac{\partial V}{\partial m}.
$$
%
Next, let $u(x)= \prod_{i=1}^{N}x_{i}^{\alpha_i}$ where $\alpha_i>0$ for all $i$.
Since $\log(\cdot)$ function is strictly monotone, the optimization problem is equivalent to maximize $u(x)=\sum_{i=1}^{N}\alpha_{i}\log(x_i)$.
The corresponding Lagrangian is 
%
$$
\mathcal{L}(x, \lambda, p, m) = \sum_{i=1}^{N}\alpha_{i}\log(x_{i}) - \lambda (\sum_{i}^{N}p_{i}x_{i}- m)
$$
%
The first-order conditions yield
%
$$
\begin{align*}
\frac{\partial\mathcal{L}}{\partial{x_{j}}}&= \frac{\alpha_{i}}{x_{j}}-\lambda p_{j=0} \qquad (j=1,\dots, N)\\
\frac{\partial\mathcal{L}}{\partial{\lambda}}&= \sum_{i}^{N}p_{i}x_{i}- m = 0 \\
\Longrightarrow \lambda&= \frac{\sum_{i=1}^{N}\alpha_{i}}{m}\\
x_{j}&= \frac{\alpha_{j}}{\lambda p_{j}} 
=\frac{\alpha_{j}}{\sum_{i=1}^{N}\alpha_{i}} \frac{m}{p_{j}}
\end{align*}
$$
%
Hence, the optimal value function is 
%
$$
V(p,m)=u(x^{*}(p,m))=\prod_{i=1}^{N}\left(\frac{\alpha_{j}}{\sum_{i=1}^{N}\alpha_{i}} \frac{m}{p_{j}}\right)^{\alpha_{i}}
$$
%
To verify Roy's identity, observe that 
%
$$
\begin{align*}
\frac{\partial{V}}{\partial{p_{j}}}&=\frac{\partial}{\partial{p_{j}}} e^{\log v} 
= e^{\log v} \frac{\partial}{\partial{p_{j}}}\log v = v \frac{\partial}{\partial{p_{j}}}\log v\\
&= v\frac{\partial}{\partial{p_{j}}}\left\{\sum_{i=1}^{N}\log\left(\frac{\alpha_{i}}{\sum_{i}\alpha_{i}}\right)+\alpha_{i}\log \frac{m}{p_{i}} \right\}\\
&= v \left(\alpha_{j} \frac{1}{\frac{m}{p_{j}}}\frac{-m}{p_{j}^{2}} \right)
=\frac{-\alpha_j}{p_{j}}v \\
\frac{\partial{v}}{\partial{m}}&=v\frac{\partial}{\partial{p_{j}}} \log v \\
&= v \sum_{i=1}^{N}\alpha_{i}\frac{1}{\frac{m}{p_{j}}} \frac{1}{p_{i}} = \frac{\sum_{i=1}^{N}\alpha_{i}}{m} v 
\end{align*}

$$
%
Then, we have
%
$$

-\frac{\partial{v}}{\partial{p_{j}}}\Bigg/ \frac{\partial{v}}{\partial{m}}
=\frac{\frac{-\alpha_j}{p_{j}}v} {\frac{\sum_{i=1}^{N}\alpha_{i}}{m} v}
=\frac{\alpha_{j}}{\sum_{i=1}^{N}\alpha_{i}} \frac{m}{p_{j}} 
=x_{j}.

$$
%


````

