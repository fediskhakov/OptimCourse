---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

% EDITS:
% - Only deterministic finite horizon
% - Value function
% - Maximum theorems (without correspondences and hemi-continuity?)
% - Give full formulation, and immediately a special case with concavity, unique solution and concave value function
% - Cake eating problem
% - Existence of the solution
% - FOC: Envelope and Euler equation
% - Solution as multi-variate optimization problem or ..
% - dynamic programming, Bellman principle and Bellman equation
% - Backwards induction algorithm
% - Solve the cake eating problem by hand?

```{admonition} DRAFT
:class: danger

This is a DRAFT of the future lecture!
```

# üìñ Dynamic optimization

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>


> 1. Dynamic decision problems
> 2. Dynamic programming for finite horizon problems
> 3. Dynamic programming for infinite horizon problems


## Dynamic decision problems

As before, let's start with our definition of a general optimization problem

```{admonition} Definition
:class: caution

The general form of the optimization problem is

%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to } x \in \mathcal{D}(\theta)
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $\mathcal{D}(\theta)$ denotes the admissible set
%
$$
\mathcal{D}(\theta) = \left\{ 
x \in \mathbb{R}^N \colon
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}, \;
h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}
\right\}
$$
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}$ where $h_j \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are inequality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function
```

```{admonition} Crazy thought

Can we let the objective function include the value function as a component?
```

- Think of an optimization problem that repeats over and over again, for example in time
- Value function (as the maximum attainable value) of the next optimization problem may be part of the current optimization problems
- The parameters $\theta$ may be adjusted to link these optimization problems together

```{admonition} Example
:class: tip

Imagine the optimization problem with $g(x,\theta)= \sum_{t=0}^{\infty}\beta^{t}u(x,\theta)$ where $b \in (0,1)$ to ensure the infinite sum converges. 
We can write
%
$$
\begin{array}{lll}
V(\theta) & = & \max_x \sum_{t=0}^{\infty}\beta^{t} u(x_t,\theta) \\
& = & \max_x \Big[u(x_0)+\beta\max_x \sum_{t=1}^{\infty}\beta^{t-1}u(x_t,\theta)\Big] \\
& = & \max_x \Big[u(c_{0})+\beta V(\theta)\Big]
\end{array}
$$

```

- Note that the trick in the example above works because $g(x,\theta)$ is an infinite sum
- Among other things this implies that $x$ is infinite dimension!
- Let the parameter $\theta$ to *keep track* time, so let $\theta = (t, s_t)$
- Then it is possible to use the same principle for the optimization problems without infinite sums

Introduce new notation for the value function
%
$$
\theta=(t, s_t) \rightarrow V(\theta) = V(t, s_t) = V_t(s_t)
$$

```{admonition} Definition: dynamic optimization problems
:class: caution
:name: dynamic-optimization-problem

The general form of the *deterministic dynamic optimization* problem is

%
$$
V_t(s_t)
= \max_{x} \big[ f(x,s_t) + \beta V_{t+1}(s_{t+1}) \big] \\
\text {subject to } x \in \mathcal{D}_t(s_t)
$$
%
where:
- $f(x,s_t) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an instantaneous reward function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $s_t \in \mathbb{R}^K$ are *state variables*
- state transitions are given by $s_{t+1} = g(x,s_t)$ where $g \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}^K$
- $\mathcal{D}_t(s_t) \subset \mathbb{R}^N$ denotes the admissible set at decision instance (time period) $t$
- $\beta \in (0,1)$ is a discount factor
- $V_t(s_t) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Denote the set of maximizers of each instantaneous optimization problem as
%
$$
x^\star_t(s_t) = \mathrm{argmax}_{x \in \mathcal{D}_t(s_t)} \big[ f(x,s_t) + \beta V_{t+1}(s_{t+1}) \big]
$$
%
- $x^\star_t(s_t)$ is called a *policy correspondence* or *policy function*
```

- State space $s_t$ is of primary importance for the economic interpretation of the problem: it encompasses _all_ the information that the decision maker conditions their decisions on
- Has to be carefully founded in economic theory and common sense
- Similarly the transition function $g(x,s_t)$ reflects the exact beliefs of the decision maker about how the future state is determined and has direct implications for the optimal decisions

```{note}
In the formulation above, as seen from transition function $g(x,s_t)$, we implicitly assume that the history of states from some initial time period $t_0$ is not taken into account.
Decision problems where only the current state is relevant for the future are called **Markovian decision problems**
```

### Dynamic programming and Bellman principle of optimality

> An optimal policy has a property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. \
> üìñ Bellman, 1957 ‚ÄúDynamic Programming‚Äù

```{admonition} Main idea

Breaking the problem into sequence of small problems
```

> Dynamic programming is recursive method for solving sequential decision problems \
> üìñ Rust 2006, *New Palgrave Dictionary of Economics*

- In computer science the meaning of the term is broader: DP is a general algorithm design technique for solving problems with overlapping sub-problems

- Thus, the sequential decision problem is broken into *current decision*
  and the *future decisions* problems, and solved recursively

- The solution can be computed through **backward induction** which is the process of solving a sequential decision problem from the later periods to the earlier ones, this is the main algorithm used in **dynamic programming (DP)**

- Embodiment of the recursive way of modeling sequential decisions is
  **Bellman equation**

```{admonition} Definition
:class: caution
:name: bellman-equation

Bellman equation is a *functional equation* in an unknown function $V(\cdot)$ which embodies a dynamic optimization problem and has the form
%
$$
V(\text{state}) = \max_{\text{feasible decisions}} \big[ U(\text{state},\text{decision}) + \beta \mathbb{E}\big\{ V(\text{next state}) \big| \text{state},\text{decision} \big\} \big]
$$
%
- expectation $\mathbb{E}\{\cdot\}$ is taken over the distribution of the next period state conditional on current state and decision when the state transitions are given by a stochastic process

- the optimal choices (policy correspondence) are revealed along the solution of the Bellman equation as decisions which solve the maximization problem in the right hand side, i.e.
%
$$
\text{decision}^\star(\text{state}) = \underset{\text{feasible decisions}}{\mathrm{argmax}}\big[ U(\text{state},\text{decision}) + \beta \mathbb{E}\big\{ V(\text{next state}) \big| \text{state},\text{decision} \big\} \big]
$$
%
```

- DP is a the main tool in analyzing modern micro and macto economic models
- DP provides a framework to study decision making over time and under uncertainty and can accommodate:
    - learning and human capital formation
    - wealth accumulation
    - health dynamics
    - strategic interactions between agents (game theory)
    - market interactions (equilibrium theory)
    - etc

- Many important problems and economic models are analyzed and solved
using dynamic programming: 

    - Dynamic models of labor supply 
    - Job search
    - Human capital accumulation 
    - Health process, insurance and long term care 
    - Consumption/savings choices 
    - Durable consumption 
    - Growth models
    - Heterogeneous agents models 
    - Overlapping generation models

**Origin of the term Dynamic Programming**

> The 1950‚Äôs were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defence, and he actually had a pathological fear and hatred of the word ‚Äúresearch‚Äù.\
> I‚Äôm not using the term lightly; I‚Äôm using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term, research, in his presence. You can imagine how he felt, then,
about the term, mathematical. \
> Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. \
> What title, what name, could I choose? \
> In the first place, I was interested in planning, in decision-making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word, ‚Äúprogramming‚Äù. \
> I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. \
> I thought, let‚Äôs kill two birds with one stone. Let‚Äôs take a word which has an absolutely precise meaning, namely dynamic, in the classical physical sense. \
> It also has a very interesting property as an adjective, and that is it‚Äôs impossible to use the word, dynamic, in the pejorative sense. \
> Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities. \
> üìñ Bellman‚Äôs autobiography ‚ÄúThe Eye of the Hurricane‚Äù 

### Finite and infinite horizon problems

Although dynamic programming primarily deals with infinite horizon problems, it can also be applied to the finite horizon problems where $T < \infty$
  
  - Terms **backwards induction** and **dynamic programming** are associated by many authors respectively with finite and infinite horizon problems

  - Our definition of a dynamic optimization problem contains elements of both finite and infinite horizon problem setup, and needs more detail

***Finite horizon problems***

- Time is discrete and finite, $t \in \{0,1,\dots,T\}$ where $T < \infty$
- Parameter vector $\theta$ indeed contains $t$ such that the value and policy functions along with other elements of the model do depend on $t$, i.e. may differ between time periods
- "Bellman equation" is a (common!) misuse of the term, because with time subscripts for the value function, it is not a functional equation any more
- However, the term "Bellman equation" is still used for the finite horizon problems to denote the following expression:
%
$$
V_t(s_t) = 
\begin{cases}
\max_{x \in \mathcal{D}_t(s_t)} \big[ f(x,s_t) + \beta V_{t+1}(s_{t+1}) \big] ,& \text{ if } t<T \\
\max_{x \in \mathcal{D}_T(s_T)} f(x,s_T) ,& \text{ if } t=T 
\end{cases}
$$
%
- Period $t=T$ is referred to as the *terminal period*, and the value function at this period is called the *terminal value function*
- Because there is no next period in the terminal period, $V_{T+1}(s_{T+1}) = 0$ for all $s_T$, giving the expression for the Bellman equation above

***Infinite horizon problems***

- Time is discrete and infinite, $t \in \{0,1,\dots,\infty\}$, sometimes denoted as $T = \infty$
- Parameter vector $\theta$ does not contain $t$ and so the value and policy functions can not depend on $t$, and so other elements of the model
- In other words, the solution of the model in this case has to be time invariant
- Bellman equation is a proper functional equation, which together with the policy function, describes the optimal decision rule in the long term. In some sense, the solution to the infinite dimensional problem is one pair $(V(s), x^\star(s))$, whereas in the general setup both elements of the solution are tuples
%
$$
V(s)
= \max_{x \in \mathcal{D}(s)} \big[ f(x,s) + \beta V(s') \big]
$$
%


Let's look at a particular example: cake eating



## Infinite horizon and stochastic dynamic optimization problems

- $T = \infty$
- subscripts $t$ can be dropped
- solution of the model $V(s), x^\star(s)$ is time invariant
- Bellman equation is a proper functional equation where the value function $V(s)$ is an unknown

In infinite horizon the dynamic optimization problem
%
$$
V(s)
= \max_{x} \big[ f(x,s) + \beta V(s') \big] \\
\text {subject to } x \in \mathcal{D}(s)
$$
%
where $s'$ is the *next period* state,
becomes a **fixed point** problem of a Bellman operator, i.e. the problem of solving a functional equation.

```{admonition} Definition
:class: caution
:name: bellman-operator

Let $B(S)$ denote a set of all bounded real-valued functions on a set $S \subset \mathbb{R}^K$.

**Bellman operator** is a mapping from $B(S)$ to itself defined as
%
$$
T \colon B(S) \ni V \mapsto \max_{x \in \mathcal{D}(s)} 
\big[ f(x,s) + \beta V\big(g(s,x)\big) \big] \in B(S)
$$
%
where:
- $f(x,s) \colon \mathbb{R}^N \times S \to \mathbb{R}$ is an instantaneous reward function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $s \in S \subset \mathbb{R}^K$ are *state variables*
- state transitions are given by $s' = g(x,s)$ where $g \colon \mathbb{R}^N \times S \to S$
- $\mathcal{D}(s) \subset \mathbb{R}^N$ denotes the admissible set
- $\beta \in (0,1)$ is a discount factor
- $V(s) \colon S \to \mathbb{R}$ is a value function

```

Solution of the Bellman equation is given by a fixed point of the Bellman operator, i.e. such function $V(\cdot)$ which is mapped to itself by the Bellman operator, and the corresponding policy function
%
$$
x^\star(s) = \mathrm{argmax}_{x \in \mathcal{D}(s)} \big[ f(x,s) + \beta V\big(g(x,s)\big) \big]
$$
%

### Theory of dynamic programming

```{admonition} Definition: contraction mapping
:class: caution
:name: contraction-mapping

Let $(S,\rho)$ be a complete metric space, i.e. a metric space where every Cauchy sequence converges to a point in $S$.

Let $T: S \rightarrow S$ denote an operator mapping $S$ to itself.
$T$ is called a *contraction* on $S$ with modulus $\lambda$ if $0 \le \lambda < 1$ and 
%
$$
\rho(Tx,Ty) \le \lambda \rho(x,y) \; \forall x,y \in S
$$
%
```

*Contraction mapping brings points in its domain "closer" to each other!*

```{admonition} Example
:class: tip

What is the value of annuity $V$ paying regular payments $c$ forever?
%
$$
 \stackrel{\nearrow}{V} \quad
   \stackrel{\searrow}{c} \quad
   \stackrel{\searrow}{c} \quad
   \stackrel{\searrow}{c} \quad
   \dots
$$
%
Let $r$ be the interest rate, then the value of annuity is given by
%
$$
 V=\quad
   \frac{c}{(1+r)^0} + \quad
   \frac{c}{(1+r)^1} + \quad
   \frac{c}{(1+r)^2} + \quad
   \frac{c}{(1+r)^3} + \quad
   \dots
$$
%
Equivalently with $\beta = \frac{1}{1+r}$
%
$$
V=\quad
c + \quad
c \beta + \quad
c \beta^2 + \quad
c \beta^3 + \quad
\dots
=
\sum_{t=0}^{\infty} \beta^t c
$$
%
Can reformulate recursively (as "Bellman equation" without choice)
%
$$
 V = c + \beta ( c + \beta c + \beta^2 c + \dots ) = c + \beta V
$$
%
with the corresponding "Bellman operator"
%
$$
T \colon V \mapsto c + \beta V
$$
%
Is $T(V)$ a contraction?
%
$$
|T(V_1) - T(V_2)| = |(c + \beta V_1) - (c + \beta V_2)| = \beta | V_1 - V_2 |
$$
%
Yes as long as long as $\beta < 1$!

- contraction mapping under Euclidean norm
- modulus of the contraction is $\beta$
```

Contractions are _invaluable_ because of uniqueness of their fixed point _and_ a surefire algorithm to compute them

```{admonition} Banach contraction mapping theorem (fixed point theorem)
:class: important
:name: contraction-mapping-theorem

Let $(S,\rho)$ be a complete metric space with a contraction mapping $T: S \rightarrow S$.
Then 

1. $T$ admits a unique fixed-point $V^{\star} \in S \colon T(V^{\star}) = V^{\star}$ 
2. $V^{\star}$ can be found by repeated application of the operator $T$, i.e. $T^n(V) \rightarrow V^{\star}$ as $n\rightarrow \infty$

```

In other words, the fixed point can be found by successive approximations from any starting point $\rightarrow$ commonly known in economics as value function iterations (VFI) algorithm

```{admonition} Value function iterations (VFI) algorithm
:class: caution
:name: vfi

1. Start with a guess for value function $V_0$, $i=0$
2. Apply Bellman operator to $V_i$ and increment $i$
%
$$
V_{i+1} = T(V_i)
$$
%
3. Repeat until convergence, i.e. $\| V_{i+1}-V_i \|<\epsilon$ for some small $\e

```


```{code-cell} python3
:tags: [hide-cell]

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

class annuity():

    def __init__(self,c=1,beta=.9):
        self.c = c           # Annual payment
        self.beta = beta     # Discount factor
        self.analytic = c/(1-beta)  # compute analytic solution right away
    
    def bellman(self,V):
        '''Bellman equation'''
        return self.c + self.beta*V

    def solve(self, maxiter = 1000, tol=1e-4, verbose=False):
        '''Solves the model using successive approximations'''
        if verbose: print('{:<4} {:>15} {:>15}'.format('Iter','Value','Error'))
        V0=0
        for i in range(maxiter):
            V1=self.bellman(V0)
            if verbose: print('{:<4d} {:>15.8f} {:>15.8f}'.format(i,V1,V1-self.analytic))
            if abs(V1-V0) < tol:
                break
            V0=V1
        else:  # when i went up to maxiter
            print('No convergence: maximum number of iterations achieved!')
        return V1

````

```{code-cell} python3

a = annuity(c=10,beta=0.92)
a.solve(verbose=True)
print('Numeric solution is ',a.solve())
````

Answer by the geometric series formula, assuming $\beta<1$
%
$$
 V = \sum_{t=0}^{\infty} \beta^t c = \frac{c}{1-\beta}
$$
%

```{code-cell} python3

print(f'Analytic solution is {a.analytic}')
````


**When is Bellman operator a contraction?**

%
$$
T(V)(\text{state}) = \max_{\text{decisions}} \big[ U(\text{state},\text{decision}) + \beta \mathbb{E}\big\{ V(\text{next state}) \big| \text{state},\text{decision} \big\} \big]
$$
%

- Bellman operator $T: U \rightarrow U$ from functional space $U$ to itself
- metric space $(U,d_{\infty})$ with uniform/infinity/sup norm (max abs distance between functions over their domain)



```{admonition} Blackwell sufficient conditions for contraction
:class: important
:name: blackwell-condition

Let $X \subseteq \mathbb{R}^n$ and $B(x)$ be the space of bounded functions $f: X \rightarrow \mathbb{R}$ defined on $X$.
Suppose that $T: B(X) \rightarrow B(X)$ is an operator satisfying the following conditions:

1. (monotonicity) For any $f,g \in B(X)$ and $f(x) \le g(x)$ for all $x\in X$ implies $T(f)(x) \le T(g)(x)$ for all $x\in X$,
2. (discounting) There exists $\beta \in (0,1)$ such that

%
$$
T(f+a)(x) \le T(f)(x) + \beta a, \text{ for all } f\in B(X), a \ge 0, x\in X,
$$
%

Then $T$ is a contraction mapping with modulus $\beta$.

```

- Monotonicity of Bellman equation follows trivially due to maximization in $T(V)(x)$
- Discounting: satisfied by elementary argument when $\beta<1$

Bellman operator is contraction mapping by Blackwell condition as long as the value function is bounded and the discount factor is less than one

- In practical application with the upper bound on the state space, the value function is generally bounded
- In many applications the norm $\rho$ in the metric space can be adjusted to make the value function bounded

$\Rightarrow$

- unique solution
- VFI algorithm is globally convergent
- does not depend on the numerical implementation of the Bellman operator


**Cake eating in infinite horizon**

(ref-dynamic-classification)=
## Other classes of dynamic optimization problems

Classification of dynamic problems by various criteria:

1. Nature of time
  - discrete time: here
  - continuous time: studied in the *calculus of variation*, *optimal control theory* and other branches of math

2. Uncertainty in the beliefs of the decision makers
  - deterministic problems: all transition rules are given by laws of motion, i.e. mappings from states and choices to next period states
  - stochastic problems: some transitions rules are given by conditional densities or transition probabilities from current to next period states

3. Nature of choice space
  - discrete: well suited for numerical full enumeration methods (grid search)
  - continuous: optimization theory of this course applied in full
  - mixed discrete-continuous: math is more complicated and interesting

4. Nature of state space
  - discrete: well suited for numerical methods
  - continuous: most often convert to discrete by discretization (grids)
  - mixed discrete-continuous

5. Multiple decision making processes
  - single agent models: here
  - equilibrium models: each decision makers takes environment variables as given, yet the equilibrium environment is formed by the aggregate behavior
  - multiple agents models: strategic interaction between decision makers, very hard because the joint Bellman operator is no more a contraction

Corresponding to many types of dynamic optimization problems there are many-many variations of the solution approaches

For finite horizon dynamic problems:
1. Standard backwards induction: solving Bellman equation sequentially
1. Backwards induction using F.O.C. of Bellman maximization problem (for problems with continuous choice)  
1. Finite horizon version of endogenous gridpoint method (for a particular class of problems)  

For infinite horizon dynamic problems:
1. Value function iterations (successive approximations to solve for the fixed point of Bellman operator)
1. Time iterations (successive approximations to solve for the fixed point of Coleman-Reffett operator in policy function space)  
1. Policy iteration method (Howard‚Äôs policy improvement algorithm, iterative solution for the fixed point of Bellman operator)  
1. Newton-Kantorovich method (Newton solver for the fixed point of Bellman operator)  
1. Endogenous gridpoint method (for a particular class of problems)  




## References and reading

```{dropdown} References
- {cite:ps}`sundaram1996`: Chapter 11

```

```{dropdown} Further reading and self-learning

- Computer science view on DP https://www.techiedelight.com/introduction-dynamic-programming
- Popular optimal stopping https://www.americanscientist.org/article/knowing-when-to-stop
- For more details the inventory problem see [my screencast on writing this code](https://youtu.be/JSdjTmaXr0w)
- For more dynamic programming and numerical methods see my [Foundations of Computational Economics](https://fedor.iskh.me/compecon) online course

```
