---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

NOTE: 
- start with matrices and operations on matrices
- then all the mutivariate differentiation

- differentiability, derivatives
- continuity <--> differentiability



- definition of derivative

- total derivative
- derivative in a particular direction
- steepest descent

- end with the logit example: compute hessian using matrix operations
- (to be continued in the lecture on convex optimization)
- problems set on solving FOCs from the exam (do not loose stationary points)

# üìñ Multivariate calculus

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>


## Derivatives

```{admonition} Definition
:class: caution
:name: derivative

Let $f \colon (a, b) \to \mathbb{R}$ and let $x \in (a, b)$

Let $H$ be all sequences $\{h_n\}$ such that 
$h_n \ne 0$ and $h_n \to 0$

If there exists a constant $f'(x)$ such that
%
$$
\frac{f(x + h_n) - f(x)}{h_n} \to f'(x)
$$
%
for every $\{h_n\} \in H$, then 

- $f$ is said to be **differentiable** at $x$
- $f'(x)$ is called the **derivative** of $f$ at $x$ 

```

```{figure} _static/plots/derivative.png
```

```{admonition} Example
:class: tip

Let $f \colon \mathbb{R} \to \mathbb{R}$ be defined by $f(x) = x^2$

Fix any $x \in \mathbb{R}$ and any $h_n \to 0$ with $n \to \infty$

We have
%    
$$
\frac{f(x + h_n) - f(x)}{h_n} 
= \frac{(x + h_n)^2 - x^2}{h_n} =
$$
$$
= \frac{x^2 + 2xh_n + h_n^2 - x^2}{h_n}
= 2x + h_n
$$
$$
\text{therefore }
f'(x)
= \lim_{n \to \infty} (2x + h_n)
= 2x
$$
%
```

```{admonition} Example
:class: tip

Let $f \colon \mathbb{R} \to \mathbb{R}$ be defined by $f(x) = |x|$

This function is not differentiable at $x=0$

Indeed, if $h_n = 1/n$, then
%
$$
\frac{f(0 + h_n) - f(0)}{h_n} 
= \frac{|0 + 1/n| - |0|}{1/n} \to 1
$$
%
On the other hand, if $h_n = -1/n$, then
%
$$
\frac{f(0 + h_n) - f(0)}{h_n} 
= \frac{|0 - 1/n| - |0|}{-1/n} \to -1
$$
%
```

(ref-taylor)=
## Taylor series

Loosely speaking, if $f \colon \mathbb{R} \to \mathbb{R}$ is suitably
differentiable at $a$, then
%
$$
f(x) \approx f(a) + f'(a)(x-a) 
$$
%
for $x$ very close to $a$,
%
$$
f(x) \approx f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 
$$
%
on a slightly wider interval, etc.

These are the 1st and 2nd order **Taylor series approximations**
to $f$ at $a$ respectively

As the order goes higher we get better approximation

```{figure} _static/plots/taylor_4.png
:name: taylor_4

4th order Taylor series for $f(x) = \sin(x)/x$ at 0
```

```{figure} _static/plots/taylor_6.png
:name: taylor_6

6th order Taylor series for $f(x) = \sin(x)/x$ at 0
```

```{figure} _static/plots/taylor_8.png
:name: taylor_8

8th order Taylor series for $f(x) = \sin(x)/x$ at 0
```

```{figure} _static/plots/taylor_10.png
:name: taylor_10

10th order Taylor series for $f(x) = \sin(x)/x$ at 0
```


## Multivariate calculus

```{admonition} Reminder of the derivative in one dimension
:class: caution

Let $f \colon (a, b) \to \mathbb{R}$ and let $x \in (a, b)$

Let $H$ be all sequences $\{h_n\}$ such that 
$h_n \ne 0$ and $h_n \to 0$

If there exists a constant $f'(x)$ such that
%
$$
\frac{f(x + h_n) - f(x)}{h_n} \to f'(x)
$$
%
for every $\{h_n\} \in H$, then 

- $f$ is said to be ***differentiable*** at $x$
- $f'(x)$ is called the ***derivative*** of $f$ at $x$ 

```

In higher dimensions this definition has to be adjusted to account for the fact that $h_n$ is a vector in $\mathbb{R}^N$

```{admonition} Definition
:class: caution
:name: jacobian

Let 
- $A \subset \mathbb{R}^N$ be an open set in $N$-dimensional space.
- $f \colon A \to \mathbb{R}^K$ be a $K$ dimensional vector function
- ${\bf x} \in A$, and so $\exists \epsilon \colon B_{\epsilon}({\bf x}) \subset A$

Then if there exists a $K \times N$ matrix $J$ such that
%
$$
\frac{\|f({\bf x} + {\bf h}_n) - f({\bf x}) - J {\bf h}_n\|}{\|{\bf h}_n\|} \to 0
$$
%
for all converging to zero sequences $\{{\bf h}_n\}$, ${\bf h}_n \in B_{\epsilon}({\bf 0}) \setminus \{{\bf 0}\}$, ${\bf h}_n \to {\bf 0}$,
then 

- $f$ is said to be ***differentiable*** at ${\bf x}$
- matrix $J$ is called the ***total derivative*** (or ***Jacobian*** matrix) of $f$ at ${\bf x}$, and is denoted by $Df({\bf x})$ or simply $f'({\bf x})$
```

```{note}
The vector function $f \colon \mathbb{R}^N \to \mathbb{R}^K$ (also known as *vector-valued* function) 
can be thought of as a tuple of $K$ functions 
$f_j \colon \mathbb{R}^N \to \mathbb{R}$, $j \in \{1, \dots, K\}$
```

Contrast the definition of total derivative to the definition of the *partial derivative*

```{admonition} Definition
:class: caution
:name: partial-derivative

Let $f \colon A \to \mathbb{R}$ and let ${\bf x} \in A \subset \mathbb{R}^N$.
Denote ${\bf e}_i$ the $i$-th unit vector in $\mathbb{R}^N$, i.e. ${\bf e}_i = (0, \dots, 0, 1, 0, \dots, 0)$ where $1$ is in the $i$-th position.

If there exists a constant $a \in \mathbb{R}$ such that 
%
$$
\frac{\| f({\bf x} + h_n {\bf e}_i) - f({\bf x})\|}{h_n} \to a
$$
%
for every sequence $\{h_n\}$, $h_n \in \mathbb{R}$, 
such that $h_n \ne 0$ and $h_n \to 0$ as $n \to \infty$, then 
$a$ is called a **partial derivative** of $f$ with respect to $x_i$ at ${\bf x}$, and is denoted $f'_i({\bf x})$ or $\frac{\partial f}{\partial x_i}({\bf x})$

```

Total derivative of $f \colon \mathbb{R}^N \to \mathbb{R}^K$ defines a linear map $J \colon \mathbb{R}^N \to \mathbb{R}^K$ given by $K \times N$ matrix $Df(x)$ which gives a *affine* approximation of function $f$ by a tangent hyperplane at point ${\bf x}$.
This is similar to the tangent line to a one-dimensional function determined by a derivative at any given point.

```{admonition} Fact
:class: important

If a vector function $f \colon \mathbb{R}^N \to \mathbb{R}^K$ is differentiable at ${\bf x}$, then all partial derivatives at ${\bf x}$ exist and the total derivative (Jacobian) is given by
%
$$
Df({\bf x}) =
\left(
\begin{array}{ccc}
\frac{\partial f_1}{\partial x_1}({\bf x}) & 
\cdots &
\frac{\partial f_1}{\partial x_N}({\bf x}) \\
\vdots & \ddots & \vdots \\
\frac{\partial f_K}{\partial x_1}({\bf x}) &
\cdots & 
\frac{\partial f_K}{\partial x_N}({\bf x})
\end{array}
\right)
$$
%
```

```{admonition} Fact
:class: important

If all partial derivatives of $f \colon \mathbb{R}^N \to \mathbb{R}^K$ exist and are continuous in ${\bf x}$, then $f$ is differentiable at ${\bf x}$

```

The last two facts can be stated for sets by adding the condition that the statements hold in all elements on the sets (such as "for all ${\bf x} \in A$")

```{admonition} Example
:class: tip

%
$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1^2 + x_2 x_3 \\
x_1+x_2^2+x_3^3
\end{array}
\right)
$$
%
Partial derivatives are
%
$$
\frac{\partial f_1}{\partial x_1} = 2 x_1, \;
\frac{\partial f_1}{\partial x_2} = x_3, \;
\frac{\partial f_1}{\partial x_3} = x_2 
$$
%
$$
\frac{\partial f_2}{\partial x_1} = 1, \;
\frac{\partial f_2}{\partial x_2} = 2x_2, \;
\frac{\partial f_2}{\partial x_3} = 3x_3^2
$$
%
Jacobian matrix is
%
$$
Df({\bf x}) = 
\left(
\begin{array}{rrr}
2x_1, & x_3, & x_2 \\
1, & 2x_2, & 3x_3^2
\end{array}
\right)
$$
%
Now, evaluating at a particular points
%
$$
Df({\bf 0}) = 
\left(
\begin{array}{rrr}
0, & 0, & 0 \\
1, & 0, & 0
\end{array}
\right)
$$
%
$$
Df({\bf 1}) = 
\left(
\begin{array}{rrr}
2, & 1, & 1 \\
1, & 2, & 3
\end{array}
\right)
$$
%
$$
Df((1,2,3)') = 
\left(
\begin{array}{rrr}
2, & 3, & 2 \\
1, & 4, & 27
\end{array}
\right)
$$
%

```

```{admonition} Example
:class: tip

%
$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1 + x_1 x_2 x_3 \\
x_1+2x_2-x_3 \\
x_1+2x_3 \\
x_2x_3
\end{array}
\right)
$$
%

**Exercise:** Derive Jacobian maxtix of $f$ and compute the  total derivative of $f$ at ${\bf x} = (1, 0, 2)$

```

```{admonition} Definition
:class: caution
:name: gradient

For a function $f \colon \mathbb{R}^N \to \mathbb{R}$ the Jacobian matrix takes is $1 \times N$ vector which is called ***gradient*** and is often denoted $\nabla f({\bf x}) \in \mathbb{R}^N$
%
$$
\nabla f({\bf x}) = \left(\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_N} \right) \in \mathbb{R}^N
$$

```

### Rules of differentiation

```{admonition} Fact
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and let $f, g \colon A \to \mathbb{R}^K$ be differentiable functions at ${\bf x} \in A$.

1. $f+g$ is differentiable at ${\bf x}$ and $D(f+g)({\bf x}) = Df({\bf x}) + Dg({\bf x})$
2. $cf$ is differentiable at ${\bf x}$ and $D(cf)({\bf x}) = c Df({\bf x})$ for any scalar $c$

```

```{admonition} Fact: Product (Leibniz) rule
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and let $f \colon A \to \mathbb{R}$ and $g \colon A \to \mathbb{R}^K$ be both differentiable functions at ${\bf x} \in A$.

Then $f g$ is differentiable at ${\bf x}$ and $D(f g)({\bf x}) = f({\bf x}) Dg({\bf x}) + g({\bf x}) \nabla f({\bf x})$

```

% ```{admonition} Definition
% :class: caution
% 
% Outer product of two vectors ${\bf x} \in \mathbb{R}^N$ and ${\bf y} \in \mathbb{R}^K$ is a $N \times K$ matrix given by
% %
% $$
% {\bf x} {\bf y}' =
% \left(
% \begin{array}{c}
% x_1 \\ \vdots \\ x_N
% \end{array}
% \right)
% (y_1,\dots,y_K)
% =
% \left(
% \begin{array}{lll}
% x_1 y_1 & 
% \cdots &
% x_1 y_K \\
% \vdots & \ddots & \vdots \\
% x_N y_1 &
% \cdots & 
% x_N y_K
% \end{array}
% \right)
% $$
% 
% ```
% 


```{admonition} Fact: Dot product rule
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and let $f,g \colon A \to \mathbb{R}^K$ be both differentiable functions at ${\bf x} \in A$.

Then $f \cdot g$ is differentiable at ${\bf x}$ and $D(f \cdot g)({\bf x}) = [f({\bf x})]' Dg({\bf x}) + [g({\bf x})]' Df({\bf x})$, where $[\cdot]'$ denotes vector transpose

```

```{admonition} Fact: Chain rule
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and $C$ denote an open set in $\mathbb{R}^K$.
Let $f \colon A \to C$ be differentiable at ${\bf x} \in A$, and
$g \colon C \to \mathbb{R}^L$ be differentiable at $f({\bf x}) \in C$.

Then the composition $g \circ f$ is differentiable at ${\bf x}$ and its total derivatibe is given by $D(g \circ f)({\bf x}) = Dg(f({\bf x})) Df({\bf x})$

```



```{admonition} Example
:class: tip

%
$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1^3 + x_2 x_3 \\
x_2+x_1 x_3^2
\end{array}
\right)
$$
%
$$
Df({\bf x}) = 
\left(
\begin{array}{lll}
3x_1^2, & x_3, & x_2 \\
x_3^2, & 1, & 2x_1x_3
\end{array}
\right)
$$
%
$$
g \colon
\left(
\begin{array}{c}
y_1 \\
y_2
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
2 y_1^2 \\
y_1 + y_2 \\
y_1 - y_2 \\
4 y_2^2 \\
\end{array}
\right)
$$
%
$$
Dg({\bf y}) = 
\left(
\begin{array}{lll}
4y_1, & 0 \\
1, & 1 \\
1, & -1 \\
0, & 8y_2
\end{array}
\right)
$$
%
Applying the chain rule
%
$$
D(g \circ f)({\bf x}) = 
\left(
\begin{array}{lll}
4(x_1^3 + x_2 x_3), & 0 \\
1, & 1 \\
1, & -1 \\
0, & 8(x_2+x_1 x_3^2)
\end{array}
\right)
\left(
\begin{array}{lll}
3x_1^2, & x_3, & x_2 \\
x_3^2, & 1, & 2x_1x_3
\end{array}
\right)
$$
%
$$
D(g \circ f)({\bf x}) = 
\left(
\begin{array}{lll}
12x_1^2(x_1^3 + x_2 x_3), & 
4x_3(x_1^3 + x_2 x_3), & 
4x_2(x_1^3 + x_2 x_3)\\
3x_1^2+x_3^2, &
x_3+1, &
x_2+2x_1x_3 \\
3x_1^2-x_3^2, &
x_3-1, &
x_2-2x_1x_3 \\
8x_3^2(x_2+x_1 x_3^2), &
8(x_2+x_1 x_3^2), &
16 x_1 x_3(x_2+x_1 x_3^2)
\end{array}
\right)
$$

Now we can evaluate $D(g \circ f)({\bf x})$ at a particular points in $\mathbb{R}^3$

```

```{admonition} Example
:class: tip

$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1^2 + x_2^2 \\
x_1 x_2
\end{array}
\right)
$$
%
$$
g \colon
\left(
\begin{array}{c}
y_1 \\
y_2
\end{array}
\right)
\mapsto
(y_1+2 y_2)^2
$$
%
**Exercise:** Using the chain rule, find the total derivative of the composition $g \circ f$.

```

- The chain rule very powerful tool in computing complex derivatives (think backwards propagation in deep neural networks)

### Higher order derivatives

The higher order derivatives of the function in one dimension generalize naturally to the multivariate case, but the complexity of the needed matrixes and multidimensional arrays (*tensors*) grows fast.

```{admonition} Definition
:class: caution
:name: hessian

Let $A$ denote an open set in $\mathbb{R}^N$, and let $f \colon A \to \mathbb{R}$. Assume that $f$ is twice differentiable at ${\bf x} \in A$.

The total derivative of the gradient of function $f$ at point ${\bf x}$, $\nabla f({\bf x})$ is called the ***Hessian*** matrix of $f$ denoted by $Hf$ or $\nabla^2 f$, and is given by a $N \times N$ matrix
%
$$
Hf({\bf x}) = \nabla^2 f({\bf x}) = 
\left(
\begin{array}{ccc}
\frac{\partial^2 f}{\partial x_1 \partial x_1}({\bf x}) & 
\cdots &
\frac{\partial^2 f}{\partial x_1 \partial x_N}({\bf x}) \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_N \partial x_1}({\bf x}) &
\cdots & 
\frac{\partial^2 f}{\partial x_N \partial x_N}({\bf x})
\end{array}
\right)
$$
```
```{admonition} Fact
:class: important

For every ${\bf x} \in A \subset \mathbb{R}^N$ where $A$ is an open and $f \colon A \to \mathbb{R}^N$ is twice continuously differentiable, the Hessian matrix $\nabla^2 f({\bf x})$ is symmetric
```

```{admonition} Example
:class: tip

$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right)
\mapsto
(x_1 - 2 x_2)^2
$$
%
$$
\nabla^2 f({\bf x}) = 
\left(
\begin{array}{rr}
2, & -4 \\ -4, & 8
\end{array}
\right)
$$
%
**Exercise:** check
```

```{admonition} Example
:class: tip

$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right)
\mapsto
(x_1 - 2 x_2)^3
$$
%
$$
\nabla^2 f({\bf x}) = 
6 (x_1 - 2 x_2) 
\left(
\begin{array}{rr}
1, & -2 \\ -2, & 4
\end{array}
\right)
$$
%
**Exercise:** check
```

