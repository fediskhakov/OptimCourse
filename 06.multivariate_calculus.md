---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

````{admonition} Announcements & Reminders
:class: note
:class: dropdown

1. Quiz 1

2. The midterm is scheduled at **18:30 to 19:45** (1 hour + 15 min reading time) on **Monday, April 15** in **Melville Hall**

````


```{note}

- definition of derivative

- total derivative
- derivative in a particular direction
- steepest descent

- end with the logit example: compute hessian using matrix operations
- (to be continued in the lecture on convex optimization)
- problems set on solving FOCs from the exam (do not loose stationary points)
```

# üìñ Multivariate calculus

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>


> Today we continue to get comfortable with the multidimensional space of real numbers $\mathbb{R}^N$ 
> 
> Main goal: learn and practice multidimensional differentiation
> 
> - Refresh matrix operations (linear algebra)
> - Refresh definition of derivative and differentiability
> 


## Matrices and matrix arithmetic

### Matrix

```{admonition} Definition
:class: caution

A **matrix** is an array of numbers or variables which are organised into rows and columns

An $(n \times m)$ matrix takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$

- $n$ and $m$ are **dimensions** of the matrix A
```

An $(n \times m)$ matrix has $n$ rows and $m$ columns

Note that, while it is possible that $n=m$, it is also possible that $n \neq m$

```{admonition} Definition
:class: caution

Matrix with has the same number of column as rows, $n=m$, is called a **square matrix**.
```

### Scalars and vectors

```{admonition} Definition
:class: caution

A **scalar** is a real number $(a \in \mathbb{R})$
```

```{admonition} Definition
:class: caution

A **column vector** is an $(n \times 1)$ matrix of the form

$$
A=\left(\begin{array}{c}
a_{11} \\
a_{21} \\
a_{31} \\
\vdots \\
a_{n 1}
\end{array}\right)
$$

A **row vector** is a $(1 \times m)$ matrix of the form

$$
A=\left(\begin{array}{lllll}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m}
\end{array}\right) .
$$

```

- Vectors can be thought of as matrixes with one dimension equal to $1$
- Scalars can be thought of as $1 \times 1$ matrixes


## Matrix arithmetic

- Scalar multiplication of a matrix
- Matrix addition
- Matrix subtraction
- Matrix multiplication: The inner, or dot, product
- The transpose of a matrix and matrix symmetry
- The additive inverse of a matrix and the null matrix
- The multiplicative inverse of a matrix and the identity matrix
- Powers of matrices and the idempotent matrix

```{Note}

Many binary operations with matrices are not commutative! (order of operands is important)
```

Let $A$ is an $(n \times m)$ matrix takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$

We will assume that $a_{i j} \in \mathbb{R}$ for all

$$
(i, j) \in\{1,2, \cdots, n\} \times\{1,2, \cdots, m\} .
$$


### Scalar multiplication

Suppose that $c \in \mathbb{R}$.
The scalar pre-product and post-product of this constant with this matrix is given by

$$
c A & = A c =
\left(\begin{array}{ccccc}
c a_{11} & c a_{12} & c a_{13} & \cdots & c a_{1 m} \\
c a_{21} & c a_{22} & c a_{23} & \cdots & c a_{2 m} \\
c a_{31} & c a_{32} & c a_{33} & \cdots & c a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c a_{n 1} & c a_{n 2} & c a_{n 3} & \cdots & c a_{n m}
\end{array}\right)
$$

Note that $c A=A c$. As such, we can just talk about the **scalar product** of a constant with a matrix, without specifying the order in which the multiplication takes place.

```{admonition} Example
:class: tip

$$
2\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right)=\left(\begin{array}{ll}
2\cdot1 & 2\cdot1 \\
2\cdot1 & 2\cdot2
\end{array}\right)=\left(\begin{array}{ll}
2 & 2 \\
2 & 4
\end{array}\right)
$$
```

```{admonition} Example
:class: tip

$$
2\left(\begin{array}{cc}
-1 & -2 \\
3 & 1
\end{array}\right)=\left(\begin{array}{cc}
2\cdot(-1) & 2\cdot(-2) \\
2\cdot3 & 2\cdot1
\end{array}\right)=\left(\begin{array}{cc}
-2 & -4 \\
6 & 2
\end{array}\right)
$$
```

```{admonition} Example
:class: tip

$$
\left(\begin{array}{ccc}
1 & 2 & 0 \\
4 & -3 & 1
\end{array}\right) \cdot 2
=\left(\begin{array}{ccc}
3 & 6 & 0 \\
12 & -9 & 3
\end{array}\right)
$$
```

```{admonition} Example
:class: tip

$$
2\left(\begin{array}{lll}
0 & 1 & 2 \\
1 & 0 & 2
\end{array}\right)=
& =\left(\begin{array}{ccc}
0 & -\tfrac{1}{2} & -1 \\
-\tfrac{1}{2} & 0 & -1
\end{array}\right)
$$
```


### Matrix addition

The sum of two matrices is only defined if the two matrices have exactly the same dimensions.

```{admonition} Definition
:class: caution

Two matrixes are called **conformable** for a given operation if their dimensions are such that the operation is well defined.

```

Suppose that $B$ is an $(n \times m)$ matrix that takes the following form:

$$
B=\left(\begin{array}{ccccc}
b_{11} & b_{12} & b_{13} & \cdots & b_{1 m} \\
b_{21} & b_{22} & b_{23} & \cdots & b_{2 m} \\
b_{31} & b_{32} & b_{33} & \cdots & b_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
b_{n 1} & b_{n 2} & b_{n 3} & \cdots & b_{n m}
\end{array}\right)
$$


The **matrix sum** $(A+B)$ is an $(n \times m)$ matrix that takes the following form:

$$
A+B = B+A
=\left(\begin{array}{cccc}
a_{11}+b_{11} & a_{12}+b_{12} & \cdots & a_{1 m}+b_{1 m} \\
a_{21}+b_{21} & a_{22}+b_{22} & \cdots & a_{2 m}+b_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1}+b_{n 1} & a_{n 2}+b_{n 2} & \cdots & a_{n m}+b_{n m}
\end{array}\right)
$$

Note matrix summation is also commutative, $A+B=B+A$. 

**Exercise:** Convince yourself in this fact.


### Some examples of matrix addition

```{admonition} Fact
:class: important

Suppose that $A$ is an $(m \times n)$ matrix, $B$ is an $(n \times m)$ matrix and $C$ is an $(n \times p)$ matrix, where $m \neq n, m \neq p$ and $n \neq p$.

- Neither the matrix sum $A+B$ nor the matrix sum $B+A$ are defined.
- Neither the matrix sum $A+C$ nor the matrix sum $C+A$ are defined.
- Neither the matrix sum $B+C$ nor the matrix sum $C+B$ are defined.

```

```{admonition} Example
:class: tip

$$
\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right)+\left(\begin{array}{ll}
1 & 0 \\
1 & 2
\end{array}\right)=\left(\begin{array}{ll}
1+1 & 1+0 \\
1+1 & 2+2
\end{array}\right)=\left(\begin{array}{ll}
2 & 1 \\
2 & 4
\end{array}\right)
$$
```

```{admonition} Example
:class: tip

$$
\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right)+\left(\begin{array}{ll}
-1 & 3 \\
-2 & 1
\end{array}\right)=\left(\begin{array}{ll}
1+(-1) & 1+3 \\
1+(-2) & 2+1
\end{array}\right) 
 =\left(\begin{array}{cc}
0 & 4 \\
-1 & 3
\end{array}\right)
$$
```

```{admonition} Example
:class: tip

$$
\begin{gathered}
\left(\begin{array}{ccc}
1 & 2 & 0 \\
4 & -3 & -1
\end{array}\right)+\left(\begin{array}{lll}
0 & 1 & 2 \\
1 & 0 & 2
\end{array}\right) 
=\left(\begin{array}{ccc}
1 & 3 & 2 \\
5 & -3 & 1
\end{array}\right)
\end{gathered}
$$
```

### Matrix subtraction

Matrix subtraction involves a combination of (i) scalar multiplication of a matrix, and (ii) matrix addition.

As with matrix addition, the difference of two matrices is only defined if the two matrices have exactly the same dimensions.

Suppose that $A$ and $B$ are both $(n \times m)$ matrices. The difference between $A$ and $B$ is defined to be

$$
A-B=A+(-1) B
$$

In general, $A-B \neq B-A$

````{admonition} Example
:class: tip

Under what circumstances will $A-B=B-A$ ?


```{admonition} Solution
:class: dropdown

:::{math}
A-B = B-A \\
A = B - A + B \\
A + A = B + B \\
2A = 2B \\
A = B
:::

```
````


### Matrix multiplication

- The standard matrix product is the **dot, or inner, product** of two matrices.
- The dot product of two matrices is only defined for cases in which the number of columns of the first listed matrix is identical to the number of rows of the second listed matrix.
- If the dot product is defined, the solution matrix will have the same number of rows as the first listed matrix and the same number of columns as the second listed matrix.


```{admonition} Fact
:class: important

Matrix product is defined for two matrices $A$ and $B$ if the number of columns of $A$ is equal to the number of rows of $B$.

Suppose that $X$ is an $(m \times n)$ matrix, $Y$ is an $(n \times m)$ matrix and $Z$ is an $(n \times p)$ matrix, where $m \neq n, m \neq p$ and $n \neq p$.

- The matrix product $X Y$ is defined and will be an $(m \times m)$ matrix.
- The matrix product $Y X$ is defined and will be an $(n \times n)$ matrix.
- The matrix product $X Z$ is defined and will be an $(m \times p)$ matrix.
- The matrix products $Z X, Y Z$ and $Z Y$ are not defined.

```

Let $B$ be an $(m \times p)$ matrix that takes the following form:

$$
B=\left(\begin{array}{ccccc}
b_{11} & b_{12} & b_{13} & \cdots & b_{1 p} \\
b_{21} & b_{22} & b_{23} & \cdots & b_{2 p} \\
b_{31} & b_{32} & b_{33} & \cdots & b_{3 p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
b_{m 1} & b_{m 2} & b_{m 3} & \cdots & b_{m p}
\end{array}\right)
$$


The **matrix product** $A B$ is defined and will be an $(n \times p)$ matrix. The solution matrix is given by

$$
\begin{array}{rl}
AB
&=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n m}
\end{array}\right) \cdot\left(\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1 p} \\
b_{21} & b_{22} & \cdots & b_{2 p} \\
\vdots & \vdots & \ddots & \vdots \\
b_{m 1} & b_{m 2} & \cdots & b_{m p}
\end{array}\right) \\[20pt]
&=\left(\begin{array}{ccccc}
\sum_{k=1}^{m} a_{1 k} b_{k 1} & \sum_{k=1}^{m} a_{1 k} b_{k 2} & \cdots & \sum_{k=1}^{m} a_{1 k} b_{k p} \\
\sum_{k=1}^{m} a_{2 k} b_{k 1} & \sum_{k=1}^{m} a_{2 k} b_{k 2} & \cdots & \sum_{k=1}^{m} a_{2 k} b_{k p} \\
\vdots & \vdots & \ddots & \vdots \\
\sum_{k=1}^{m} a_{n k} b_{k 1} & \sum_{k=1}^{m} a_{n k} b_{k 2} & \cdots & \sum_{k=1}^{m} a_{n k} b_{k p}
\end{array}\right)
\end{array}
$$


```{figure} _static/plots/mmult2.png
:width: 60%

Matrix multiplication: conformable matrices
```

```{admonition} Definition
:class: caution

A dot product of two vectors $x = (x_1,\dots,x_N) \in \mathbb{R}^N$ and $y = (y_1,\dots,y_N) \in \mathbb{R}^N$ is given by
:::{math}
x \cdot y = \sum_{i=1}^N x_i y_i
:::
```

```{figure} _static/plots/mmult1.png

Matrix multiplication: combination of dot products of vectors
```

```{admonition} Fact
:class: important

Matrix multiplication is not commutative in general

$$
A B \neq B A
$$

```

```{admonition} Example
:class: tip

$$
\begin{array}{l}
\left(\begin{array}{cc}
1 & 2 \\
-2 & 4
\end{array}\right) \cdot\left(\begin{array}{lll}
0 & 2 & 2 \\
1 & 0 & 5
\end{array}\right) \\
=\left(\begin{array}{ccc}
(1)(0)+(2)(1) & (1)(2)+(2)(0) & (1)(2)+(2)(5) \\
(-2)(0)+(4)(1) & (-2)(2)+(4)(0) & (-2)(2)+(4)(5)
\end{array}\right) \\
=\left(\begin{array}{ccc}
0+2 & 2+0 & 2+10 \\
0+4 & -4+0 & -4+20
\end{array}\right) \\
=\left(\begin{array}{ccc}
2 & 2 & 12 \\
4 & -4 & 16
\end{array}\right)
\end{array}
$$
```

```{admonition} Example
:class: tip

$$
\begin{aligned}
A C &=\left(\begin{array}{cc}
1 & 2 \\
-2 & 4
\end{array}\right) \cdot\left(\begin{array}{cc}
3 & -2 \\
5 & 0
\end{array}\right) \\
&=\left(\begin{array}{cc}
(1)(3)+(2)(5) & (1)(-2)+(2)(0) \\
(-2)(3)+(4)(5) & (-2)(-2)+(4)(0)
\end{array}\right) \\
&=\left(\begin{array}{cc}
3+10 & -2+0 \\
-6+20 & 4+0
\end{array}\right) \\
&=\left(\begin{array}{cc}
13 & -2 \\
14 & 4
\end{array}\right)
\end{aligned}
$$

$$
\begin{aligned}
C A &=\left(\begin{array}{cc}
3 & -2 \\
5 & 0
\end{array}\right) \cdot\left(\begin{array}{cc}
1 & 2 \\
-2 & 4
\end{array}\right) \\
&=\left(\begin{array}{cc}
(3)(1)+(-2)(-2) & (3)(2)+(-2)(4) \\
(5)(1)+(0)(-2) & (5)(2)+(0)(4)
\end{array}\right) \\
&=\left(\begin{array}{cc}
3+4 & 6+(-8) \\
5+0 & 10+0
\end{array}\right) \\
&=\left(\begin{array}{cc}
7 & -2 \\
5 & 10
\end{array}\right)
\end{aligned}
$$

Note that $A C \neq C A$.
```

```{Note}

Do not confuse various names for the two types of matrix/vector multiplication:

**Dot/scalar/inner product** denoted with a dot $\cdot$ is the sum of products of the vector coordinates, outputs a scalar

**Vector/cross product** denoted with a cross $\times$ is a vector that is orthogonal to the two input vectors with a length equal to the area of the parallelogram spanned by the two input vectors (not covered in this course)
```


### Matrix transposition

Suppose that $A$ is an $(n \times m)$ matrix. The **transpose** of the matrix $A$, which is denoted by $A^{T}$, is the $(m \times n)$ matrix that is formed by taking the rows of $A$ and turning them into columns, without changing their order. In other words, the $i$ th column of $A^{T}$ is the ith row of $A$. This also means that the jth row of $A^{T}$ is the $j$ th column of $A$.

The **transpose** of the matrix $A$ is the $(m \times n)$ matrix that takes the following form:

$$
A^{T}=\left(\begin{array}{ccccc}
a_{11} & a_{21} & a_{31} & \cdots & a_{n 1} \\
a_{12} & a_{22} & a_{32} & \cdots & a_{n 2} \\
a_{13} & a_{23} & a_{33} & \cdots & a_{n 3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{1 m} & a_{2 m} & a_{3 m} & \cdots & a_{n m}
\end{array}\right)
$$


```{admonition} Example
:class: tip

- If $
x=\left(\begin{array}{l}
1 \\
3 \\
5
\end{array}\right)
$ then $X^{T}=(1,3,5)$.  
<br></br>

- If $
Y=\left(\begin{array}{ll}
2 & 3 \\
5 & 9 \\
7 & 6
\end{array}\right)
$ then $
Y^{T}=\left(\begin{array}{lll}
2 & 5 & 7 \\
3 & 9 & 6
\end{array}\right)
$.  
<br></br>

- If $
Z=\left(\begin{array}{ccc}
1 & 3 & 7 \\
4 & 5 & 11 \\
6 & 8 & 10
\end{array}\right)
$ then $
Z^{T}=\left(\begin{array}{ccc}
1 & 4 & 6 \\
3 & 5 & 8 \\
7 & 11 & 10
\end{array}\right)
$.  
```

In general, $A^{T} \neq A$.


```{admonition} Definition
:class: caution

If matrix $A$ is such that $A^{T}=A$, we say that $A$ is a **symmetric matrix**.

```


```{admonition} Example
:class: tip

$
A=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right)=A^{T}
$

$
B=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)=B^{T} 
$

$
C=\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right)=C^{T} 
$

$
D=\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right)=D^{T} 
$
```

### Null matrices

```{admonition} Definition
:class: caution

A **null matrix** (or vector) is a matrix that consists solely of zeroes.

```

For example, the $(3 \times 3)$ null matrix is

$$
0=\left(\begin{array}{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right)
$$


```{admonition} Fact
:class: important

For an $(n \times m)$ null matrix and an $(n \times m)$ matrix $A$, it holds $A+0=0+A=A$.

```


```{admonition} Definition
:class: caution

Suppose that $A$ is an $(n \times m)$ matrix and $\mathbb{0}$ is the $(n \times m)$ null matrix.
The $(n \times m)$ matrix $B$ is called the **additive inverse** of $A$ if and only if $A+B=B+A=0$.

```

The additive inverse of $A$ is

$$
B=-A=\left(\begin{array}{cccc}
-a_{11} & -a_{12} & \cdots & -a_{1 m} \\
-a_{21} & -a_{22} & \cdots & -a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
-a_{n 1} & -a_{n 2} & \cdots & -a_{n m}
\end{array}\right)
$$


### Identity matrices

```{admonition} Definition
:class: caution

An **identity matrix** is a square matrix that has ones on the main (north-west to south-east) diagonal and zeros everywhere else.

```

For example, the $(2 \times 2)$ identity matrix is

$$
I=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)
$$

```{admonition} Fact
:class: important

For an $(n \times n)$ identity matrix $I$ it holds:
- If $A$ is an $(n \times n)$ matrix, then $A I=I A=A$.
- If $A$ is an $(m \times n)$ matrix, then $A I=A$.
- If $A$ is an $(n \times m)$ matrix, then $I A=A$.

```

```{admonition} Definition
:class: caution

Let $A$ be an $(n \times n)$ matrix and $I$ be the $(n \times n)$ identity matrix.
The $(n \times n)$ matrix $B$ is the **multiplicative inverse** (usually just referred to as the inverse) of $A$ if and only if $A B=B A=I$.
```

- *Only square* matrices have any chance of having a multiplicative inverse
- Some, but *not all*, square matrices will have a multiplicative inverse

```{admonition} Definition
:class: caution

- A square matrix that has an inverse is said to be **non-singular**.
- A square matrix that does not have an inverse is said to be **singular**.
```

- We will talk about methods for determining whether or not a matrix is non-singular later in this course

```{admonition} Fact
:class: important

> The transpose of the inverse is equal to the inverse of the transpose

If $A$ is a non-singular square matrix whose multiplicative inverse is $A^{-1}$, then we have
:::{math}
\left(A^{-1}\right)^{T}=\left(A^{T}\right)^{-1}
:::
```


```{admonition} Example
:class: tip

Note that

$$
\begin{aligned}
A B &=\left(\begin{array}{ll}
1 & 2 \\
3 & 7
\end{array}\right) \cdot\left(\begin{array}{cc}
7 & -2 \\
-3 & 1
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
(1)(7)+(2)(-3) & (1)(-2)+(2)(1) \\
(3)(7)+(7)(-3) & (3)(-2)+(7)(1)
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
7+(-6) & -2+2 \\
21+(-21) & -6+7
\end{array}\right) \\[10pt]
&=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right) \\[10pt]
&= I
\end{aligned}
$$

Note that

$$
\begin{aligned}
B A &=\left(\begin{array}{cc}
7 & -2 \\
-3 & 1
\end{array}\right) \cdot\left(\begin{array}{cc}
1 & 2 \\
3 & 7
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
(7)(1)+(-2)(3) & (7)(2)+(-2)(7) \\
(-3)(1)+(1)(3) & (-3)(2)+(1)(7)
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
7+(-6) & 14+(-14) \\
-3+3 & -6+7
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
1 & 0 \\
0 & 1
\end{array}\right) \\[10pt]
&= I
\end{aligned}
$$

Since $A B=B A=I$, we can conclude that $A^{-1}=B$.

{cite:ps}`haeussler1987` (p. 278, Example 1)
```







## Univariate differentiation


### Derivatives

```{admonition} Definition
:class: caution

Let $f \colon X \to \mathbb{R}$ and let $x \in (a, b)$

Let $H$ be the set of all convergent sequences $\{h_n\}$ such that 
$h_n \ne 0$ and $h_n \to 0$

If there exists a constant $f'(x)$ such that
%
$$
\frac{f(x + h_n) - f(x)}{h_n} \to f'(x)
$$
%
for every $\{h_n\} \in H$, then 

- $f$ is said to be **differentiable** at $x$
- $f'(x)$ is called the **derivative** of $f$ at $x$ 

```
```{figure} _static/plots/derivative.png
:width: 60%
```

We will use the following notation for derivative interchangeably
:::{math}
f'(x) \quad \frac{df}{dx}  \quad \frac{d}{dx}f(x)
:::

```{admonition} Definition
:class: caution

A function $f \colon X \to \mathbb{R}$ is called differentiable (on $X$) if it is differentiable in all points of $X$

```

```{admonition} Example
:class: tip

Let $f \colon \mathbb{R} \to \mathbb{R}$ be defined by $f(x) = x^2$

Fix any $x \in \mathbb{R}$ and any $h_n \to 0$ with $n \to \infty$

We have
%    
$$
\frac{f(x + h_n) - f(x)}{h_n} 
= \frac{(x + h_n)^2 - x^2}{h_n} =
$$
$$
= \frac{x^2 + 2xh_n + h_n^2 - x^2}{h_n}
= 2x + h_n
$$
$$
\text{therefore }
f'(x)
= \lim_{n \to \infty} (2x + h_n)
= 2x + \lim_{n \to \infty} h_n
= 2x
$$
%
```

```{admonition} Example
:class: tip

Let $f \colon \mathbb{R} \to \mathbb{R}$ be defined by $f(x) = |x|$

This function is not differentiable at $x=0$

Indeed, if $h_n = 1/n$, then
%
$$
\frac{f(0 + h_n) - f(0)}{h_n} 
= \frac{|0 + 1/n| - |0|}{1/n} \to 1
$$
%
On the other hand, if $h_n = -1/n$, then
%
$$
\frac{f(0 + h_n) - f(0)}{h_n} 
= \frac{|0 - 1/n| - |0|}{-1/n} \to -1
$$

Therefore the definition of the derivative is not satisfied
```

### Heine and Cauchy definitions of the limit for functions

As a side note, let's mention that there are two ways to define the limit of a function $f$.

In the last lecture when talking about continuity and just above in the definition of the derivative we implicitly used the definition of function limit defined through limit of sequences, due to *Eduard Heine*


```{admonition} Definition (Limit of a function due to Heine)
:class: caution

Given the function $f: A \subset \mathbb{R}^N \to \mathbb{R}$, $a \in \mathbb{R}$ is a limit of $f(x)$ as $x \to x_0 \in A$ if
:::{math}
\forall \{x_n\} \in A \colon \lim_{n \to \infty} x_n = x_0 \in A
\quad \implies \quad
f(x_n) \to a 
:::
```

The alternative definition of the limit is due to *Augustin-Louis Cauchy*, also known as $\epsilon$-$\delta$ definition

```{admonition} Definition (Limit of a function due to Cauchy)
:class: caution

Given the function $f: A \subset \mathbb{R}^N \to \mathbb{R}$, $a \in \mathbb{R}$ is a limit of $f(x)$ as $x \to x_0 \in A$ if
:::{math}
\forall \, \epsilon > 0, \;
\exists \, \delta >0 \; 
\text{ such that } 0 < |x ‚àí x_0| < \delta \implies |f(x) ‚àí a| < \epsilon
:::
```

- note the similarity to the definition of the sequence
- note that in the $\epsilon$-ball around $x_0$ the point $x_0$ is excluded


```{figure} _static/plots/func_lim_epsilondelta.png
:width: 40%

$\lim_{x \to x_0} f(x) = L$ under the Cauchy definition
```



```{admonition} Fact
:class: important

Cauchy and Heine definitions of the function limit are equivalent
```

Therefore we can use the same notation of the definition of the limit of a function
:::{math}
\lim_{x \to x_0} f(x) = a \quad \text{or} \quad f(x) \to a \; \text{as} \;  x \to x_0
:::

### Differentiation rules

```{admonition} Fact
:class: important

For functions $y=f(x)$ and $g(x)$, and constant $c$, we have

- Scalar Multiplication Rule
:::{math}
f^{\prime}(x)=c g^{\prime}(x)
:::
- Summation Rule: 
:::{math}
f^{\prime}(x)=g^{\prime}(x)+h^{\prime}(x)
:::
- Product Rule: 
:::{math}
f^{\prime}(x)=g^{\prime}(x) h(x)+h^{\prime}(x) g(x)
:::
- Quotient Rule: 
:::{math}
f^{\prime}(x)=\frac{g^{\prime}(x) h(x)-h^{\prime}(x) g(x)}{[h(x)]^{2}}
:::
- Chain Rule: 
:::{math}
f^{\prime}(x)=g^{\prime}(h(x)) h^{\prime}(x)
:::
- The Inverse Function Rule:
:::{math}
\frac{d x}{d y}=\frac{1}{\frac{d y}{d x}}
:::

The latter rule requires that $f(x)$ had a well defined inverse function $x=f^{-1}(y)$ and that $\frac{d y}{d x}$ is not equal to zero.

```


### Differentiability and continuity

Just a reminder: by definition a function $f: X \to \mathbb{R}$ is continuous in $a$ if $f(x) \to f(a)$ as $x \to a$

```{admonition} Fact
:class: important

A differentiable function $f \colon X \to \mathbb{R}$ is continuous on $X$.

Converse is not true in general.

:::{math}
\text{Differentiability} \quad
\begin{array}{c}
\Longrightarrow \\
\not \Longleftarrow
\end{array} \quad
\text{Continuity}
:::
```

````{admonition} Proof
:class: dropdown

Consider a function $f: X \longrightarrow \mathbb{R}$ where $X \subseteq \mathbb{R}$.
Suppose that

$$
\lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right)
$$

exists.

We want to show that this implies that $f(x)$ is continuous at the point $a \in X$.  The following proof of this proposition is drawn from {cite:ps}`ayres2013` (Chapter 8, Solved Problem 2).


First, note that

$$
\begin{gathered}
\lim _{h \rightarrow 0}(f(a+h)-f(a))=\lim _{h \rightarrow 0}\left\{\left(\frac{h}{h}\right)(f(a+h)-f(a))\right\} \\
=\lim _{h \rightarrow 0}\left\{h\left(\frac{f(a+h)-f(a)}{h}\right)\right\} \\
=\lim _{h \rightarrow 0}(h) \lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right) \\
=(0)\left(\lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right)\right) \\
=0 .
\end{gathered}
$$

Thus we have

$$
\lim _{h \rightarrow 0}(f(a+h)-f(a))=0 .
$$

Now note that

$$
\begin{gathered}
\lim _{h \rightarrow 0}(f(a+h)-f(a))=\lim _{h \rightarrow 0} f(a+h)-\lim _{h \rightarrow 0} f(a) \\
=\left(\lim _{h \rightarrow 0} f(a+h)\right)-f(a)
\end{gathered}
$$

Upon combining these two results, we obtain

$$
\left(\lim _{h \rightarrow 0} f(a+h)\right)-f(a)=0 \Longleftrightarrow \lim _{h \rightarrow 0} f(a+h)=f(a) .
$$

Finally, note that

$$
\lim _{x \rightarrow a} f(x)=\lim _{h \rightarrow 0} f(a+h) .
$$

Thus we have

$$
\lim _{x \rightarrow a} f(x)=f(a)
$$

This means that $f(x)$ is continuous at the point $x=a$.

---

Consider the function

$$
f(x)=\left\{\begin{array}{cc}
2 x & \text { if } x \leqslant 1 \\
\frac{1}{2} x+\frac{3}{2} & \text { if } x \geq 1
\end{array}\right.
$$

(There is no problem with this double definition at the point $x=1$ because the two parts of the function are equal at that point.)

This function is continuous at $x=1$ because

$$
\lim _{x \rightarrow 1} 2 x=2=\lim _{x \rightarrow 1}\left(\frac{1}{2} x+\frac{3}{2}\right)
$$

and

$$
f(1)=2
$$


However, this function is not differentiable at $x=1$ because

$$
\lim _{h \uparrow 1} \frac{f(1+h)-f(1)}{h}=2
$$

and

$$
\lim _{h \downarrow 1} \frac{f(1+h)-f(1)}{h}=\frac{1}{2}
$$

Since

$$
\lim _{h \uparrow 1} \frac{f(1+h)-f(1)}{h} \neq \lim _{h \downarrow 1} \frac{f(1+h)-f(1)}{h}
$$

we know that

$$
\lim _{h \rightarrow 1} \frac{f(1+h)-f(1)}{h}
$$

does not exist.

$\blacksquare$

````

## Higher order derivatives

Consider a derivative $f'(x)$ of a function $f(x)$ as a *new* function $g(x)$.  We can then take the derivative of $g(x)$ to obtain a *second order derivative* of $f(x)$, denoted by $f''(x)$.

```{admonition} Definition
:class: caution

Let $f \colon X \to \mathbb{R}$ and let $x \in (a, b)$

A second order derivative of $f$ at $x$ denoted $f''(x)$ or $\frac{d^2 f}{dxdx}$ is the (first order) derivative of the function $f'(x)$, if it exists

More generally, the $k$-th order derivative of $f$ at $x$ denoted $f^{(m)}(x)$ or $\frac{d^k f}{dx \dots dx}$ is the derivative of function $\frac{d^{k-1} f}{dx \dots dx}$, if it exists

The function $f: X \to \mathbb{R}$ is said to be of differentiability class $C^m$ if derivatives $f'$, $f''$, $\dots$, $f^{(m)}$ exist and are continuous on $X$

```


(ref-taylor)=
## Taylor series

```{admonition} Fact
:class: important

Consider $f: X \to \mathbb{R}$ and let $f$ to be a $C^m$ function. Assume also that $f^{(m+1)}$ exists, although may not necessarily be continuous.

For any $x,a \in X$ there is such $z$ between $a$ and $x$ that
:::{math}
f(x) = f(a) + f'(x)(x-a) + \frac{f''(x)}{2!}(x-a)^2 + \dots + \frac{f^{(m)}(x)}{m!}(x-a)^m + R_m(x),
:::
where the remainder is given by
:::{math}
R_m(x) = \frac{f^{(m+1)}(z)(x-a)^{m+1}}{(m+1)!} = o\big((x-a)^m\big)
:::
```

```{admonition} Definition
:class: caution

Little-o notation is used to describe functions that approach zero faster than a given function
:::{math}
f(x) = o\big(g(x)\big) \; \text{as} \; x \to a \quad \iff \quad \lim_{x \to a} \frac{f(x)}{g(x)} = 0
:::
```



Loosely speaking, if $f \colon \mathbb{R} \to \mathbb{R}$ is suitably
differentiable at $a$, then
%
$$
f(x) \approx f(a) + f'(a)(x-a) 
$$
%
for $x$ very close to $a$,
%
$$
f(x) \approx f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 
$$
%
on a slightly wider interval, etc.

These are the 1st and 2nd order **Taylor series approximations**
to $f$ at $a$ respectively

As the order goes higher we get better approximation

```{figure} _static/plots/taylor_4.png
:width: 60%

4th order Taylor series for $f(x) = \sin(x)/x$ at 0
```

```{figure} _static/plots/taylor_6.png
:width: 60%

6th order Taylor series for $f(x) = \sin(x)/x$ at 0
```

```{figure} _static/plots/taylor_8.png
:width: 60%

8th order Taylor series for $f(x) = \sin(x)/x$ at 0
```

```{figure} _static/plots/taylor_10.png
:width: 60%

10th order Taylor series for $f(x) = \sin(x)/x$ at 0
```










## Multivariate differentiation


### Functions from $\mathbb{R}^N$ to $\mathbb{R}$

Extending the one-dimensional definition of derivative to multi-variate case is tricky!

With multiple variables, we have to decide *which one* to increment with the elements of the sequence $\{h_n\}$

We need a $i$-th unit vector of the form $(0, \dots, 0, 1, 0, \dots, 0)$ where $1$ is in the $i$-th position to choose one particular variable

```{admonition} Definition
:class: caution
:name: partial-derivative-R1

Let $f \colon A \to \mathbb{R}$ and let $x \in A \subset \mathbb{R}^N$.
Denote $e_i$ the $i$-th unit vector in $\mathbb{R}^N$, i.e. $e_i = (0, \dots, 0, 1, 0, \dots, 0)$ where $1$ is in the $i$-th position.

If there exists a constant $a \in \mathbb{R}$ such that 
%
$$
\frac{f(x + h_n e_i) - f(x)}{h_n} \to a
$$
%
for every sequence $\{h_n\}$, $h_n \in \mathbb{R}$, 
such that $h_n \ne 0$ and $h_n \to 0$ as $n \to \infty$, then 
$a$ is called a **partial derivative** of $f$ with respect to $x_i$ at $x$, and is denoted 

$$
f'_i(x) \quad \text{or} \quad \frac{\partial f}{\partial x_i}(x)
$$

```
- Note the slight difference in notation $\frac{d f}{d x}(x)$ of univariate derivative and $\frac{\partial f}{\partial x_i}(x)$ of partial derivative
- Partial derivatives behave just like regular derivatives of the univariate functions, assuming all variables except one are treated as constants
- Usual rules of differentiation apply

```{admonition} Example
:class: tip


EXAMPLE of partial derivative and operations with it
```

However, the partial derivatives do not get it there all the way:
- full analogue of the multivariate derivative to the univariate one should include *all sequences converging to zero* in the $\mathbb{R}^N$ space?
- what is the analogue of differentiability in the multivariate case, with the same implication to the continuety?

```{admonition} Definition
:class: caution
:name: jacobian-R1

Let 
- $A \subset \mathbb{R}^N$ be an open set in $N$-dimensional space
- $f \colon A \to \mathbb{R}$ be multivariate function
- $x \in A$, and so $\exists \epsilon \colon B_{\epsilon}(x) \subset A$

Then if there exists a vector $g \in \mathbb{R}^N$ such that
%
$$
\frac{f(x + h_n) - f(x) - g \cdot h_n}{\|h_n\|} \to 0
$$
%
for all converging to zero sequences $\{h_n\}$, $h_n \in B_{\epsilon}(0) \setminus \{0\}$, $h_n \to 0$,
then 
- $f$ is said to be **differentiable** at $x$
- vector $g$ is called the **gradient** of $f$ at $x$, and is denoted by $\nabla f(x)$ or simply $f'(x)$
```

- note the dot product of $g$ and $h_n$ in the definition
- the gradient is a vector of partial derivatives

```{admonition} Fact
:class: important

If a function $f \colon \mathbb{R}^N \to \mathbb{R}$ is differentiable at $x$, then all partial derivatives at $x$ exist and the gradient is given by
%
$$
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}(x), \dots, \frac{\partial f}{\partial x_N}(x) \right)
$$
%
```

````{admonition} Proof
:class: dropdown

The idea of the proof: consider all converging sequences in $\mathbb{R}^N$ and corresponding set of sequences in each dimension, component-wise.
````


```{admonition} Example
:class: tip

EXAMPLE of gradient
```

```{admonition} Fact
:class: important

Sufficient conditions for differentiability of a function $f \colon \mathbb{R}^N \to \mathbb{R}$ at $x$ are that all partial derivatives exist and are continuous in $x$
```

````{admonition} Proof
:class: dropdown

The idea of the proof: consider all converging sequences in $\mathbb{R}^N$ as composed component-wise from the converging sequences in each dimension, 
and verify that due to continuity, the vector of partial derivatives forms the vector $g$ required by the definition of differentiability.

````

### Directional derivatives

To understand the role of the gradient of $\mathbb{R}^N \to \mathbb{R}$ function, we need to introduce the concept of the directional derivative

Direction in $\mathbb{R}^N$ can be given by any vector, after all vector has
- absolute value (length)
- direction
But we should focus on the *unit vectors*, i.e. vectors of length 1

```{admonition} Definition
:class: caution

Let 
- $A \subset \mathbb{R}^N$ be an open set in $N$-dimensional space
- $f \colon A \to \mathbb{R}$ be multivariate function
- $x \in A$, and so $\exists \epsilon \colon B_{\epsilon}(x) \subset A$
- $v \in \mathbb{R}^N$ is a unit vector, i.e. $\|v\| = 1$

Then if there exists a constant $D_v f(x) \in \mathbb{R}$ such that
:::{math}
\lim_{h \to 0} \frac{f(x + hv) - f(x)}{h} = D_v f(x)
:::
then it is called a **directional derivative** of $f$ at $x$ in the direction of $v$
```
- note that the step size $h$ here is scalar
- the directional derivative is a scalar as well
- the whole expression $\frac{f(x + hv) - f(x)}{h}$ can be thought of a univariate function of $h$
- the directional derivative is a simple (univariate) derivative of this function

```{admonition} Example
:class: tip

Directional derivative
```

```{admonition} Fact
:class: important

Directional derivative of a differentiable function $f \colon \mathbb{R}^N \to \mathbb{R}$ at $x$ in given by
:::{math}
D_v f(x) = \nabla f(x) \cdot v = \sum_{i=1}^N \frac{\partial f}{\partial x_i}(x) v_i
:::
```

````{admonition} Proof
:class: dropdown

Main idea: carefully convert the limit from the definition of differentiability of $f$ to the univariate limit in the definition of the directional derivative. The latter happens to equal teh dot product of the gradient and the direction vector.
````

```{admonition} Example
:class: tip

Check previous example using the new formula
```

```{admonition} Fact
:class: important

The direction of the fastest ascent of a differentiable function $f \colon \mathbb{R}^N \to \mathbb{R}$ at $x$ is given by the direction of its gradient $\nabla f(x)$.
```

````{admonition} Proof
:class: dropdown

First, Cauchy‚ÄìBunyakovsky‚ÄìSchwarz inequality for a Euclidean space $\mathbb{R}^N$ states that for any two vectors $u, v \in \mathbb{R}^N$ we have
:::{math}
|u \cdot v| \leqslant \|u\| \|v\|
:::
where "$\cdot$" is dot product and $\|x\| = \sqrt{x \cdot x}$ is the standard Euclidean norm.

Consider the case when at least one element of $\nabla f(x)$ is positive, otherwise $D_v f(x) = 0$ implying that the function does not increase in any direction.

For any unit direction vector $v$ (in the same general direction as $\nabla f(x)$ so that $\nabla f(x) \cdot v >0$), we have
:::{math}
D_v f(x) = \nabla f(x) \cdot v \leqslant \|\nabla f(x)\| \|v\| = \|\nabla f(x)\|
:::
The inequality is due to Cauchy‚ÄìBunyakovsky‚ÄìSchwarz.

Now consider $v = \frac{\nabla f(x)}{\|\nabla f(x)\|}$. For this direction vector we have
:::{math}
D_v f(x) = \nabla f(x) \cdot \frac{\nabla f(x)}{\|\nabla f(x)\|} =
\frac{\nabla f(x) \cdot \nabla f(x)}{\|\nabla f(x)\|} =
\frac{(\|\nabla f(x)\|)^2}{\|\nabla f(x)\|} = \|\nabla f(x)\|
:::

Thus, the upper bound on $D_v f(x)$ is achieved when vector $v$ has the same direction as gradient vector $\nabla f(x)$.
$\blacksquare$
````

- Steepest descent algorithm uses the opposite direction of the gradient
- Stochastic steepest descent algorithm uses the same idea (ML and AI applications!)


### Functions from $\mathbb{R}^N$ to $\mathbb{R}^K$

Let's now consider more complex case: functions from multi-dimensional space to multi-dimensional space

```{admonition} Definition
:class: caution

The *vector-valued function* $f \colon \mathbb{R}^N \to \mathbb{R}^K$
is a tuple of $K$ functions $f_j \colon \mathbb{R}^N \to \mathbb{R}$, $j \in \{1, \dots, K\}$
```

```{admonition} Definition
:class: caution
:name: jacobian-RK

Let 
- $A \subset \mathbb{R}^N$ be an open set in $N$-dimensional space.
- $f \colon A \to \mathbb{R}^K$ be a $K$ dimensional vector function
- $x \in A$, and so $\exists \epsilon \colon B_{\epsilon}(x) \subset A$

Then if there exists a $(K \times N)$ matrix $J$ such that
%
$$
\frac{\|f(x + h_n) - f(x) - J h_n\|}{\|h_n\|} \to 0
$$
%
for all converging to zero sequences $\{h_n\}$, $h_n \in B_{\epsilon}(0) \setminus \{0\}$, $h_n \to 0$,
then 

- $f$ is said to be **differentiable** at $x$
- matrix $J$ is called the **total derivative** (or **Jacobian** matrix) of $f$ at $x$, and is denoted by $Df(x)$ or simply $f'(x)$
```
- in this definition $h_n$ is also a converging to zero sequence in $\mathbb{R}^N$
- the different is that in the numerator we have to use the norm to measure the distance in the $\mathbb{R}^K$ space
- instead of gradient vector $\nabla f$ we have a $(K \times N)$ Jacobian matrix $J$

Similar to how the gradient, Jacobian is composed of partial derivatives

```{admonition} Fact
:class: important

If a vector-valued function $f \colon \mathbb{R}^N \to \mathbb{R}^K$ is differentiable at $x$, then all partial derivatives at $x$ exist and the total derivative (Jacobian) is given by
%
$$
Df(x) =
\left(
\begin{array}{ccc}
\frac{\partial f_1}{\partial x_1}(x) & 
\cdots &
\frac{\partial f_1}{\partial x_N}(x) \\
\vdots & \ddots & \vdots \\
\frac{\partial f_K}{\partial x_1}(x) &
\cdots & 
\frac{\partial f_K}{\partial x_N}(x)
\end{array}
\right)
$$
%
```
- note that partial derivatives of a vector-valued function $f$ are indexed with two subscripts: $i$ for $f$ and $j$ for $x$
- index $i \in \{1,\dots,K\}$ indexes functions and rows of the Jacobian matrix
- index $j \in \{1,\dots,N\}$ indexes variables and columns of the Jacobian matrix
- Jacobian matrix is $(K \times N)$, i.e. number of functions in the tuple by the number of variables
- the gradient is a special case of Jacobian with $K=1$, as expected!

The sufficient conditions of differentiability are similar for vector-valued functions

```{admonition} Fact
:class: important

Sufficient conditions for differentiability of a function $f \colon \mathbb{R}^N \to \mathbb{R}^K$ at $x$ are that all partial derivatives exist and are continuous in $x$
```

Total derivative of $f \colon \mathbb{R}^N \to \mathbb{R}^K$ defines a linear map $J \colon \mathbb{R}^N \to \mathbb{R}^K$ given by $K \times N$ matrix $Df(x)$ which gives a *affine* approximation of function $f$ by a tangent hyperplane at point $x$.
This is similar to the tangent line to a one-dimensional function determined by a derivative at any given point.


```{admonition} Example
:class: tip

%
$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1^2 + x_2 x_3 \\
x_1+x_2^2+x_3^3
\end{array}
\right)
$$
%
Partial derivatives are
%
$$
\frac{\partial f_1}{\partial x_1} = 2 x_1, \;
\frac{\partial f_1}{\partial x_2} = x_3, \;
\frac{\partial f_1}{\partial x_3} = x_2 
$$
%
$$
\frac{\partial f_2}{\partial x_1} = 1, \;
\frac{\partial f_2}{\partial x_2} = 2x_2, \;
\frac{\partial f_2}{\partial x_3} = 3x_3^2
$$
%
Jacobian matrix is
%
$$
Df(x) = 
\left(
\begin{array}{rrr}
2x_1, & x_3, & x_2 \\
1, & 2x_2, & 3x_3^2
\end{array}
\right)
$$
%
Now, evaluating at a particular points (bold font denotes vectors in \mathbb{R}^3)
%
$$
Df({\bf 0}) = 
\left(
\begin{array}{rrr}
0, & 0, & 0 \\
1, & 0, & 0
\end{array}
\right)
$$
%
$$
Df({\bf 1}) = 
\left(
\begin{array}{rrr}
2, & 1, & 1 \\
1, & 2, & 3
\end{array}
\right)
$$
%
$$
Df((1,2,3)') = 
\left(
\begin{array}{rrr}
2, & 3, & 2 \\
1, & 4, & 27
\end{array}
\right)
$$
%

```

```{admonition} Example
:class: tip

%
$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1 + x_1 x_2 x_3 \\
x_1+2x_2-x_3 \\
x_1+2x_3 \\
x_2x_3
\end{array}
\right)
$$
%

**Exercise:** Derive Jacobian maxtix of $f$ and compute the  total derivative of $f$ at $x = (1, 0, 2)$

```



### Compositions of multivariate functions

- first correctly define superposition in different combinations

- fact: differentiability of compositions (follows form differentiability of components) and chain rule

- fact: partial derivatives of compositions





### Rules of differentiation

```{admonition} Fact
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and let $f, g \colon A \to \mathbb{R}^K$ be differentiable functions at $x \in A$.

1. $f+g$ is differentiable at $x$ and $D(f+g)(x) = Df(x) + Dg(x)$
2. $cf$ is differentiable at $x$ and $D(cf)(x) = c Df(x)$ for any scalar $c$

```

```{admonition} Fact: Product (Leibniz) rule
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and let $f \colon A \to \mathbb{R}$ and $g \colon A \to \mathbb{R}^K$ be both differentiable functions at $x \in A$.

Then $f g$ is differentiable at $x$ and $D(f g)(x) = f(x) Dg(x) + g(x) \nabla f(x)$

```

% ```{admonition} Definition
% :class: caution
% 
% Outer product of two vectors $x \in \mathbb{R}^N$ and $y \in \mathbb{R}^K$ is a $N \times K$ matrix given by
% %
% $$
% x y' =
% \left(
% \begin{array}{c}
% x_1 \\ \vdots \\ x_N
% \end{array}
% \right)
% (y_1,\dots,y_K)
% =
% \left(
% \begin{array}{lll}
% x_1 y_1 & 
% \cdots &
% x_1 y_K \\
% \vdots & \ddots & \vdots \\
% x_N y_1 &
% \cdots & 
% x_N y_K
% \end{array}
% \right)
% $$
% 
% ```
% 


```{admonition} Fact: Dot product rule
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and let $f,g \colon A \to \mathbb{R}^K$ be both differentiable functions at $x \in A$.

Then $f \cdot g$ is differentiable at $x$ and $D(f \cdot g)(x) = [f(x)]' Dg(x) + [g(x)]' Df(x)$, where $[\cdot]'$ denotes vector transpose

```

```{admonition} Fact: Chain rule
:class: important

Let $A$ denote an open set in $\mathbb{R}^N$ and $C$ denote an open set in $\mathbb{R}^K$.
Let $f \colon A \to C$ be differentiable at $x \in A$, and
$g \colon C \to \mathbb{R}^L$ be differentiable at $f(x) \in C$.

Then the composition $g \circ f$ is differentiable at $x$ and its total derivatibe is given by $D(g \circ f)(x) = Dg(f(x)) Df(x)$

```



```{admonition} Example
:class: tip

%
$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1^3 + x_2 x_3 \\
x_2+x_1 x_3^2
\end{array}
\right)
$$
%
$$
Df(x) = 
\left(
\begin{array}{lll}
3x_1^2, & x_3, & x_2 \\
x_3^2, & 1, & 2x_1x_3
\end{array}
\right)
$$
%
$$
g \colon
\left(
\begin{array}{c}
y_1 \\
y_2
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
2 y_1^2 \\
y_1 + y_2 \\
y_1 - y_2 \\
4 y_2^2 \\
\end{array}
\right)
$$
%
$$
Dg(y) = 
\left(
\begin{array}{lll}
4y_1, & 0 \\
1, & 1 \\
1, & -1 \\
0, & 8y_2
\end{array}
\right)
$$
%
Applying the chain rule
%
$$
D(g \circ f)(x) = 
\left(
\begin{array}{lll}
4(x_1^3 + x_2 x_3), & 0 \\
1, & 1 \\
1, & -1 \\
0, & 8(x_2+x_1 x_3^2)
\end{array}
\right)
\left(
\begin{array}{lll}
3x_1^2, & x_3, & x_2 \\
x_3^2, & 1, & 2x_1x_3
\end{array}
\right)
$$
%
$$
D(g \circ f)(x) = 
\left(
\begin{array}{lll}
12x_1^2(x_1^3 + x_2 x_3), & 
4x_3(x_1^3 + x_2 x_3), & 
4x_2(x_1^3 + x_2 x_3)\\
3x_1^2+x_3^2, &
x_3+1, &
x_2+2x_1x_3 \\
3x_1^2-x_3^2, &
x_3-1, &
x_2-2x_1x_3 \\
8x_3^2(x_2+x_1 x_3^2), &
8(x_2+x_1 x_3^2), &
16 x_1 x_3(x_2+x_1 x_3^2)
\end{array}
\right)
$$

Now we can evaluate $D(g \circ f)(x)$ at a particular points in $\mathbb{R}^3$

```

```{admonition} Example
:class: tip

$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right)
\mapsto
\left(
\begin{array}{l}
x_1^2 + x_2^2 \\
x_1 x_2
\end{array}
\right)
$$
%
$$
g \colon
\left(
\begin{array}{c}
y_1 \\
y_2
\end{array}
\right)
\mapsto
(y_1+2 y_2)^2
$$
%
**Exercise:** Using the chain rule, find the total derivative of the composition $g \circ f$.

```

- The chain rule very powerful tool in computing complex derivatives (think backwards propagation in deep neural networks)




## Higher order derivatives

The higher order derivatives of the function in one dimension generalize naturally to the multivariate case, but the complexity of the needed matrixes and multidimensional arrays (*tensors*) grows fast.

```{admonition} Definition
:class: caution
:name: hessian

Let $A$ denote an open set in $\mathbb{R}^N$, and let $f \colon A \to \mathbb{R}$. Assume that $f$ is twice differentiable at $x \in A$.

The total derivative of the gradient of function $f$ at point $x$, $\nabla f(x)$ is called the **Hessian** matrix of $f$ denoted by $Hf$ or $\nabla^2 f$, and is given by a $N \times N$ matrix
%
$$
Hf(x) = \nabla^2 f(x) = 
\left(
\begin{array}{ccc}
\frac{\partial^2 f}{\partial x_1 \partial x_1}(x) & 
\cdots &
\frac{\partial^2 f}{\partial x_1 \partial x_N}(x) \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_N \partial x_1}(x) &
\cdots & 
\frac{\partial^2 f}{\partial x_N \partial x_N}(x)
\end{array}
\right)
$$
```
```{admonition} Fact
:class: important

For every $x \in A \subset \mathbb{R}^N$ where $A$ is an open and $f \colon A \to \mathbb{R}^N$ is twice continuously differentiable, the Hessian matrix $\nabla^2 f(x)$ is symmetric
```

```{admonition} Example
:class: tip

$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right)
\mapsto
(x_1 - 2 x_2)^2
$$
%
$$
\nabla^2 f(x) = 
\left(
\begin{array}{rr}
2, & -4 \\ -4, & 8
\end{array}
\right)
$$
%
**Exercise:** check
```

```{admonition} Example
:class: tip

$$
f \colon
\left(
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right)
\mapsto
(x_1 - 2 x_2)^3
$$
%
$$
\nabla^2 f(x) = 
6 (x_1 - 2 x_2) 
\left(
\begin{array}{rr}
1, & -2 \\ -2, & 4
\end{array}
\right)
$$
%
**Exercise:** check
```

For the vector-valued function $f \colon \mathbb{R}^N \to \mathbb{R}^K$ the first derivative is given by a $(K \times N)$ Jacobian matrix, and the second derivative is given by a $(K \times N \times N)$ tensor, containing $K$ Hessian matrices, one for each $f_j$ components.
*(this course only mentions this without going further)*

## Multivariate Taylor series

Consider a function $f\colon A \subset\mathbb{R}^N \to \mathbb{R}$ differentiable at $x \in A$

Introduce multi-index notation given vectors $\alpha \in \mathbb{R}^N$ and $x = \in \mathbb{R}^N$
- $|\alpha| \equiv \alpha_1 + \alpha_2 + \dots + \alpha_N$
- $\alpha! \equiv \alpha_1! \cdot \alpha_2! \cdot \dots \cdot \alpha_N!$
- $x^{\alpha} \equiv x_1^{\alpha_1} x_2^{\alpha_2} \dots x_N^{\alpha_N}$

```{admonition} Fact (Clairaut-Young theorem)
:class: important

If all the $k$-th order partial derivatives of $f$ exist and are continuous in a neighborhood of $x$, then the order of differentiation does not matter, and the mixed partial derivatives are equal
```

Therefore the following notation for the up to $k$ order partial derivatives is well defined
:::{math}
D^{\alpha} f(x) = \frac{\partial^{|\alpha|} f}{\partial x_1^{\alpha_1} \partial x_2^{\alpha_2} \dots \partial x_N^{\alpha_N}}(x), \; |\alpha|\leqslant k
:::

In these circumstances (that Clairaut-Young theorem applies for all $k = 1,2,\dots,k$) we say taht $f$ is $k$-times differentiable at $x$

```{admonition} Fact
:class: important

Let $f\colon A \subset\mathbb{R}^N \to \mathbb{R}$ be a $k$-times continuously differentiable function at a point $a \in \mathbb{R}^N$.
Then there exist functions $h_\alpha \colon \mathbb{R}^N \to \mathbb{R}$, where $|\alpha|=k$ such that
:::{math}
f(x) = \sum_{|\alpha|\leqslant k} \frac{D^{\alpha} f(a)}{\alpha!} (x-a)^{\alpha} + \sum_{|\alpha|=k} h_\alpha(x) (x-a)^{\alpha}
:::
where $\lim_{x \to a}h_\alpha(x) = 0.$
```


```{admonition} Example
:class: tip

The third-order Taylor polynomial of a smooth function$f:\mathbb R^2\to\mathbb R$

Let $\boldsymbol{x}-\boldsymbol{a}=\boldsymbol{v}$

:::{math}
\begin{align}
P_3(\boldsymbol{x}) = f ( \boldsymbol{a} ) + {} &\frac{\partial f}{\partial x_1}( \boldsymbol{a} ) v_1 + \frac{\partial f}{\partial x_2}( \boldsymbol{a} ) v_2 + \frac{\partial^2 f}{\partial x_1^2}( \boldsymbol{a} ) \frac {v_1^2}{2!} +  \frac{\partial^2 f}{\partial x_1 \partial x_2}( \boldsymbol{a} ) v_1 v_2 + \frac{\partial^2 f}{\partial x_2^2}( \boldsymbol{a} ) \frac{v_2^2}{2!}  \\
& + \frac{\partial^3 f}{\partial x_1^3}( \boldsymbol{a} ) \frac{v_1^3}{3!} + \frac{\partial^3 f}{\partial x_1^2 \partial x_2}( \boldsymbol{a} ) \frac{v_1^2 v_2}{2!} + \frac{\partial^3 f}{\partial x_1 \partial x_2^2}( \boldsymbol{a} ) \frac{v_1 v_2^2}{2!} + \frac{\partial^3 f}{\partial x_2^3}( \boldsymbol{a} ) \frac{v_2^3}{3!}
\end{align}
:::

Source: [Wiki](https://en.wikipedia.org/wiki/Taylor%27s_theorem#Generalizations_of_Taylor's_theorem)
```



