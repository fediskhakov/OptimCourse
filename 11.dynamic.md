---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
---

NOTES:

- Only deterministic finite horizon
- Value function
- Maximum theorems (without correspondences and hemi-continuity?)
- Give full formulation, and immediately a special case with concavity, unique solution and concave value function
- Cake eating problem
- Existence of the solution
- FOC: Envelope and Euler equation
- Solution as multi-variate optimization problem or ..
- dynamic programming, Bellman principle and Bellman equation
- Backwards induction algorithm
- Solve the cake eating problem by hand?

# Dynamic optimization

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>


## Envelope and maximum theorems

**Plan for this lecture**

1. Correspondences and continuity
2. The maximum theorem
3. Envelope theorem

**Supplementary reading:**
- Simon & Blume: 19.1, 19.2
- Sundaram: chapter 9, 5.2.3

## Parametric continuity

### Value function and parameters of optimization problems

Let's start with recalling the definition of a general optimization problem

```{admonition} Definition
:class: caution

The general form of the optimization problem is

%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to}
\\
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}\\
h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}$ where $h_j \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are inequality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function
```

This lecture focuses on the *value function* in the optimization problem $V(\theta)$, and how it depends on the parameters $\theta$.

In economics we are interested how the optimized behavior changes when the circumstances of the decision-making process change
- income/budget/wealth changes
- intertemporal effects of changes in other time periods

We would like to establish the properties of the value function $V(\theta)$:

- continuity $\rightarrow$ ***The maximum theorem***
- changes/derivative (*if differentiable*) $\rightarrow$ ***Envelope theorem***
- monotonicity $\rightarrow$ Supermodularity and increasing differences (*not covered here*, see Sundaram ch.10)

```{admonition} Main idea for the maximum theorem

When the components of the optimization problem $f(x,\theta)$, $g_i(x,\theta)$ and $h_j(x,\theta)$ are continuous, then the value function $V(\theta)$ is also continuous, in certain sense

```

We need to accurately define the notion of continuity for all components of the optimization problem

  - objective function
  - constraints $\leftrightarrow$ admissible set

Denote the admissible set $\mathcal{D}(\theta)$
%
$$
\mathcal{D}(\theta) = \left\{ 
x \in \mathbb{R}^N \colon
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}, \;
h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}
\right\}
$$

In solving the optimization problem we are not only interested in the attainable optimal value $V(\theta)$, but also in the set of maximizers/minimizers $\mathcal{D}^\star(\theta)$ corresponding to each parameter value $\theta$

```{admonition} Definition
:class: caution

We will refer to the pair
%
$$
V(\theta) = \max_{x} f(x,\theta) \\
\mathcal{D}^\star(\theta) = \mathrm{arg}\max_x f(x,\theta)
$$
%
as the **solution of the optimization problem**
%
$$
f(x,\theta) \to \max_{x} \\
\text{subject to} \; x \in \mathcal{D}(\theta),
$$
%
where
%
$$
\begin{array}{l}
f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R},\\
\mathcal{D}(\theta) \subset \mathbb{R}^N \text{ for all } \theta,\\
\theta \in \Theta \subset \mathbb{R}^K
\end{array}
$$

```

(ref-correspondences)=
### Correspondences 

Note that the mappings of $\theta$ to $\mathcal{D}(\theta)$ or $\mathcal{D}^\star(\theta)$ are not functions because both $\mathcal{D}(\theta)$ and often $\mathcal{D}^\star(\theta)$ have multiple elements for a given $\theta$

```{admonition} Definition
:class: caution

A correspondence (set-valued function) is a map that associates elements of its domain to *sets* of elements in its range, i.e.
%
$$
f \colon \Theta \subset \mathbb{R}^K \to P(\mathbb{R}^N)
$$
%
where $P(\mathbb{R}^N)$ denotes the **power set** of $\mathbb{R}^N$, i.e. the set of all subsets of $\mathbb{R}^N$. It can also be denoted as $2^{\mathbb{R}^N}$.
```

```{admonition} Example
:class: tip

Let correspondence $\Phi$ be defined as
%
$$
\Phi \colon \theta \in \Theta \subset \mathbb{R}^K \to \mathcal{D}(\theta) \subset P(\mathbb{R}^N)
$$
%

```

```{admonition} Example
:class: tip

$$
\phi \colon x \in [0,1] \mapsto \{0,1\}
$$
%
$$
\phi \colon x \in [0,1] \mapsto (0,1)
$$
%
$$
\phi \colon x \in [0,1] \mapsto [0,1]
$$
%
$$
\phi \colon x \in [0,1] \mapsto [x,1]
$$
%
$$
\phi \colon x \in [0,1] \mapsto (0,1/x]
$$
%
$$
\phi \colon x \in (0,1] \mapsto (0,1/x]
$$

```

```{figure} _static/img/uhc_lhc.png
:name: uhc_lhc
:scale: 80%

Examples of correspondences (for labels see below)
```


Correspondences are classified by the properties of the sets they output:
  - open-valued correspondences
  - closed-valued correspondences
  - non-empty-valued correspondences
  - bounded-valued correspondences
  - convex-valued correspondences
  - compact-valued correspondences
  - finite-valued correspondences
  - singleton-valued correspondences (functions?)

### Continuity of correspondences

Recall the definition of the continuous function

```{admonition} Definition
:class: caution

Function $f \colon A \subset \mathbb{R}^n \to \mathbb{R}^m$ is called ***continuous at*** ${\bf x} \in A$ if as $n \to \infty$ for every converging to ${\bf x}$ sequence
%
$$
{\bf x}_n \to {\bf x}
\quad \implies \quad
f({\bf x}_n) \to f({\bf x}) 
$$
%
```

The equivalent definition of continuity relies on the the open epsilon-balls

```{admonition} Fact
:class: important

A function $f \colon A \subset \mathbb{R}^n \to \mathbb{R}^m$ is continuous at ${\bf x} \in A$ if and only if for every $\epsilon >0$ there is a $\delta>0$ such that
%
$$
{\bf x}' \in B_\delta({\bf x})
\implies f({\bf x}') \in B_\epsilon(f({\bf x}))
$$
%
```

````{admonition} Proof
:class: dropdown

Omitted here, but see https://www.u.arizona.edu/~mwalker/MathCamp2020/ContinuousFunctions.pdf

````

Thinking of the definition of limit stated in terms of open epsilon-balls, it is not hard to see the equivalence result


Generalization to correspondences is not straightforward because $\in$ operation does not convert to the set-valued case immediately:
  - can be replaced by set inclusion $\subset$
  - can be represented by non-empty intersection $\bar{\cap}$, i.e. $A \bar{\cap} B \iff A \cap B \ne \emptyset $

Namely, the condition in the definition above can be replaced with either
- ${\bf x}' \in B_\delta({\bf x}) \implies f({\bf x}') \subset B_\epsilon(f({\bf x}))$, or
- ${\bf x}' \in B_\delta({\bf x}) \ne \emptyset \implies f({\bf x}') \cap B_\epsilon(f({\bf x})) \ne \emptyset$

```{admonition} Definition
:class: caution
:name: hemicontinuity

Correspondence $\gamma \colon X \to 2^Y$ is called ***upper hemi-continuous (uhc)*** at ${\bf x} \in X$ if for every open set $V$ *containing* $f({\bf x})$, i.e. $f({\bf x}) \subset V$, there is an open set $U$ such that
%
$$
{\bf x} \in U \text{ and }
{\bf x}' \in U \implies f({\bf x}') \subset V
$$
%
Correspondence $\gamma \colon X \to 2^Y$ is called ***lower hemi-continuous (lhc)*** at ${\bf x} \in X$ if for every open set $V$ *intersecting with* $f({\bf x})$, i.e. $f({\bf x}) \cap V \ne \emptyset$, there is an open set $U$ such that
%
$$
{\bf x} \in U \text{ and }
{\bf x}' \in U \implies f({\bf x}') \cap V \ne \emptyset
$$
%
```

```{admonition} Definition
:class: caution

A corresponse is called continous if it is both upper and lower hemi-continuous
```

```{note}
*Semi-continuity* is a special notion of continuity for functions, and may be used as equivalent to *hemi-continuity* for correspondences
```

*Examples, examples, examples (whiteboard)*

```{admonition} Fact
:class: important

Constant correspondences are both uhc and lhc
%
$$
\gamma \colon x \in X \mapsto \mathcal{D} \subset Y \text { for every } x
\implies \gamma \text{ is continuous (uhc and lhc)}
$$


```

```{note}

For closed-valued correspondences, a good rule of thumb for determining hemi-continuity at ${\bf x}$ is:
- if moving "a little amount" away from ${\bf x}$ no new points are "discontinuously/suddenly" **appear** outside of $f({\bf x})$, $f$ is **uhc**
- if moving "a little amount" away from ${\bf x}$ no points "suddenly" **disappear** from $f({\bf x})$, $f$ is **lhc**

```

### The statement of the maximum theorem

Extremely useful in _many_ fields of economics:
- demand (consumer) theory
- theory of the firm: supply of products, demand for inputs
- theory of economic growth
- game theory and industrial relations

```{admonition} The maximum theorem
:class: important
:name: maximum-theorem

Let $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ be a continuous function, and $\mathcal{D}(\theta)$ be a compact-valued continuous correspondence.
Then the value function $V(\theta)$ is continuous on $\mathbb{R}^K$ and $\mathcal{D}^\star(\theta)$ is a compact-valued upper hemi-continuous correspondence on $\Theta$

```

In other words, continuity of the fundamentals of the optimization problem are inherited by the value function, but not to the full extent (continuity $\to$ hemi-continuity)
%
$$
\begin{array}{l}
\text{$f$ function continuous}\\
\text{$\theta \mapsto \mathcal{D}(\theta)$ compact-valued}\\
\text{$\theta \mapsto \mathcal{D}(\theta)$ continuous (uhc + lhc)}
\end{array}
\longrightarrow 
\begin{array}{l}
V(\theta) \text{ continuous}\\
\mathcal{D}^\star(\theta) \text{ upper hemi-continuous}
\end{array}
$$

```{admonition} Maximum theorem under convexity
:class: important
:name: maximum-theorem-convexity

Let $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ be a continuous function, and $\mathcal{D}(\theta)$ be a compact-valued continuous correspondence.
Then:
1. The value function $V(\theta)$ is continuous on $\mathbb{R}^K$ and $\mathcal{D}^\star(\theta)$ is a compact-valued upper hemi-continuous correspondence on $\Theta$ (as before)
2. If $f(x,\theta)$ is concave in $x$ and $\mathcal{D}(\theta)$ is convex-valued for every $\theta$, then $\mathcal{D}^\star(\theta)$ is a convex-valued. \
If concavity of $f(x,\theta)$ is strict, then $\mathcal{D}^\star(\theta)$ is a singleton-valued upper hemi-continuous correspondence, hence a continuous function
3. If $f$ is concave in $(x,\theta)$ and the graph of $\mathcal{D}(\theta)$ is convex, in addition to the above the value function $V(\theta)$ is concave. Under strict concavity $V(\theta)$ is also strictly concave.

```

```{admonition} Definition
:class: caution

The graph of a correspondence $\gamma \colon X \to 2^Y$ is defined as the set $\{(x,y) \in X \times Y \colon y \in f(x)\}$

```


Consider a special case for when the optimizer is unique for each $\theta$

```{admonition} Fact
:class: important

A single-valued correspondence that is hemi-continuous (either uhc or lhc) is continuous when viewed as a function.
Conversely, every continuous function, when viewed as a single-valued correspondence, is both uhc and lhc.

```




In this case the upper semi-continuity, lower semi-continuity coincide with the "usual" continuity

```{admonition} Example
:class: tip

Budget correspondence in the two goods consumer optimization problem.
Assuming $p_1>0,\, p_2>0,\, m>0$, the budget correspondence can be defined as
%
$$
\beta \colon (p_1,p_2,m) \mapsto \{(x_1,x_2) \in \mathbb{R}^2 \colon \forall i\; x_i \ge 0, p_1 x_1 + p_2 x_2 \leq m\}
$$

```

```{image} _static/img/desmos-budget.png
:class: bg-primary
:width: 50%
:align: center
:target: https://www.desmos.com/calculator/pdxihfxpbm
```

See [online animation](https://www.desmos.com/calculator/pdxihfxpbm)


```{admonition} Fact
:class: important

Budget correspondence $\beta$ defined above is both uhc and lhc, and therefore continuous

```




```{admonition} Example
:class: tip

Maximization of log utility subject to budget constraint
%
$$ 
u(x_1, x_2) = \alpha \log(x_1) + \beta \log(x_2) \to \max_{x_1, x_2} \\
\text{ subject to} \\
p_1 x_1 + p_2 x_2 \leq m
$$
%
- $p_i$ is the price of good $i$, $p_i>0$
- $m$ is the budget, assumed non-negative
- $\alpha>0$, $\beta>0$
- $x_1 \geq 0, \; x_2 \geq 0$, can show that these constraints never bind

```

```{figure} _static/plots/log_util.png
:scale: 50%
:name: 

Log utility with $\alpha=0.4$, $\beta=0.5$
```

```{figure} _static/plots/budget_set_3.png
:scale: 50%
:name: 

Utility max for $p_1=1$, $p_2 = 1.2$, $m=4$, $\alpha=0.4$,
$\beta=0.5$
```

The maximizer according to the FOC conditions and verified with SOC (see lecture 8) is
%
$$
x_1^\star = \frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} \\
x_2^\star = \frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2}
$$
%

**Applying the maximum theorem:**

1. Objective function is continuous in all arguments
2. Constrained set is compact for all parameters (as they are defined)
3. Budget correspondence is continuous

Therefore the theorem applies and we have: the value function is continuous in all parameters, and the set of maximizers is upper hemi-continuous.

Moreover, we can show that the utility function is *strictly concave* for all parameters, therefore the clause 2 of the maximum theorem under convexity applies, and the set of maximizers is a singleton-valued correspondence, i.e. a function. We have already found it above.

We can verify that the value function is indeed continuous by plugging the maximizer back into the objective function
%
$$
V(p_1,p_2,m) = \alpha \log\left(
\frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} 
\right) + \beta \log\left(
\frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2}
\right)
$$
%

```{admonition} Question
Can $\alpha$ and $\beta$ be also considered as parameters in the previous analysis?
```

For the numerical example set $\alpha = 2$, $\beta = 1$, $m = 10$, and let $p_1$ and $p_2$ vary

```{code-cell} python3
---
mystnb:
  figure:
    width: 80%
    align: center
    caption: Value function in the space of prices $p_1,p_2$
tags:
  - hide-input
---

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

alpha = 2
beta = 1
m = 10
f = lambda x: alpha * np.log( alpha * m / ( (alpha+beta)*x[0] )) + beta * np.log( beta * m / ( (alpha+beta)*x[1] ))

lb,ub = .05,5

x = y = np.linspace(lb,ub, 100)
X, Y = np.meshgrid(x, y)
zs = np.array([f((x,y)) for x,y in zip(np.ravel(X), np.ravel(Y))])
Z = zs.reshape(X.shape)

fig = plt.figure(dpi=160)
ax2 = fig.add_subplot(111)
ax2.set_aspect('equal', 'box')
ax2.contour(X, Y, Z, 50,
            cmap=cm.jet)
plt.setp(ax2, xticks=[],yticks=[])

fig = plt.figure(dpi=160)
ax3 = fig.add_subplot(111, projection='3d')
ax3.plot_surface(X, Y, Z, 
            rstride=2, 
            cstride=2,
            alpha=0.7,
            linewidth=0.25)

plt.show()

```

```{code-cell} python3
---
mystnb:
  figure:
    width: 80%
    align: center
    caption: Optimal choice of $x_1$ and $x_2$ as function of prices $p_1$ and $p_2$ (aka demand curve)
tags:
  - hide-input
---
alpha = 2
beta = 1
m = 10
# f = lambda x: alpha * np.log( alpha * m / ( (alpha+beta)*x[0] )) + beta * np.log( beta * m / ( (alpha+beta)*x[1] ))
f1 = lambda x: alpha * m / ( (alpha+beta)*x)
f2 = lambda x: beta * m / ( (alpha+beta)*x)

lb,ub = .5,5
x = np.linspace(lb,ub, 100)

fig = plt.figure(dpi=160)
ax = fig.add_subplot(111)
ax.plot(x,f1(x),label=r"$x_1^\star(p_1)$")
ax.plot(x,f2(x),label=r"$x_2^\star(p_2)$")
plt.legend(loc='upper right', frameon=False)
plt.show()
```




```{admonition} Example
:class: tip

Maximization of log-linear utility subject to budget constraint
%
$$ 
u(x_1, x_2) = \alpha x_1 + \beta \log(x_2) \to \max_{x_1, x_2} \\
\text{ subject to} \\
p_1 x_1 + p_2 x_2 \leq m \\
x_1 \geq 0, \; x_2 \geq 0
$$
%
- $p_i$ is the price of good $i$, assumed non-negative
- $m$ is the budget, assumed non-negative
- $\alpha>0$, $\beta>0$


Form the Lagrangian with 3 inequality constraints (have to flip the sign for non-negativity to stay within the general formulation)
%
$$
\mathcal{L}(x_1,x_2,\lambda_1,\lambda_2,\lambda_3) = \\
= \alpha x_1 + \beta\log(x_2)  - \lambda_1 (-x_1) - \lambda_2 (-x_2) - \lambda_3 (p_1 x_1 + p_2 x_2 -m) = \\
= \alpha x_1 + \beta\log(x_2) + \lambda_1 x_1+ \lambda_2 x_2 - \lambda_3 (p_1 x_1 + p_2 x_2 -m)
$$
%
The necessary KKT conditions are given by the following system of equations
%
$$
\begin{cases}
\frac{\partial \mathcal{L}}{\partial x_1} = 0 \implies
\alpha + \lambda_1 - \lambda_3 p_1 = 0 \\
\frac{\partial \mathcal{L}}{\partial x_2} = 0 \implies
\frac{\beta}{x_2} + \lambda_2 - \lambda_3 p_2 = 0 \\
x_1 \ge 0 \\
x_2 \ge 0 \\
x_1 p_1 + x_2 p_2 \le m \\
\lambda_1 \ge 0 \text { and } \lambda_1 x_1 = 0 \\
\lambda_2 \ge 0 \text { and } \lambda_2 x_2 = 0 \\
\lambda_3 \ge 0 \text { and } \lambda_3 (x_1 p_1 + x_2 p_2 -m) = 0 \\
\end{cases}
$$
%
The KKT conditions can be solved systematically by considering all combinations of the multipliers. The two cases where the system is consistent give the solution
%
$$
\begin{cases}
x_1^\star = \frac{m}{p_1} - \frac{\beta}{\alpha}, \;
x_2^\star = \frac{\beta p_1}{\alpha p_2}, & 
\text{ if } p_1/m \le \alpha/\beta, \\
x_1^\star = 0, \;
x_2^\star = \frac{m}{p_2}, & 
\text{ if } p_1/m > \alpha/\beta \\
\end{cases}
$$

```

```{figure} _static/plots/corner_sol_2.png
:name: 

Corner solution
```

**Applying the maximum theorem:**

1. Objective function is continuous in all arguments
2. Constrained set is compact for all parameters (as they are defined)
3. Budget correspondence is continuous

Therefore the theorem applies and we have: the value function is continuous in all parameters, and the set of maximizers is upper hemi-continuous.

Moreover, we can show that the utility function is *strictly concave* for all parameters, therefore the clause 2 of the maximum theorem under convexity applies, and the set of maximizers is a singleton-valued correspondence, i.e. a function. We have already found it above.

We can verify that the value function is indeed continuous by plugging the maximizer back into the objective function
%
$$
V(p_1,p_2,m) = 
\begin{cases}
\frac{\alpha m}{p_1} - \beta
+ \beta \log\left(
\frac{\beta p_1}{\alpha p_2}
\right), & 
\text{ if } p_1/m \le \alpha/\beta, \\
\beta \log\left(
\frac{m}{p_2}
\right), & 
\text{ if } p_1/m > \alpha/\beta \\
\end{cases}
$$


```{code-cell} python3
---
mystnb:
  figure:
    width: 80%
    align: center
    caption: Value function in the space of prices $p_1,p_2$
tags:
  - hide-input
---

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

alpha = 1
beta = 3
m = 10
cond = lambda p1: p1/m <= alpha/beta
f = lambda p: cond(p[0])*( alpha * m / p[0] - beta + beta * np.log( beta * p[0] / ( alpha * p[1] )) ) + (1-cond(p[0])) * (beta * np.log( m / p[1] ))

lb,ub = .1,5

x = y = np.linspace(lb,ub, 100)
X, Y = np.meshgrid(x, y)
zs = np.array([f((x,y)) for x,y in zip(np.ravel(X), np.ravel(Y))])
Z = zs.reshape(X.shape)

fig = plt.figure(dpi=160)
ax2 = fig.add_subplot(111)
ax2.set_aspect('equal', 'box')
ax2.contour(X, Y, Z, 50,
            cmap=cm.jet)
plt.setp(ax2, xticks=[],yticks=[])

fig = plt.figure(dpi=160)
ax3 = fig.add_subplot(111, projection='3d')
ax3.plot_surface(X, Y, Z, 
            rstride=2, 
            cstride=2,
            alpha=0.7,
            linewidth=0.25)
ax3.view_init(elev=5, azim=-34)

plt.show()

```

```{code-cell} python3
---
mystnb:
  figure:
    width: 80%
    align: center
    caption: Optimal choice of $x_1$ as a function of $p_1$ (aka demand curve)
tags:
  - hide-input
---
alpha = 1
beta = 3
m = 10
cond = lambda p1: p1/m <= alpha/beta
f1 = lambda x: cond(x)*(m/x - beta/alpha) + (1-cond(x))*0
lb,ub = .5,10
x = np.linspace(lb,ub, 100)
fig = plt.figure(dpi=160)
ax = fig.add_subplot(111)
ax.plot(x,f1(x),label=r"$x_1^\star(p_1)$")
plt.legend(loc='upper right', frameon=False)
plt.show()
```

```{code-cell} python3
---
mystnb:
  figure:
    width: 80%
    align: center
    caption: Optimal choice of $x_2$ as a function of $(p_1,p_2)$ (aka demand curve)
tags:
  - hide-input
---
alpha = 1
beta = 3
m = 10
cond = lambda p1: p1/m <= alpha/beta
f = lambda p: cond(p[0])*( beta * p[0] / (alpha *p[1]) ) + (1-cond(p[0])) * (m / p[1])
lb,ub = .5,5
x = y = np.linspace(lb,ub, 100)
X, Y = np.meshgrid(x, y)
zs = np.array([f((x,y)) for x,y in zip(np.ravel(X), np.ravel(Y))])
Z = zs.reshape(X.shape)
fig = plt.figure(dpi=160)
ax3 = fig.add_subplot(111, projection='3d')
ax3.plot_surface(X, Y, Z, 
            rstride=2, 
            cstride=2,
            alpha=0.7,
            linewidth=0.25)
plt.setp(ax3,xticks=[],yticks=[],zticks=[])
ax3.view_init(elev=22, azim=123)
plt.show()
```


## Envelope theorems

Next step after continuity -- differentiability of the value function $V(\theta)$ and the marginal effect of relaxing the constraint (*derivatives!*)

Let's start with an unconstrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

```{admonition} Envelope theorem for unconstrained problems
:class: important
:name: envelope-unconstrained

Let $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ be a differentiable function, and $x^\star(\theta)$ be the maximizer of $f(x,\theta)$ for every $\theta$.
Suppose that $x^\star(\theta)$ is differentiable function itself.
Then the value function of the problem $V(\theta) = f\big(x^\star(\theta),\theta)$ is differentiable w.r.t. $\theta$ and
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial f}{\partial \theta_j} \big(x^\star(\theta),\theta\big),
\forall j
$$

```

In other words, the marginal changes in the value function are given by the partial derivative of the objective function evaluated at the maximizer.


````{admonition} Proof
:class: dropdown

**Exercise**

````

```{note}
When $K=1$, so that $\theta$ is a scalar, the envelope theorem can be written as
%
$$
\frac{d f\big(x^\star(\theta),\theta)}{d \theta} =
\frac{\partial f}{\partial \theta} \big(x^\star(\theta),\theta\big)
$$
% 
so that the meaning is carried only by the derivative sign change
```

```{admonition} Example
:class: tip

Consider $f(x,a) = -x^2 +2ax +4a^2 \to \max_x$. \
What is the (approximate) effect of a unit increase in $a$ on the attained maximum?

FOC: $-2x+2a=0$, giving $x^\star(a) = a$.

So, $V(a) = f(a,a) = 5a^2$, and $V'(a)=10a$. The value increases at a rate of $10a$ per unit increase in $a$.

Using the envelope theorem, we could go directly to
%
$$
V'(a) = \frac{\partial f}{\partial a} (x^\star(a),a) = 2x + 8a \Big|_{x=a} = 10a
$$

```

```{admonition} Envelope theorem for constrained problems
:class: important
:name: envelope-constrained

Consider an equality constrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to}
\\
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}\\
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Assume that the maximizer correspondence $\mathcal{D}^\star(\theta) = \mathrm{arg}\max f(x,\theta)$ is *single-valued* and can be represented by the function $x^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^N$, with the corresponding Lagrange multipliers $\lambda^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^K$.

Assume that both $x^\star(\theta)$ and $\lambda^\star(\theta)$ are differentiable, and that the constraint qualification assumption holds.
Then 
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial \mathcal{L}}{\partial \theta_j} \big(x^\star(\theta),\lambda^\star(\theta),\theta\big),
\forall j
$$
%
where $\mathcal{L}(x,\lambda,\theta)$ is the Lagrangian of the problem.
```

````{admonition} Proof
:class: dropdown

**Exercise**

````

```{note}
What about the inequality constraints? 

Well, if the solution is interior and none of the constrains are binding, the unconstrained version of the envelope theorem applies.
If any of the constrains are binding, their combination can be represented as a set of equality constraint, and the constrained version of the envelope theorem applies.
*Care needs to be taken* to avoid the changes in the parameter that lead to a switch in the binding constraints. Such points are most likely non-differentiable, and the envelope theorem does not apply there at all!
```




```{admonition} Example
:class: tip

Back to the log utility case
%
$$ 
u(x_1, x_2) = \alpha \log(x_1) + \beta \log(x_2) \to \max_{x_1, x_2} \\
\text{ subject to} \\
p_1 x_1 + p_2 x_2 = m
$$
%
The Lagrangain is
%
$$ 
\mathcal{L}(x_1,x_2,\lambda) = \alpha \log(x_1) + \beta \log(x_2)
- \lambda(p_1 x_1 + p_2 x_2 - m)
$$
%
Solution is 
%
$$
x_1^\star(p_1,p_2,m) = \frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} \\
x_2^\star(p_1,p_2,m) = \frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2} \\
\lambda^\star(p_1,p_2,m) = \frac{\alpha + \beta}{m}
$$
%

Value function is
%
$$
V(p_1,p_2,m) = \alpha \log\left(
\frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} 
\right) + \beta \log\left(
\frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2}
\right)
$$
%

```

We can verify the Envelope theorem by noting that direct differentiation gives
%
$$
\frac{\partial V}{\partial p_1} = 
-\frac{\alpha}{p_1}, \quad
\frac{\partial V}{\partial p_2} = 
-\frac{\beta}{p_2}, \quad
\frac{\partial V}{\partial m} = 
\frac{\alpha+\beta}{m}
$$
%
And applying the envelope theorem we have
%
$$
\frac{\partial V}{\partial p_1} = 
\frac{\partial \mathcal{L}}{\partial p_1}(x_1^\star,x_2^\star,\lambda^\star) =
-\lambda^\star x_1^\star = -\frac{\alpha}{p_1}
$$
%
$$
\frac{\partial V}{\partial p_2} = 
\frac{\partial \mathcal{L}}{\partial p_2}(x_1^\star,x_2^\star,\lambda^\star) =
-\lambda^\star x_2^\star = -\frac{\beta}{p_2}
$$
%
$$
\frac{\partial V}{\partial m} = 
\frac{\partial \mathcal{L}}{\partial m}(x_1^\star,x_2^\star,\lambda^\star) =
\lambda^\star = \frac{\alpha+\beta}{m}
$$


### Lagrange multiplyers as shadow prices

In the equality constrained optimization problem, the Lagrange multiplier $\lambda_i$ can be interpreted as the shadow price of the constraint $g_i(x,\theta) = a$, i.e. the change in the value function resulting from a change in parameter $a$, in other words *relaxing* the constraint $g_i(x,\theta) = 0$.

**Exercise:** prove this statement


## Extra study material

- [Mark Walker's video lectures on correspondences](https://www.youtube.com/playlist?list=PLcjqUUQt__ZGcsvV6o19QxIQlHG3DlSfQ)
- [Mark Walker's video lecture on envelope theorem](https://youtu.be/DiRwRERgglw?si=Wzn-mJirHjbT1l4p)


## DYNAMIC

**Plan for this lecture**

1. Dynamic decision problems
2. Dynamic programming for finite horizon problems
3. Dynamic programming for infinite horizon problems

**Supplementary reading:**
- Sundaram: chapter 11

## Dynamic decision problems

As before, let's start with our definition of a general optimization problem

```{admonition} Definition
:class: caution

The general form of the optimization problem is

%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to } x \in \mathcal{D}(\theta)
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $\mathcal{D}(\theta)$ denotes the admissible set
%
$$
\mathcal{D}(\theta) = \left\{ 
x \in \mathbb{R}^N \colon
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}, \;
h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}
\right\}
$$
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}$ where $h_j \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are inequality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function
```

```{admonition} Crazy thought

Can we let the objective function include the value function as a component?
```

- Think of an optimization problem that repeats over and over again, for example in time
- Value function (as the maximum attainable value) of the next optimization problem may be part of the current optimization problems
- The parameters $\theta$ may be adjusted to link these optimization problems together

```{admonition} Example
:class: tip

Imagine the optimization problem with $g(x,\theta)= \sum_{t=0}^{\infty}\beta^{t}u(x,\theta)$ where $b \in (0,1)$ to ensure the infinite sum converges. 
We can write
%
$$
\begin{array}{lll}
V(\theta) & = & \max_x \sum_{t=0}^{\infty}\beta^{t} u(x_t,\theta) \\
& = & \max_x \Big[u(x_0)+\beta\max_x \sum_{t=1}^{\infty}\beta^{t-1}u(x_t,\theta)\Big] \\
& = & \max_x \Big[u(c_{0})+\beta V(\theta)\Big]
\end{array}
$$

```

- Note that the trick in the example above works because $g(x,\theta)$ is an infinite sum
- Among other things this implies that $x$ is infinite dimension!
- Let the parameter $\theta$ to *keep track* time, so let $\theta = (t, s_t)$
- Then it is possible to use the same principle for the optimization problems without infinite sums

Introduce new notation for the value function
%
$$
\theta=(t, s_t) \rightarrow V(\theta) = V(t, s_t) = V_t(s_t)
$$

```{admonition} Definition: dynamic optimization problems
:class: caution
:name: dynamic-optimization-problem

The general form of the *deterministic dynamic optimization* problem is

%
$$
V_t(s_t)
= \max_{x} \big[ f(x,s_t) + \beta V_{t+1}(s_{t+1}) \big] \\
\text {subject to } x \in \mathcal{D}_t(s_t)
$$
%
where:
- $f(x,s_t) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an instantaneous reward function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $s_t \in \mathbb{R}^K$ are *state variables*
- state transitions are given by $s_{t+1} = g(x,s_t)$ where $g \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}^K$
- $\mathcal{D}_t(s_t) \subset \mathbb{R}^N$ denotes the admissible set at decision instance (time period) $t$
- $\beta \in (0,1)$ is a discount factor
- $V_t(s_t) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Denote the set of maximizers of each instantaneous optimization problem as
%
$$
x^\star_t(s_t) = \mathrm{argmax}_{x \in \mathcal{D}_t(s_t)} \big[ f(x,s_t) + \beta V_{t+1}(s_{t+1}) \big]
$$
%
- $x^\star_t(s_t)$ is called a *policy correspondence* or *policy function*
```

- State space $s_t$ is of primary importance for the economic interpretation of the problem: it encompasses _all_ the information that the decision maker conditions their decisions on
- Has to be carefully founded in economic theory and common sense
- Similarly the transition function $g(x,s_t)$ reflects the exact beliefs of the decision maker about how the future state is determined and has direct implications for the optimal decisions

```{note}
In the formulation above, as seen from transition function $g(x,s_t)$, we implicitly assume that the history of states from some initial time period $t_0$ is not taken into account.
Decision problems where only the current state is relevant for the future are called **Markovian decision problems**
```

### Dynamic programming and Bellman principle of optimality

> An optimal policy has a property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. \
> üìñ Bellman, 1957 ‚ÄúDynamic Programming‚Äù

```{admonition} Main idea

Breaking the problem into sequence of small problems
```

> Dynamic programming is recursive method for solving sequential decision problems \
> üìñ Rust 2006, *New Palgrave Dictionary of Economics*

- In computer science the meaning of the term is broader: DP is a general algorithm design technique for solving problems with overlapping sub-problems

- Thus, the sequential decision problem is broken into *current decision*
  and the *future decisions* problems, and solved recursively

- The solution can be computed through **backward induction** which is the process of solving a sequential decision problem from the later periods to the earlier ones, this is the main algorithm used in **dynamic programming (DP)**

- Embodiment of the recursive way of modeling sequential decisions is
  **Bellman equation**

```{admonition} Definition
:class: caution
:name: bellman-equation

Bellman equation is a *functional equation* in an unknown function $V(\cdot)$ which embodies a dynamic optimization problem and has the form
%
$$
V(\text{state}) = \max_{\text{feasible decisions}} \big[ U(\text{state},\text{decision}) + \beta \mathbb{E}\big\{ V(\text{next state}) \big| \text{state},\text{decision} \big\} \big]
$$
%
- expectation $\mathbb{E}\{\cdot\}$ is taken over the distribution of the next period state conditional on current state and decision when the state transitions are given by a stochastic process

- the optimal choices (policy correspondence) are revealed along the solution of the Bellman equation as decisions which solve the maximization problem in the right hand side, i.e.
%
$$
\text{decision}^\star(\text{state}) = \underset{\text{feasible decisions}}{\mathrm{argmax}}\big[ U(\text{state},\text{decision}) + \beta \mathbb{E}\big\{ V(\text{next state}) \big| \text{state},\text{decision} \big\} \big]
$$
%
```

- DP is a the main tool in analyzing modern micro and macto economic models
- DP provides a framework to study decision making over time and under uncertainty and can accommodate:
    - learning and human capital formation
    - wealth accumulation
    - health dynamics
    - strategic interactions between agents (game theory)
    - market interactions (equilibrium theory)
    - etc

- Many important problems and economic models are analyzed and solved
using dynamic programming: 

    - Dynamic models of labor supply 
    - Job search
    - Human capital accumulation 
    - Health process, insurance and long term care 
    - Consumption/savings choices 
    - Durable consumption 
    - Growth models
    - Heterogeneous agents models 
    - Overlapping generation models

**Origin of the term Dynamic Programming**

> The 1950‚Äôs were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defence, and he actually had a pathological fear and hatred of the word ‚Äúresearch‚Äù.\
> I‚Äôm not using the term lightly; I‚Äôm using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term, research, in his presence. You can imagine how he felt, then,
about the term, mathematical. \
> Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. \
> What title, what name, could I choose? \
> In the first place, I was interested in planning, in decision-making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word, ‚Äúprogramming‚Äù. \
> I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. \
> I thought, let‚Äôs kill two birds with one stone. Let‚Äôs take a word which has an absolutely precise meaning, namely dynamic, in the classical physical sense. \
> It also has a very interesting property as an adjective, and that is it‚Äôs impossible to use the word, dynamic, in the pejorative sense. \
> Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities. \
> üìñ Bellman‚Äôs autobiography ‚ÄúThe Eye of the Hurricane‚Äù 

### Finite and infinite horizon problems

Although dynamic programming primarily deals with infinite horizon problems, it can also be applied to the finite horizon problems where $T < \infty$
  
  - Terms **backwards induction** and **dynamic programming** are associated by many authors respectively with finite and infinite horizon problems

  - Our definition of a dynamic optimization problem contains elements of both finite and infinite horizon problem setup, and needs more detail

***Finite horizon problems***

- Time is discrete and finite, $t \in \{0,1,\dots,T\}$ where $T < \infty$
- Parameter vector $\theta$ indeed contains $t$ such that the value and policy functions along with other elements of the model do depend on $t$, i.e. may differ between time periods
- "Bellman equation" is a (common!) misuse of the term, because with time subscripts for the value function, it is not a functional equation any more
- However, the term "Bellman equation" is still used for the finite horizon problems to denote the following expression:
%
$$
V_t(s_t) = 
\begin{cases}
\max_{x \in \mathcal{D}_t(s_t)} \big[ f(x,s_t) + \beta V_{t+1}(s_{t+1}) \big] ,& \text{ if } t<T \\
\max_{x \in \mathcal{D}_T(s_T)} f(x,s_T) ,& \text{ if } t=T 
\end{cases}
$$
%
- Period $t=T$ is referred to as the *terminal period*, and the value function at this period is called the *terminal value function*
- Because there is no next period in the terminal period, $V_{T+1}(s_{T+1}) = 0$ for all $s_T$, giving the expression for the Bellman equation above

***Infinite horizon problems***

- Time is discrete and infinite, $t \in \{0,1,\dots,\infty\}$, sometimes denoted as $T = \infty$
- Parameter vector $\theta$ does not contain $t$ and so the value and policy functions can not depend on $t$, and so other elements of the model
- In other words, the solution of the model in this case has to be time invariant
- Bellman equation is a proper functional equation, which together with the policy function, describes the optimal decision rule in the long term. In some sense, the solution to the infinite dimensional problem is one pair $(V(s), x^\star(s))$, whereas in the general setup both elements of the solution are tuples
%
$$
V(s)
= \max_{x \in \mathcal{D}(s)} \big[ f(x,s) + \beta V(s') \big]
$$
%


Let's look at a particular example

### Inventory management in finite horizon

```{admonition} Example
:class: tip

The manager of a store has to order the stock of a product from a wholesales company as the inventory is depleted by sales.

The manager can order any amount of the product at any time, but there is a fixed cost $c$ of ordering any amount of new inventory. The manager can also store the product in the warehouse, but there is a cost $r$ of storing one unit of the product.

The store receives a profit $p$ for each unit of the product sold and so the manager's goal is to maximize the net profit over the time horizon $t=0,\dots,T$.

We have:

- $p$ is the profit per one unit of sold good
- $c$ is the fixed cost of ordering any amount of new inventory
- $r$ is the cost of storing one unit of good

Let's introduce some notation:

- $x_t\ge 0$ is inventory at period $t$, let it be measured in *discrete units*
- $d_t\ge 0$ is *potentially stochastic* demand at period $t$
- $q_t\ge 0$ is the order of new inventory

Then:
- the sales in period $t$ are given by $\min\{x_t,d_t\}$
- inventory to be stored till next period is given by 
%
$$
\max\{x_t-d_t,0\} + q_t = x_t - \min\{x_t,d_t\} + q_t = x_{t+1}
$$

- profit in period $t$ is given by
%
$$
\begin{array}{rcl}
\pi_t & = & p \cdot \text{sales}_t - r \cdot \text{storage}_t - c \cdot (\text{order made in period }t) \\
& = & 
p \min\{x_t,d_t\} - r (x_t - \min\{x_t,d_t\}) - c \mathbb{1}\{q_t>0\} \\
& = & 
(p+r) \min\{x_t,d_t\} - r x_t - c \mathbb{1}\{q_t>0\}
\end{array}
$$
%

- assuming all $q_t \ge 0$, we have $\mathcal{D}_t = \{q_t \colon q_t \ge 0 \}$
- to make the set of feasible choices *compact* we may assume a maximum level of inventory $x_{\max}$, and so $\mathcal{D}_t = \{q_t \colon 0 \le q_t \le x_{\max} \}$

The expected profit maximizing problem is given by
%
$$
\sum_{t=0}^{T} \beta^t \pi_t
\to \max_{\{q_t\}_{t=0}^T \in \times _{t=0}^T \mathcal{D}_t}
$$
%
or in the case of stochastic with the expectation over its distribution
%
$$
\mathbb{E}\Big[ \sum_{t=0}^{T} \beta^t \pi_t \Big]
\to \max_{\{q_t\}_{t=0}^T \in \times _{t=0}^T \mathcal{D}_t}
$$
%
where $\beta$ is discount factor
```

```{note}

The maximization problem is formulated with respect to a series of choices $\{q_t\}_{t=0}^T$ which is a tuple of $T+1$ elements, one for each period $t=0,\dots,T$.
Given that in each period the parameters of the optimization problem change, at least potentially, we will keep in mind that the elements of this series should be thought of as dependent on the corresponding parameters, as in the theorem of maximum. In this case the relevant parameter is the state variable $s_t$, and thus we will write $\{q_t(s_t)\}_{t=0}^T$.
```

***Bellman equation for the problem***

Decisions: $q_t$, how much new inventory to order

What is important for the inventory decision at time period $t$?
- timing of the decision making process: (beginning of period) - current inventory - demand - (choice) order - stored inventory - storage during the period $\rightarrow$ (beginning of the next of period)
- instanteneous utility (profit) contains $x_t$ and $d_t$

So, both $x_t$ and $d_t$ are taken into account for the new order to be made, forming the state space

Therefore, for $t<T$, we can write the following Bellman equation
%
$$
\begin{array}{rcl}
V_t(x_t,d_t) &=& \max_{q_t \ge 0} \Big\{ \pi_t + \beta \mathbb{E}\Big[ V_{t+1}\big(x_{t+1} , d_{t+1} \big) \Big| x_t,d_t,q_t \Big] \Big\} \\
&=& \max_{q_t \ge 0} \Big\{ 
(p+r) \min\{x_t,d_t\} - r x_t - c \mathbb{1}\{q_t>0\}
+ \beta \mathbb{E}\Big[ V_{t+1}\big(x_{t+1}, d_{t+1} \big) \Big] \Big\}
\end{array}
$$
%
%
$$
x_{t+1} = x_t - \min\{x_t,d_t\} + q_t
$$
%

The expectation in the Bellman equation is taken over the distribution of the next period demand $d_{t+1}$, which we assume is independent of any other variables and across time (idiosyncratic), thus the expectation is not conditioned on current state and choice variable $(x_t,d_t,q_t)$ (although in general it would be)

Expectation can be written as an integral over the distribution of demand $F(d)$, but since inventory is discrete it's natural to assume demand is as well
Then the integral then transforms into a sum over the possible value of demand, weighted by their probabilities $pr(d)$
%
$$
V_t(x_t,d_t) =
\max_{q_t \ge 0} \Big\{ 
(p+r) \min\{x_t,d_t\} - r x_t - c \mathbb{1}\{q_t>0\}
+ \beta \sum_d V_{t+1}\big(x_{t+1}, d \big) pr(d) \Big\}
$$
%

Because the problem is set in finite time, for $t=T$ the Bellman takes the simple form without the next period value
%
$$
V_T(x_T,d_T) =
\max_{q_T \ge 0} \Big\{ 
(p+r) \min\{x_T,d_T\} - r x_T - c \mathbb{1}\{q_T>0\}\Big\}
$$
%
This part of the Bellman equation can be solved immediately: it is obvious that making any order in the terminal period only brings on the fixed cost of order, and therefore the optimal choice is $q_T^\star = 0$, leading to
%
$$
V_T(x_T,d_T) = (p+r) \min\{x_T,d_T\} - r x_T
$$
%

**Solving the deterministic version** 

Let $d_t =d$ be fixed and constant across time. How does the Bellman equation change?

In the deterministic case with fixed $d$, it can be simply dropped from the state space, and the Bellman equation can be simplified to
%
$$
V_t(x_t) =
\max_{q_t \ge 0} \Big\{ 
(p+r) \min\{x_t,d\} - r x_t - c \mathbb{1}\{q_t>0\}
+ \beta V_{t+1}\big(x_t - \min\{x_t,d\} + q_t \big) \Big\}
$$
%

```{admonition} Backwards induction algorithm
:class: important
:name: backwards-induction-algorithm

Solver for the finite horizon dynamic programming problems

1. Start at $t=T$, assume $V_T(s_T)=0$
2. Solve Bellman equation at $t$ using the already known $V_{t+1}(s_{t+1})$
3. Record optimal choice (policy function) $x^\star_t(s_t)$ and the value function $V_t(s_t)$
4. While $t>1$, decrease $t$ by 1 and return to step 2, otherwise stop

```

We already solved the model in terminal period above and determined that $q_T^\star = 0$, so we can start at $t=T-1$ and solve the Bellman equation for $V_{T-1}(x_{T-1})$ and find $q_{T-1}^\star(x_{T-1})$.
Because the choice variable $q_{T-1}$ is discrete (takes values from a finite/countable set), there is no FOCs, and we simply compare the value of the objective at different values of the choice variable, and choose the maximum one.

Let's do this numerically!

```{code-cell} python3
:tags: [hide-input]

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rcParams['figure.figsize'] = [12, 8]

class inventory_model:
    '''Small class to hold model fundamentals and its solution'''

    def __init__(self,label='noname',
                 max_inventory=10,  # upper bound on the state space
                 c = 3.2,            # fixed cost of order
                 p = 2.5,           # profit per unit of good
                 r = 0.5,           # storage cost per unit of good
                 Œ≤ = 0.95,          # discount factor
                 dp = 0.5,          # parameter in geometric distribution of demand
                 demand = 4         # fixed demand
                 ):
        '''Create model with default parameters'''
        self.label=label # label for the model instance
        self.c, self.p, self.r, self.Œ≤, self.dp= c, p, r, Œ≤, dp
        self.demand = demand
        # created dependent attributes (it would be better to have them updated when underlying parameters change)
        self.n = max_inventory+1    # number of inventory levels
        self.upper = max_inventory  # upper boundary on inventory
        self.x = np.arange(self.n)  # all possible values of inventory and demand (state space)

    def __repr__(self):
        '''String representation of the model'''
        return 'Inventory model labeled "{}"\nParamters (c,p,r,Œ≤) = ({},{},{},{})\nDemand={}\nUpper bound on inventory {}' \
               .format (self.label,self.c,self.p,self.r,self.Œ≤,self.demand,self.upper)

    def sales(self,x,d):
        '''Sales in given period'''
        return np.minimum(x,d)

    def next_x(self,x,d,q):
        '''Inventory to be stored, becomes next period state'''
        return x - self.sales(x,d) + q

    def profit(self,x,d,q):
        '''Profit in given period'''
        return self.p * self.sales(x,d) - self.r * self.next_x(x,d,q) - self.c * (q>0)

    def demand_pr(self,plot=False):
        '''Computes stochastic demand probs'''
        k = np.arange(self.n)  # all possible values of demand
        pr = (1-self.dp)**k *self.dp
        pr[-1] = 1 - pr[:-1].sum()  # update last prob to ensure sum=1
        if plot:
            plt.step(self.x,pr,where='mid')
            plt.title('Distribution of demand')
            plt.show()
        return pr
```

```{code-cell} python3

model=inventory_model(label='test')
print(model)
```

```{code-cell} python3
:tags: [hide-input]

def bellman(m,v0):
    '''Bellman equation for inventory model
        Inputs: model object
                next period value function
    '''
    # create the grid of choices (same as x), column-vector
    q = m.x[:,np.newaxis]
    # compute current period profit (relying on numpy broadcasting to get the matrix with choices in rows)
    p = m.profit(m.x,m.demand,q)
    # indexes for next period value with extrapolation using last value
    i = np.minimum(m.next_x(m.x,m.demand,q),m.upper)
    # compute the Bellman maximand
    vm = p + m.Œ≤*v0[i]
    # find max and argmax
    v1 = np.amax(vm,axis=0)   # maximum in every column
    q1 = np.argmax(vm,axis=0) # arg-maximum in every column = order volume
    return v1, q1

def solver_backwards_induction(m,T=10,verbose=False):
    '''Backwards induction solver for the finite horizon case'''
    # solution is time dependent
    m.value = np.zeros((m.n,T))
    m.policy = np.zeros((m.n,T))
    # main DP loop (from T to 1)
    for t in range(T,0,-1):
        if verbose:
            print('Time period %d\n'%t)
        j = t-1 # index of value and policy functions for period t
        if t==T:
            # terminal period: ordering zero is optimal
            m.value[:,j] = m.profit(m.x,m.demand,np.zeros(m.n))
            m.policy[:,j] = np.zeros(m.n)
        else:
            # all other periods
            m.value[:,j], m.policy[:,j] = bellman(m,m.value[:,j+1]) # next period to Bellman
        if verbose:
            print(m.value,'\n')
    # return model with updated value and policy functions
    return m

```

```{code-cell} python3

model = inventory_model(label='illustration')
model=solver_backwards_induction(model,T=5,verbose=True)
print('Optimal policy:\n',model.policy)

```

```{code-cell} python3
---
mystnb:
  image:
    width: 80%
    align: center
tags:
  - hide-input
---

def plot_solution(model):
    plt.step(model.x,model.value,where='mid')
    plt.xlabel(r'Current inventory, $x_t$')
    plt.legend([f't={i+1}' for i in range(model.value.shape[1])])
    plt.title('Value function')
    plt.show()
    plt.step(model.x,model.policy,where='mid')
    plt.xlabel(r'Current inventory, $x_t$')
    plt.legend([f't={i+1}' for i in range(model.policy.shape[1])])
    plt.title('Policy function (optimal order sizes)')
    plt.show()

plot_solution(model)

````

```{code-cell} python3
---
mystnb:
  image:
    width: 80%
    align: center
---

mod = inventory_model(label='production',max_inventory=50)
mod.demand = 15
mod.c = 5
mod.p = 2.5
mod.r = 1.4
mod.Œ≤ = 0.975
mod = solver_backwards_induction(mod,T=15)
plot_solution(mod)

````

```{note}

Do we need to use backwards induction for the model above?

- we could set it as a multivariate optimization w.r.t. the vector $(q_0,\dots,q_T)$
- the objective can be rewritten by plugging in the maximand of in period $t+1$ into the equation for period $t$ using the transition equation $g(\cdot)$

Thus any deterministic infinite horizon problem can be written and solved as a standard constrained optimization problem

But as soon as we introduce uncertainty, this approach breaks down:
- there is no transition equation $g(\cdot)$ to plug in, instead there is a transition probability and an expectation
- the optimization problem in each period depends on the paramters (state variables) in a non-trivial way, and has to be solved separately for each value of parameters

```

## Infinite horizon and stochastic dynamic optimization problems

- $T = \infty$
- subscripts $t$ can be dropped
- solution of the model $V(s), x^\star(s)$ is time invariant
- Bellman equation is a proper functional equation where the value function $V(s)$ is an unknown

In infinite horizon the dynamic optimization problem
%
$$
V(s)
= \max_{x} \big[ f(x,s) + \beta V(s') \big] \\
\text {subject to } x \in \mathcal{D}(s)
$$
%
where $s'$ is the *next period* state,
becomes a **fixed point** problem of a Bellman operator, i.e. the problem of solving a functional equation.

```{admonition} Definition
:class: caution
:name: bellman-operator

Let $B(S)$ denote a set of all bounded real-valued functions on a set $S \subset \mathbb{R}^K$.

**Bellman operator** is a mapping from $B(S)$ to itself defined as
%
$$
T \colon B(S) \ni V \mapsto \max_{x \in \mathcal{D}(s)} 
\big[ f(x,s) + \beta V\big(g(s,x)\big) \big] \in B(S)
$$
%
where:
- $f(x,s) \colon \mathbb{R}^N \times S \to \mathbb{R}$ is an instantaneous reward function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $s \in S \subset \mathbb{R}^K$ are *state variables*
- state transitions are given by $s' = g(x,s)$ where $g \colon \mathbb{R}^N \times S \to S$
- $\mathcal{D}(s) \subset \mathbb{R}^N$ denotes the admissible set
- $\beta \in (0,1)$ is a discount factor
- $V(s) \colon S \to \mathbb{R}$ is a value function

```

Solution of the Bellman equation is given by a fixed point of the Bellman operator, i.e. such function $V(\cdot)$ which is mapped to itself by the Bellman operator, and the corresponding policy function
%
$$
x^\star(s) = \mathrm{argmax}_{x \in \mathcal{D}(s)} \big[ f(x,s) + \beta V\big(g(x,s)\big) \big]
$$
%

### Theory of dynamic programming

```{admonition} Definition: contraction mapping
:class: caution
:name: contraction-mapping

Let $(S,\rho)$ be a complete metric space, i.e. a metric space where every Cauchy sequence converges to a point in $S$.

Let $T: S \rightarrow S$ denote an operator mapping $S$ to itself.
$T$ is called a *contraction* on $S$ with modulus $\lambda$ if $0 \le \lambda < 1$ and 
%
$$
\rho(Tx,Ty) \le \lambda \rho(x,y) \; \forall x,y \in S
$$
%
```

*Contraction mapping brings points in its domain "closer" to each other!*

```{admonition} Example
:class: tip

What is the value of annuity $V$ paying regular payments $c$ forever?
%
$$
 \stackrel{\nearrow}{V} \quad
   \stackrel{\searrow}{c} \quad
   \stackrel{\searrow}{c} \quad
   \stackrel{\searrow}{c} \quad
   \dots
$$
%
Let $r$ be the interest rate, then the value of annuity is given by
%
$$
 V=\quad
   \frac{c}{(1+r)^0} + \quad
   \frac{c}{(1+r)^1} + \quad
   \frac{c}{(1+r)^2} + \quad
   \frac{c}{(1+r)^3} + \quad
   \dots
$$
%
Equivalently with $\beta = \frac{1}{1+r}$
%
$$
V=\quad
c + \quad
c \beta + \quad
c \beta^2 + \quad
c \beta^3 + \quad
\dots
=
\sum_{t=0}^{\infty} \beta^t c
$$
%
Can reformulate recursively (as "Bellman equation" without choice)
%
$$
 V = c + \beta ( c + \beta c + \beta^2 c + \dots ) = c + \beta V
$$
%
with the corresponding "Bellman operator"
%
$$
T \colon V \mapsto c + \beta V
$$
%
Is $T(V)$ a contraction?
%
$$
|T(V_1) - T(V_2)| = |(c + \beta V_1) - (c + \beta V_2)| = \beta | V_1 - V_2 |
$$
%
Yes as long as long as $\beta < 1$!

- contraction mapping under Euclidean norm
- modulus of the contraction is $\beta$
```

Contractions are _invaluable_ because of uniqueness of their fixed point _and_ a surefire algorithm to compute them

```{admonition} Banach contraction mapping theorem (fixed point theorem)
:class: important
:name: contraction-mapping-theorem

Let $(S,\rho)$ be a complete metric space with a contraction mapping $T: S \rightarrow S$.
Then 

1. $T$ admits a unique fixed-point $V^{\star} \in S \colon T(V^{\star}) = V^{\star}$ 
2. $V^{\star}$ can be found by repeated application of the operator $T$, i.e. $T^n(V) \rightarrow V^{\star}$ as $n\rightarrow \infty$

```

In other words, the fixed point can be found by successive approximations from any starting point $\rightarrow$ commonly known in economics as value function iterations (VFI) algorithm

```{admonition} Value function iterations (VFI) algorithm
:class: caution
:name: vfi

1. Start with a guess for value function $V_0$, $i=0$
2. Apply Bellman operator to $V_i$ and increment $i$
%
$$
V_{i+1} = T(V_i)
$$
%
3. Repeat until convergence, i.e. $\| V_{i+1}-V_i \|<\epsilon$ for some small $\e

```


```{code-cell} python3
:tags: [hide-cell]

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

class annuity():

    def __init__(self,c=1,beta=.9):
        self.c = c           # Annual payment
        self.beta = beta     # Discount factor
        self.analytic = c/(1-beta)  # compute analytic solution right away
    
    def bellman(self,V):
        '''Bellman equation'''
        return self.c + self.beta*V

    def solve(self, maxiter = 1000, tol=1e-4, verbose=False):
        '''Solves the model using successive approximations'''
        if verbose: print('{:<4} {:>15} {:>15}'.format('Iter','Value','Error'))
        V0=0
        for i in range(maxiter):
            V1=self.bellman(V0)
            if verbose: print('{:<4d} {:>15.8f} {:>15.8f}'.format(i,V1,V1-self.analytic))
            if abs(V1-V0) < tol:
                break
            V0=V1
        else:  # when i went up to maxiter
            print('No convergence: maximum number of iterations achieved!')
        return V1

````

```{code-cell} python3

a = annuity(c=10,beta=0.92)
a.solve(verbose=True)
print('Numeric solution is ',a.solve())
````

Answer by the geometric series formula, assuming $\beta<1$
%
$$
 V = \sum_{t=0}^{\infty} \beta^t c = \frac{c}{1-\beta}
$$
%

```{code-cell} python3

print(f'Analytic solution is {a.analytic}')
````


**When is Bellman operator a contraction?**

%
$$
T(V)(\text{state}) = \max_{\text{decisions}} \big[ U(\text{state},\text{decision}) + \beta \mathbb{E}\big\{ V(\text{next state}) \big| \text{state},\text{decision} \big\} \big]
$$
%

- Bellman operator $T: U \rightarrow U$ from functional space $U$ to itself
- metric space $(U,d_{\infty})$ with uniform/infinity/sup norm (max abs distance between functions over their domain)



```{admonition} Blackwell sufficient conditions for contraction
:class: important
:name: blackwell-condition

Let $X \subseteq \mathbb{R}^n$ and $B(x)$ be the space of bounded functions $f: X \rightarrow \mathbb{R}$ defined on $X$.
Suppose that $T: B(X) \rightarrow B(X)$ is an operator satisfying the following conditions:

1. (monotonicity) For any $f,g \in B(X)$ and $f(x) \le g(x)$ for all $x\in X$ implies $T(f)(x) \le T(g)(x)$ for all $x\in X$,
2. (discounting) There exists $\beta \in (0,1)$ such that

%
$$
T(f+a)(x) \le T(f)(x) + \beta a, \text{ for all } f\in B(X), a \ge 0, x\in X,
$$
%

Then $T$ is a contraction mapping with modulus $\beta$.

```

- Monotonicity of Bellman equation follows trivially due to maximization in $T(V)(x)$
- Discounting: satisfied by elementary argument when $\beta<1$

Bellman operator is contraction mapping by Blackwell condition as long as the value function is bounded and the discount factor is less than one

- In practical application with the upper bound on the state space, the value function is generally bounded
- In many applications the norm $\rho$ in the metric space can be adjusted to make the value function bounded

$\Rightarrow$

- unique solution
- VFI algorithm is globally convergent
- does not depend on the numerical implementation of the Bellman operator


### Inventory management in infinite horizon

```{admonition} Example
:class: tip

Return to the above example, now with the following modifications:
- time horizon is infinite
- demand is stochastic with known distribution

After dropping the time subscripts the Bellman equation is
%
$$
V(x,d) =
\max_{q \ge 0} \Big\{ 
(p+r) \min\{x,d\} - r x - c \mathbb{1}\{q>0\}
+ \beta \sum_{d'} V\big(x - \min\{x,d\} + q, d' \big) pr(d') \Big\}
$$
%
The sum in the RHS of the equation computes the expectation over realizations of discrete random demands $d'$, each happening with probability $pr(d')$.

We could solve the problem as is, but note that $d$ does not enter the "next period part" of the Bellman equation (due to the assumption that demand is idiosyncratic). In this case we can make our life easier by converting the problem of searching for the fixed point of the Bellman operator to the problem of searching the fixed point of the *expected* Bellman operator.

Taking the expectation of the Bellman equation over the distribution of demand $d$ on both sides, and denoting 
$EV(x) = \sum_{d} V(x,d) pr(d)$, we have
%
$$
EV(x) =
\sum_{d} \left[
\max_{q \ge 0} \Big\{ 
(p+r) \min\{x,d\} - r x - c \mathbb{1}\{q>0\}
+ \beta EV(x - \min\{x,d\} + q) \Big\}
\right ] pr(d)
$$
%
In the expected value function form the Bellman operator is also a contraction, but the fixed point now has to be solved only in the space of $x$ and not $x,d$. This is much easier for computational solver in the code below.

```

```{code-cell} python3
:tags: [hide-input]

def bellman_ev(m,ev0):
    '''Bellman equation for inventory model
       Inputs: model object
               next period EXPECTED value function
    '''
    pr = m.demand_pr()
    ev1 = np.zeros(shape=m.x.shape)
    for j,d in enumerate(m.x):  # over all values of demand
        # create the grid of choices (same as x), column-vector
        q = m.x[:,np.newaxis]
        # compute current period profit (relying on numpy broadcasting to get the matrix with choices in rows)
        p = m.profit(m.x,d,q)
        # indexes for next period value with extrapolation using last value
        i = np.minimum(m.next_x(m.x,d,q),m.upper)
        # compute the Bellman maximand
        vm = p + m.Œ≤*ev0[i]
        # find max and argmax
        v1 = np.amax(vm,axis=0)  # maximum in every column
        ev1 = ev1 + pr[j]*v1
        q1 = None
    return ev1, q1

def solve_vfi(self,tol=1e-6,maxiter=500,callback=None):
    '''Solves the Rust model using value function iterations
    '''
    ev0 = np.zeros(self.n) # initial point for VFI
    for i in range(maxiter):  # main loop
        ev1, q1 = bellman_ev(self,ev0)  # update approximation
        err = np.amax(np.abs(ev0-ev1))
        if callback != None: callback(iter=i,err=err,ev1=ev1,ev0=ev0,q1=q1,model=self)
        if err<tol:
            break  # break out if converged
        ev0 = ev1  # get ready to the next iteration
    else:
        raise RuntimeError('Failed to converge in %d iterations'%maxiter)
    return ev1, q1

def solve_show(self,maxiter=1000,tol=1e-6,**kvargs):
    '''Illustrate solution'''

    fig1, (ax) = plt.subplots(1,1,figsize=(12,8))
    ax.grid(which='both', color='0.65', linestyle='-')
    ax.set_xlabel('Inventory')
    ax.set_title('VFI convergence in the space of expected value functions')
    def callback(**argvars):
        mod, ev, q = argvars['model'],argvars['ev1'],argvars['q1']
        ax.step(mod.x,ev,color='k',alpha=0.25,where='mid')
    ev,pk = solve_vfi(self,maxiter=maxiter,tol=tol,callback=callback,**kvargs)
    # add solutions
    ax.step(self.x,ev,color='r',linewidth=2.5,where='mid')
    plt.show()

def optimal_policy(m,ev):
    '''Computes the optimal policy function for the stochastic
    inventory dynamics model for given EV function'''
    # idea: 3-dim array with q in axes 0, d in axis 1 and x in axis 2
    q = m.x[:,np.newaxis,np.newaxis]  # choices
    d = m.x[np.newaxis,:,np.newaxis]  # demand
    x = m.x[np.newaxis,np.newaxis,:]  # inventories
    # compute current period profit (relying on numpy broadcasting to get the matrix with choices in rows)
    p = m.profit(x,d,q)  # 3-dim array
    # indexes for next period value with extrapolation using last value
    i = np.minimum(m.next_x(x,d,q),m.upper)
    # compute the Bellman maximand
    vm = p + m.Œ≤*ev[i]
    # find argmax and argmax
    return np.argmax(vm,axis=0)  # maximum in every column


```

```{code-cell} python3

# Set up the parameters of the model
mod = inventory_model(label='production',max_inventory=25)
mod.dp=.25
mod.demand = int((1-mod.dp)/mod.dp) # mean for geometric distribution of demand
mod.c = .25
mod.p = 3.5
mod.r = 0.4
mod.Œ≤ = 0.9

# solve the finite horizon version of the model with fixed demand
print(mod)
mod = solver_backwards_induction(mod,T=15)
plot_solution(mod)

# visualize the stochastic demand
mod.demand_pr(plot=True)

# solve the infinite horizon version of the model
ev,q = solve_vfi(mod)
solve_show(mod)
q = optimal_policy(mod,ev)
print('Optimal orders of new inventory for d,x:\n(d in rows, x in columns)')
print(q)
```

```{note}
Note the symmetry in the optimal policy!
This implies that knowing both $x$ and $d$ is not necessary for the optional new order, it's enough to condition on the inventory remaining after sales, i.e. $x-\min(x,d) = \max(0,x-d)$
```

(ref-dynamic-classification)=
## Other classes of dynamic optimization problems

Classification of dynamic problems by various criteria:

1. Nature of time
  - discrete time: here
  - continuous time: studied in the *calculus of variation*, *optimal control theory* and other branches of math

2. Uncertainty in the beliefs of the decision makers
  - deterministic problems: all transition rules are given by laws of motion, i.e. mappings from states and choices to next period states
  - stochastic problems: some transitions rules are given by conditional densities or transition probabilities from current to next period states

3. Nature of choice space
  - discrete: well suited for numerical full enumeration methods (grid search)
  - continuous: optimization theory of this course applied in full
  - mixed discrete-continuous: math is more complicated and interesting

4. Nature of state space
  - discrete: well suited for numerical methods
  - continuous: most often convert to discrete by discretization (grids)
  - mixed discrete-continuous

5. Multiple decision making processes
  - single agent models: here
  - equilibrium models: each decision makers takes environment variables as given, yet the equilibrium environment is formed by the aggregate behavior
  - multiple agents models: strategic interaction between decision makers, very hard because the joint Bellman operator is no more a contraction

Corresponding to many types of dynamic optimization problems there are many-many variations of the solution approaches

For finite horizon dynamic problems:
1. Standard backwards induction: solving Bellman equation sequentially
1. Backwards induction using F.O.C. of Bellman maximization problem (for problems with continuous choice)  
1. Finite horizon version of endogenous gridpoint method (for a particular class of problems)  

For infinite horizon dynamic problems:
1. Value function iterations (successive approximations to solve for the fixed point of Bellman operator)
1. Time iterations (successive approximations to solve for the fixed point of Coleman-Reffett operator in policy function space)  
1. Policy iteration method (Howard‚Äôs policy improvement algorithm, iterative solution for the fixed point of Bellman operator)  
1. Newton-Kantorovich method (Newton solver for the fixed point of Bellman operator)  
1. Endogenous gridpoint method (for a particular class of problems)  


## Extra material

- Computer science view on DP https://www.techiedelight.com/introduction-dynamic-programming
- Popular optimal stopping https://www.americanscientist.org/article/knowing-when-to-stop
- For more details the inventory problem see [my screencast on writing this code](https://youtu.be/JSdjTmaXr0w)
- For more dynamic programming and numerical methods see my [Foundations of Computational Economics](https://fedor.iskh.me/compecon) online course
