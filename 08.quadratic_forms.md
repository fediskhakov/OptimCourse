---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---


NOTES:

determinants?

Linear algebra II: quadratic forms, positive definiteness
Ð¡Ð¼. ÐºÐ¾ÑÐ¿ÐµÐºÑ‚ II Ð¸ III
ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ, ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°
ÐŸÐ¾Ð²Ð¾Ñ€Ð¾Ñ‚ -- ÑÑƒÐ¼Ð¼Ð° ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¾Ð²
ÐšÑ€Ð¸Ð²Ñ‹Ðµ Ð¸ Ð¿Ð¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚Ð¸
Definitness and semi-definiteness



## Determinants


```{admonition} Fact
:class: important

In the $N = 2$ case this definition reduces to

%
$$
%
\det 
\left(
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right)
= ad - bc
%
$$
%
```

```{admonition} Example
:class: tip

%
$$
%
\det 
\left(
\begin{array}{cc}
2 & 0 \\
7 & -1 \\
\end{array}
\right)
= (2 \times -1) - (7 \times 0) = -2
%
$$
%

```

````{admonition} Fact
:class: important

Determinant of $3 \times 3$ matrix can be computed by the **triangles rule**:
%
$$
\mathrm{det}
\left(
\begin{array}{ccc}
a_{11},& a_{12},& a_{13} \\
a_{21},& a_{22},& a_{23} \\
a_{31},& a_{32},& a_{33}
\end{array}
\right) =
\begin{array}{l}
+ a_{11}a_{22}a_{33} \\
+ a_{12}a_{23}a_{31} \\
+ a_{13}a_{21}a_{32} \\
- a_{13}a_{22}a_{31} \\
- a_{12}a_{21}a_{33} \\
- a_{11}a_{23}a_{32}
\end{array}
$$
%

```{figure} _static/img/det33.png
:scale: 100%
```

````

Important facts concerning the determinants

```{admonition} Fact
:class: important

If $I$ is the $N \times N$ identity, $A$ and $B$ are $N \times N$ matrices and $\alpha \in \mathbb{R}$, then
%
1. $\det(I) = 1$ 

2. $A$ is nonsingular if and only if $\det(A)
\ne 0$

3. $\det(AB) = \det(A)
\det(B)$

4. $\det(\alpha A) = \alpha^N \det(A)$

9. $\det(A^{-1}) = (\det(A))^{-1}$
```





## Eigenvalues and Eigenvectors

Let $A$ be $N \times N$ 

In general $A$ mappings $x$ to some arbitrary new location $A x$

But sometimes $x$ will only be **scaled**:
%
$$
%
A x = \lambda x
\quad \text{for some scalar $\lambda$}
%
$$
%

```{admonition} Definition
:class: caution

If $A x = \lambda x$ holds and $x$ is nonzero, then 
%
1. $x$ is called an **eigenvector** of $A$
and $\lambda$ is called an **eigenvalue**

9. $(x, \lambda)$ is called an **eigenpair**

```

Clearly $(x, \lambda)$ is an eigenpair of $A$ $\implies$
$(\alpha x, \lambda)$ is an eigenpair of $A$ for any nonzero $\alpha$

```{admonition} Example
:class: tip

Let
%
$$
%
A =
\begin{pmatrix}
1 & -1 \\
3 & 5
\end{pmatrix}
%
$$
%

Then

%
$$
%
\lambda = 2 
\quad \text{ and } \quad
x
=
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
%
$$
%

form an eigenpair because $x \ne 0$ and

%
$$
%
A x =
\begin{pmatrix}
1 & -1 \\
3 & 5
\end{pmatrix}
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
=
\begin{pmatrix}
2 \\
-2
\end{pmatrix}
= 2
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
=
\lambda x 
%
$$
%
```

```{code-cell} python3
import numpy as np
A = [[1, 2],
     [2, 1]]
eigvals, eigvecs = np.linalg.eig(A)
for i in range(eigvals.size):
  x = eigvecs[:,i]
  lm = eigvals[i]
  print(f'Eigenpair {i}:\n{lm:.5f} --> {x}')
  print(f'Check Ax=lm*x: {np.dot(A, x)} = {lm * x}')
```

```{figure} _static/plots/eigenvecs.png
:width: 50%

The eigenvectors of $A$
```

Consider the matrix 
%
$$
%
R = 
\left(
\begin{array}{cc}
0 & -1 \\
1 & 0 \\
\end{array}
\right)
%
$$
%

Induces counter-clockwise rotation on any point by $90^{\circ}$

```{hint}
The rows of the matrix show where the classic basis vectors are translated to.
```

Hence no point $x$ is scaled

Hence there exists **no** pair $\lambda \in \mathbb{R}$ and $x \ne
0$
such that
%
$$R x = \lambda x$$ 
%

In other words, no **real-valued** eigenpairs exist

```{figure} _static/plots/rotation_1.png
:width: 50%

The matrix $R$ rotates points by $90^{\circ}$
```

```{figure} _static/plots/rotation_2.png
:width: 50%

The matrix $R$ rotates points by $90^{\circ}$
```

But $R x = \lambda x$ can hold **if** we allow
complex values

```{admonition} Example
:class: tip
%
$$
%
\left(
\begin{array}{cc}
0 & -1 \\
1 & 0 \\
\end{array}
\right)
\begin{pmatrix}
1 \\
-i
\end{pmatrix}
=
\begin{pmatrix}
i \\
1
\end{pmatrix}
=
i
\begin{pmatrix}
1 \\
-i
\end{pmatrix}
%
$$
%
```

That is,
%
$$
%
R x = \lambda x
\quad \text{for} \quad
\lambda = i
\quad \text{and} \quad
x = 
\begin{pmatrix}
1 \\
-i
\end{pmatrix}
%
$$
%

Hence $(x, \lambda)$ is an eigenpair provided we admit complex values 

```{note}
We do, since this is standard, but will not go into details in this course
```

```{admonition} Fact
:class: important

For any square matrix $A$ 
%
$$
%
\lambda \text{ is an eigenvalue of } A \; \iff \;
\det(A - \lambda I) = 0
%
$$
%
```

````{admonition} Proof
:class: dropdown


Let $A$ by $N \times N$ and let $I$ be the $N \times N$
identity

We have
%
$$
%
\det(A - \lambda I) = 0
\iff A - \lambda I \text{ is singular}
\\
\iff \exists \, x \ne 0 \text{ such that }
(A - \lambda I) x = 0
\\
\iff \exists \, x \ne 0 \text{ such that }
A x = \lambda x
\\
\iff \lambda 
\text{ is an eigenvalue of } A
%
$$
%
````


```{admonition} Example
:class: tip

In the $2 \times 2$ case,

%
$$
%
A =
\left(
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right)
\quad \implies \quad
A - \lambda I =
\left(
\begin{array}{cc}
a - \lambda & b \\
c & d - \lambda 
\end{array}
\right)
%
$$
%
$$
%
\implies
\det(A - \lambda I) 
= (a - \lambda)(d - \lambda) - bc
\\
= \lambda^2 - (a + d) \lambda + (ad - bc)
%
$$
%

Hence the eigenvalues of $A$ are given by the two roots of 
%
$$
%
\lambda^2 - (a + d) \lambda + (ad - bc) = 0
%
$$
%

Equivalently,
%
$$
%
\lambda^2 - \mathrm{trace}(A) \lambda + \det(A) = 0
%
$$
%
```

<!-- ### Existence of Eigenvalues

Fix $N \times N$ matrix $A$ 

```{admonition} Fact
:class: important

There exist complex numbers $\lambda_1, \ldots, \lambda_N$ such that
```
%
$$
%
\det(A - \lambda I) = \prod_{n=1}^N (\lambda_n - \lambda)
%
$$
%

Each such $\lambda_i$ is an eigenvalue of $A$ because
%
$$
%
\det(A - \lambda_i I) 
= \prod_{n=1}^N (\lambda_n - \lambda_i) 
= 0
%
$$
%

Important: Not all are necessarily distinct --- there can be repeats

```{admonition} Fact
:class: important

Given $N \times N$ matrix $A$ with eigenvalues $\lambda_1, \ldots, \lambda_N$
```
we have
%

- $\det(A) = \prod_{n=1}^N \lambda_n$

- $\mathrm{trace}(A) = \sum_{n=1}^N \lambda_n$

- If $A$ is symmetric, then $\lambda_n \in \mathbb{R}$ for all $n$

9. If $A = \mathrm{diag}(d_1, \ldots, d_N)$, then $\lambda_n = d_n$ for all $n$

Hence $A$ is nonsingular $\iff$ all eigenvalues are nonzero (why?)

```{admonition} Fact
:class: important

If $A$ is nonsingular, then 
```
%
$$
%
\text{eigenvalues of } A^{-1}
= 1/\lambda_1, \ldots, 1/\lambda_N
%
$$
% 
-->



## Diagonalization

Consider a square $N \times N$ matrix $A$

$$
%
A = 
\left(
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1N} \\
a_{21} & a_{22} & \cdots & a_{2N} \\
\vdots & \vdots & & \vdots \\
a_{N1} & a_{N2} & \cdots & a_{NN} \\
\end{array}
\right)
%
$$


```{admonition} Definition
:class: caution

The $N$ elements of the form $a_{nn}$ are called the **principal diagonal**

```

```{admonition} Definition
:class: caution

A square matrix $D$ is called **diagonal** if all entries off the
principal diagonal are zero

%
$$
%
D = 
\left(
\begin{array}{cccc}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & d_N \\
\end{array}
\right)
%
$$
%

Often written as

%
$$
%
D = \mathrm{diag}(d_1, \ldots, d_N) 
%
$$
%

```



```{admonition} Fact
:class: important

If $C = \mathrm{diag}(c_1, \ldots, c_N)$ and $D = \mathrm{diag}(d_1, \ldots,d_N)$ then
%
1. $C + D = \mathrm{diag}(c_1 + d_1, \ldots, c_N + d_N)$

2. $C D = \mathrm{diag}(c_1 d_1, \ldots, c_N d_N)$

3. $D^k = \mathrm{diag}(d^k_1, \ldots, d^k_N)$ for any $k \in \mathbb{N}$

4. $d_n \geq 0$ for all $n$ $\implies$ $D^{1/2}$ exists and equals
%
$$\mathrm{diag}(\sqrt{d_1}, \ldots, \sqrt{d_N})$$
%

5. $d_n \ne 0$ for all $n$ $\implies$ $D$ is nonsingular and 
%
$$D^{-1} = \mathrm{diag}(d_1^{-1}, \ldots, d_N^{-1})$$
%
```



```{admonition} Definition
:class: caution

Square matrix $A$ is said to be **similar** to square matrix $B$ if 
%
$$
%
\exists \text{ invertible matrix $P$ such that }
A = P B P^{-1}
%
$$ 
%

```

```{figure} _static/plots/diagonalize.png
:width: 50%

```

```{admonition} Fact
:class: important

If $A$ is similar to $B$, then $A^t$ is similar to
$B^t$ for all $t \in \mathbb{N}$
```

````{admonition} Proof
:class: dropdown

Proof for case $t=2$: 
%
$$
%
A^2
= A A \\
= P B P^{-1} P B P^{-1} \\
= P B B P^{-1} \\
= P B^2 P^{-1} 
%
$$
%

````

```{admonition} Definition
:class: caution

If $A$ is similar to a diagonal matrix, then $A$ is
called **diagonalizable**

```

```{admonition} Fact
:class: important

Let $A$ be diagonalizable with $A = P D
P^{-1}$ and let
%
1. $D = \mathrm{diag}(\lambda_1, \ldots, \lambda_N)$
9. $p_n = \mathrm{col}_n(P)$
%
Then $(p_n, \lambda_n)$ is an eigenpair of $A$ for each $n$
```

````{admonition} Proof
:class: dropdown

From $A = P D P^{-1}$ we get $A P = P D$

Equating $n$-th column on each side gives 
%
$$
%
A p_n = \lambda_n p_n
%
$$
%

Moreover $p_n \ne 0$ because $P$ is invertible (which facts?)

````

```{admonition} Fact
:class: important

If $N \times N$ matrix $A$ has $N$ distinct eigenvalues
$\lambda_1, \ldots, \lambda_N$, then
$A$ is diagonalizable as $A = P D P^{-1}$ where
%
1. $D = \mathrm{diag}(\lambda_1, \ldots, \lambda_N)$
9. $\mathrm{col}_n(P)$ is an eigenvector for $\lambda_n$
```

```{admonition} Example
:class: tip

Let
%
$$
%
A =
\begin{pmatrix}
1 & -1 \\
3 & 5
\end{pmatrix}
%
$$
%

The eigenvalues of $A$ are 2 and 4, while the eigenvectors are
%
$$
%
p_1 =
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\quad \text{and} \quad
p_2 =
\begin{pmatrix}
1 \\
-3
\end{pmatrix}
%
$$
%

Hence
%
$$
%
A = P \mathrm{diag}(2, 4) P^{-1}
%
$$
%
```


```{code-cell} python3
import numpy as np
from numpy.linalg import inv
A = np.array([[1, -1],
              [3, 5]])
eigvals, eigvecs = np.linalg.eig(A)
D = np.diag(eigvals)
P = eigvecs
print('A =',A,sep='\n')
print('D =',D,sep='\n')
print('P =',P,sep='\n')
print('P^-1 =',inv(P),sep='\n')
print('P*D*P^-1 =',P@D@inv(P),sep='\n')
```



# ðŸ“– Quadratic forms




Up till now we have studied linear functions extensively

Next level of complexity is quadratic maps

```{admonition} Definition
:class: caution

Let ${\bf A}$ be $N \times N$ and symmetric, and let ${\bf x}$ be $N \times 1$

The ***quadratic function*** on $\mathbb{R}^N$ associated with ${\bf A}$ is the map 
%
$$
%
Q \colon \mathbb{R}^N \to \mathbb{R}, \qquad
Q({\bf x}) := {\bf x}' {\bf A} {\bf x} = \sum_{j=1}^N \sum_{i=1}^N a_{ij} x_i x_j
%
$$
%

```

The properties of $Q$ depend on ${\bf A}$

An $N \times N$ symmetric matrix ${\bf A}$ is called

1. ***nonnegative definite*** if ${\bf x}' {\bf A} {\bf x} \geq 0$
for all ${\bf x} \in \mathbb{R}^N$ 

- ***positive definite*** if ${\bf x}' {\bf A} {\bf x} > 0$ for all ${\bf x}
\in \mathbb{R}^N$ with ${\bf x} \ne {\bf 0}$

- ***nonpositive definite*** if ${\bf x}' {\bf A} {\bf x} \leq 0$
for all ${\bf x} \in \mathbb{R}^N$

- ***negative definite*** if ${\bf x}' {\bf A} {\bf x} < 0$ for all ${\bf x}
\in \mathbb{R}^N$ with ${\bf x} \ne {\bf 0}$

%

```{figure} _static/plots/qform_pd.png
:name: f:qform_pd

A positive definite case: $Q({\bf x}) = {\bf x}' {\bf I} {\bf x}$ 
```

```{figure} _static/plots/qform_nd.png
:name: f:qform_nd

A negative definite case: $Q({\bf x}) =
{\bf x}'(-{\bf I}){\bf x}$ 
```

Note that some matrices have none of these properties

- ${\bf x}' {\bf A} {\bf x} < 0$ for some ${\bf x}$

- ${\bf x}' {\bf A} {\bf x} > 0$ for other ${\bf x}$

In this case ${\bf A}$ is called ***indefinite***

```{figure} _static/plots/qform_indef.png
:name: f:qform_indef

Indefinite quadratic function $Q({\bf x}) = x_1^2/2 +
8 x_1 x_2 + x_2^2/2$ 
```

```{admonition} Fact
:class: important

A symmetric matrix ${\bf A}$ is 
%
1. positive definite $\iff$ all eigenvalues are strictly positive

2. negative definite $\iff$ all eigenvalues are strictly negative

3. nonpositive definite $\iff$ all eigenvalues are nonpositive

4. nonnegative definite $\iff$ all eigenvalues are nonnegative
%
```

It follows that 
%
- ${\bf A}$ is positive definite $\implies$ $\det({\bf A}) > 0$ 

In particular, ${\bf A}$ is nonsingular



## Concavity and uniqueness of optima

Uniqueness of optima is directly connected to convexity / concavity

- Convexity is a shape property for sets

- Convexity and concavity are shape properties for functions

However, only one fundamental concept: convex sets

### Convex Sets

```{admonition} Definition
:class: caution
:name: convex-set

A set $C \subset \mathbb{R}^K$ is called ***convex*** if
%
$$
%
{\bf x}, {\bf y} \text{ in } C \text{ and } 0 \leq \lambda \leq 1
\; \implies \;
\lambda {\bf x} + (1 - \lambda) {\bf y} \in C
%
$$

```

Remark: This is vector addition and scalar multiplication

Convexity $\iff$ line between any two points in $C$ lies in $C$

```{figure} _static/plots/convex.png
:scale: 50%

```

A non-convex set

```{figure} _static/plots/non_convex.png
:scale: 50%

```

```{admonition} Example
:class: tip

The "positive cone" $P := \{ {\bf x} \in \mathbb{R}^K \colon {\bf x } \geq {\bf 0} \}$ is convex

```

````{admonition} Proof
:class: dropdown

To see this, pick any ${\bf x}$, ${\bf y}$ in $P$ and any $\lambda \in [0, 1]$

Let ${\bf z} := \lambda {\bf x} + (1 - \lambda) {\bf y}$ and let $z_k :=
{\bf e}_k' {\bf z}$

Since 
%
- $z_k = \lambda x_k + (1 - \lambda) y_k$
- $x_k \geq 0$ and $y_k \geq 0$

It is clear that $z_k \geq 0$ for all $k$

Hence ${\bf z} \in P$ as claimed

````

```{admonition} Example
:class: tip

Every $\epsilon$-ball in $\mathbb{R}^K$ is convex.
```

````{admonition} Proof
:class: dropdown

Fix ${\bf a} \in \mathbb{R}^K$, $\epsilon > 0$ and let
$B_\epsilon({\bf a})$ be the $\epsilon$-ball

Pick any ${\bf x}$, ${\bf y}$ in $B_\epsilon({\bf a})$ and any $\lambda \in [0, 1]$

The point $\lambda {\bf x} + (1 - \lambda) {\bf y}$ lies in
$B_\epsilon({\bf a})$ because
%
$$
%
\| \lambda {\bf x} + (1 - \lambda) {\bf y} - {\bf a} \|
= \| \lambda {\bf x} - \lambda {\bf a} 
+ (1 - \lambda) {\bf y} - (1 - \lambda) {\bf a} \| \leq
\\
\leq \| \lambda {\bf x} - \lambda {\bf a} \|
+ \| (1 - \lambda) {\bf y} - (1 - \lambda) {\bf a} \| =
\\
= \lambda \| {\bf x} - {\bf a} \|
+ (1 - \lambda) \| {\bf y} - {\bf a} \| <>
\\
< \lambda \epsilon + (1 - \lambda) \epsilon =
\\
= \epsilon
%
$$
%

````

```{admonition} Example
:class: tip

Let ${\bf p} \in \mathbb{R}^K$ and let $M$ be the "half-space"

%
$$
%
M := \{ {\bf x} \in \mathbb{R}^K \colon {\bf p }' {\bf x} \leq m\}
%
$$
%

The set $M$ is convex
```

````{admonition} Proof
:class: dropdown

Let ${\bf p}$, $m$ and $M$ be as described

Fix ${\bf x}$, ${\bf y}$ in $M$ and $\lambda \in [0, 1]$ 

Then $\lambda {\bf x} + (1 - \lambda) {\bf y} \in M$ because
%
$$
%
{\bf p}'[\lambda {\bf x} + (1 - \lambda) {\bf y} ] =
\lambda {\bf p}'{\bf x} + (1 - \lambda) {\bf p}'{\bf y} 
\leq \lambda m + (1 - \lambda) m
= m
%
$$
%

Hence $M$ is convex
````


```{admonition} Fact
:class: important

If $A$ and $B$ are convex, then so is $A \cap B$
```

````{admonition} Proof
:class: dropdown

Let $A$ and $B$ be convex and let $C := A \cap B$

Pick any ${\bf x}$, ${\bf y}$ in $C$ and any $\lambda \in [0, 1]$

Set 
%
$${\bf z} := \lambda {\bf x} + (1 - \lambda) {\bf y}$$
%

Since ${\bf x}$ and ${\bf y}$ lie in $A$ and $A$ is convex we have ${\bf z}
\in A$

Since ${\bf x}$ and ${\bf y}$ lie in $B$ and $B$ is convex we have ${\bf z}
\in B$

Hence ${\bf z} \in A \cap B$

````

```{admonition} Example
:class: tip

Let ${\bf p} \in \mathbb{R}^K$ be a vector of prices and consider the budget set
%
$$
%
B(m) := \{ {\bf x} \in \mathbb{R}^K \colon {\bf x } \geq {\bf 0} \text{ and }
{\bf p}' {\bf x} \leq m\}
%
$$
%

The budget set $B(m)$ is convex
```

````{admonition} Proof
:class: dropdown

To see this, note that $B(m) = P \cap M$ where
%
$$
%
P := \{ {\bf x} \in \mathbb{R}^K \colon {\bf x } \geq {\bf 0} \}
\qquad
M := \{ {\bf x} \in \mathbb{R}^K \colon {\bf p }' {\bf x} \leq m\}
%
$$
%

We already know that
%
- $P$ and $M$ are convex, intersections of convex sets are convex

Hence $B(m)$ is convex

````

### Convex Functions

Let $A \subset \mathbb{R}^K$ be a convex set and let $f$ be a function from $A$ to $\mathbb{R}$

```{admonition} Definition
:class: caution
:name: convex-function

$f$ is called ***convex*** if 
%
$$
%
f(\lambda {\bf x} + (1 - \lambda) {\bf y})
\leq \lambda f({\bf x}) + (1 - \lambda) f({\bf y})
%
$$
%
for all ${\bf x}, {\bf y} \in A$ and all $\lambda \in [0, 1]$

```

```{admonition} Definition
:class: caution

$f$ is called ***strictly convex*** if 
%
$$
%
f(\lambda {\bf x} + (1 - \lambda) {\bf y})
< \lambda f({\bf x}) + (1 - \lambda) f({\bf y})
%
$$
%
for all ${\bf x}, {\bf y} \in A$ with ${\bf x} \ne {\bf y}$ and all $\lambda \in (0, 1)$

```

```{figure} _static/plots/convex_function.png
:name: 

A strictly convex function on a subset of $\mathbb{R}$
```

```{admonition} Fact
:class: important

$f \colon A \to \mathbb{R}$ is convex if and only if its ***epigraph*** (aka supergraph)
%
$$
%
E_f := \{ ({\bf x}, y) \in A \times \mathbb{R} \colon f({\bf x \}) \leq y}
%
$$
%
is a convex subset of $\mathbb{R}^K \times \mathbb{R}$
```

```{figure} _static/plots/epigraph.png
:name: 

```

```{figure} _static/plots/qform_pd.png
:name: 

A strictly convex function on a subset of $\mathbb{R}^2$
```

```{admonition} Example
:class: tip

$f({\bf x}) = \|{\bf x}\|$ is convex on $\mathbb{R}^K$ 
```

````{admonition} Proof
:class: dropdown

To see this recall that, by the properties of norms,
%
$$
%
\|\lambda {\bf x} + (1 - \lambda) {\bf y}\|
\leq \|\lambda {\bf x}\| + \|(1 - \lambda) {\bf y}\|
\\
= \lambda \|{\bf x}\| + (1 - \lambda) \|{\bf y}\|
%
$$
%
That is,
%
$$
%
f(\lambda {\bf x} + (1 - \lambda) {\bf y})
\leq 
\lambda f({\bf x}) + (1 - \lambda) f({\bf y})
%
$$
%

````

```{admonition} Example
:class: tip

$f(x) = \cos(x)$ is ***not*** convex on $\mathbb{R}$ because

%
$$
%
1 = f(2\pi) = f(\pi/2 + 3\pi/2) > f(\pi)/2 + f(3\pi)/2 = -1
%
$$
%
```

```{admonition} Fact
:class: important

If ${\bf A}$ is $K \times K$ and positive definite, then 
%
$$
%
Q({\bf x}) = {\bf x}' {\bf A} {\bf x}
\qquad ({\bf x} \in \mathbb{R}^K)
%
$$
%
is strictly convex on $\mathbb{R}^K$
```

````{admonition} Proof
:class: dropdown

Proof: Fix ${\bf x}, {\bf y} \in \mathbb{R}^K$ with ${\bf x} \ne {\bf y}$ and $\lambda \in (0, 1)$

**Exercise:** Show that 
%
$$
%
\lambda Q({\bf x}) + (1 - \lambda) Q({\bf y})
- Q(\lambda {\bf x} + (1 - \lambda) {\bf y})
\\
= \lambda (1 - \lambda) ({\bf x} - {\bf y})' {\bf A} ({\bf x} - {\bf y})
%
$$
%

Since ${\bf x} - {\bf y} \ne {\bf 0}$ and $0 < \lambda < 1$, the right
hand side is $> 0$

Hence
%
$$
%
\lambda Q({\bf x}) + (1 - \lambda) Q({\bf y})
> Q(\lambda {\bf x} + (1 - \lambda) {\bf y})
%
$$
%
````


### Concave Functions

Let $A \subset \mathbb{R}^K$ be a convex and let $f$ be a function from $A$ to $\mathbb{R}$

```{admonition} Definition
:class: caution

$f$ is called ***concave*** if 
%
$$
%
f(\lambda {\bf x} + (1 - \lambda) {\bf y})
\geq \lambda f({\bf x}) + (1 - \lambda) f({\bf y})
%
$$
%
for all ${\bf x}, {\bf y} \in A$ and all $\lambda \in [0, 1]$

```

```{admonition} Definition
:class: caution

$f$ is called ***strictly concave*** if 
%
$$
%
f(\lambda {\bf x} + (1 - \lambda) {\bf y})
> \lambda f({\bf x}) + (1 - \lambda) f({\bf y})
%
$$
%
for all ${\bf x}, {\bf y} \in A$ with ${\bf x} \ne {\bf y}$ and all $\lambda \in (0, 1)$

```

**Exercise:** Show that 
%
1. $f$ is concave if and only if $-f$ is convex

9. $f$ is strictly concave if and only if $-f$ is strictly convex

```{admonition} Fact
:class: important

$f \colon A \to \mathbb{R}$ is concave if and only if its ***hypograph*** (aka subgraph)
%
$$
%
H_f := \{ ({\bf x}, y) \in A \times \mathbb{R} \colon f({\bf x \}) \geq y}
%
$$
%
is a convex subset of $\mathbb{R}^K \times \mathbb{R}$
```

```{figure} _static/plots/hypograph.png
:name: 

```

### Preservation of Shape

Let $A \subset \mathbb{R}^K$ be convex and let $f$ and $g$ be functions from $A$
to $\mathbb{R}$

```{admonition} Fact
:class: important

If $f$ and $g$ are convex (resp., concave) and $\alpha \geq 0$, then
%
- $\alpha f$ is convex (resp., concave)
- $f + g$ is convex (resp., concave)
```

```{admonition} Fact
:class: important

If $f$ and $g$ are strictly convex (resp., strictly concave) and $\alpha > 0$, then
%
- $\alpha f$ is strictly convex (resp., strictly concave)
- $f + g$ is strictly convex (resp., strictly concave)
```

Let's prove that $f$ and $g$ convex $\implies h := f + g$ convex

Pick any ${\bf x}, {\bf y} \in A$ and $\lambda \in [0, 1]$

We have
%
$$
%
& 
h(\lambda {\bf x} + (1 - \lambda) {\bf y})
= f(\lambda {\bf x} + (1 - \lambda) {\bf y})
+ g(\lambda {\bf x} + (1 - \lambda) {\bf y})
\\ &
\leq 
\lambda f({\bf x}) + (1 - \lambda) f({\bf y})
+
\lambda g({\bf x}) + (1 - \lambda) g({\bf y})
\\ &
=
\lambda [f({\bf x}) + g({\bf x})]
+ (1 - \lambda) [f({\bf y}) + g({\bf y})]
\\ &
=
\lambda h({\bf x}) + (1 - \lambda) h({\bf y})
%
$$
%

Hence $h$ is convex


## Uniqueness of Optimizers

```{admonition} Fact
:class: important
:name: general-uniqueness

Let $A \subset \mathbb{R}^K$ be convex and let $f \colon A \to \mathbb{R}$

1. If $f$ is strictly convex, then $f$ has at most one minimizer on $A$

9. If $f$ is strictly concave, then $f$ has at most one maximizer on $A$

```


Interpretation, strictly concave case:

- we don't know in general if $f$ has a maximizer

- but if it does, then it has exactly one

- in other words, we have uniqueness

````{admonition} Proof
:class: dropdown

Proof for the case where $f$ is strictly concave:

Suppose to the contrary that 
%
- ${\bf a}$ and ${\bf b}$ are distinct points in $A$

- both are maximizers of $f$ on $A$

By the def of maximizers, $f({\bf a}) \geq f({\bf b})$ and $f({\bf b}) \geq f({\bf a})$

Hence we have $f({\bf a}) = f({\bf b})$

By strict concavity, then
%
$$
%
f\left( \frac{1}{2} {\bf a} + \frac{1}{2} {\bf b} \right)
> \frac{1}{2} f( {\bf a}) + \frac{1}{2} f( {\bf b})
= \frac{1}{2} f( {\bf a}) + \frac{1}{2} f( {\bf a})
= f({\bf a})
%
$$
%

This contradicts the assumption that ${\bf a}$ is a maximizer

````

### A Sufficient Condition

We can now restate more precisely optimization results stated in the
introductory lectures

Let $f \colon A \to \mathbb{R}$ be a $C^2$ function where $A \subset \mathbb{R}^K$
is open, convex

Recall that ${\bf x}^* \in A$ is a stationary point of $f$ if
%
$$
%
\frac{\partial}{\partial x_i} 
f({\bf x}^*)
= 0
\quad \text{for all $i$ in } 1, \ldots, K
%
$$
%

```{admonition} Fact
:class: important
:name: general-stationary-uniqueness

If $f$ and $A$ are as above and ${\bf x}^* \in A$ is stationary, then
%
1. $f$ strictly concave $\implies$ ${\bf x}^*$ is the unique maximizer of $f$ on $A$

2. $f$ strictly convex $\implies$ ${\bf x}^*$ is the unique
minimizer of $f$ on $A$
```

%

```{figure} _static/plots/concave_max.png
:name: 

```

(references_reading)=
## References and reading

```{dropdown} References
- {cite:ps}`simon1994`: 6.1, 6.2, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 11, 16, 23.7, 23.8
- {cite:ps}`sundaram1996`: Appendix C.1, C.2
```

```{dropdown} Further reading and self-learning

- Excellent visualizations of concepts covered in this lecture, strongly recommended for further study
[3Blue1Brown: Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

```
