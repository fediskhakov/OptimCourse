---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---


# üìñ Envelope theorem

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>

## Value function and parameters of optimization problems

Let's start with recalling the definition of a general optimization problem

```{admonition} Definition
:class: caution

The general form of the optimization problem is

%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to}
\\
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}\\
h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $h_j(x,\theta) \le 0, \; j\in\{1,\dots,J\}$ where $h_j \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are inequality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function
```

This lecture focuses on the *value function* in the optimization problem $V(\theta)$, and how it depends on the parameters $\theta$.

In economics we are interested how the optimized behavior changes when the circumstances of the decision-making process change
- income/budget/wealth changes
- intertemporal effects of changes in other time periods

We would like to establish the properties of the value function $V(\theta)$:

- continuity $\rightarrow$ ***The maximum theorem*** (*not covered here*, see additional lecture notes)
- changes/derivative (*if differentiable*) $\rightarrow$ ***Envelope theorem***
- monotonicity $\rightarrow$ Supermodularity and increasing differences (*not covered here*, see Sundaram ch.10)

## Unconstrained optimization case

Let's start with an unconstrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

```{admonition} Envelope theorem for unconstrained problems
:class: important
:name: envelope-unconstrained

Let $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ be a differentiable function, and $x^\star(\theta)$ be the maximizer of $f(x,\theta)$ for every $\theta$.
Suppose that $x^\star(\theta)$ is differentiable function itself.
Then the value function of the problem $V(\theta) = f\big(x^\star(\theta),\theta)$ is differentiable w.r.t. $\theta$ and
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial f}{\partial \theta_j} \big(x^\star(\theta),\theta\big),
\forall j
$$

```

In other words, the marginal changes in the value function are given by the partial derivative of the objective function evaluated at the maximizer.


````{admonition} Proof
:class: dropdown

See Theorem 19.4 in Simon and Blume (1994), pp. 453-454
````

```{note}
When $K=1$, so that $\theta$ is a scalar, the envelope theorem can be written as
%
$$
\frac{d f\big(x^\star(\theta),\theta)}{d \theta} =
\frac{\partial f}{\partial \theta} \big(x^\star(\theta),\theta\big)
$$
% 
so that the meaning is carried only by the derivative symbol change
```

```{admonition} Example
:class: tip

Consider $f(x,a) = -x^2 +2ax +4a^2 \to \max_x$. \
What is the (approximate) effect of a unit increase in $a$ on the attained maximum?

FOC: $-2x+2a=0$, giving $x^\star(a) = a$.

So, $V(a) = f(a,a) = 5a^2$, and $V'(a)=10a$. The value increases at a rate of $10a$ per unit increase in $a$.

Using the envelope theorem, we could go directly to
%
$$
V'(a) = \frac{\partial f}{\partial a} (x^\star(a),a) = 2x + 8a \Big|_{x=a} = 10a
$$

```

## Constrained optimization case

```{admonition} Envelope theorem for constrained problems
:class: important
:name: envelope-constrained

Consider an equality constrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to}
\\
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}\\
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Assume that the maximizer correspondence $\mathcal{D}^\star(\theta) = \mathrm{arg}\max f(x,\theta)$ is *single-valued* and can be represented by the function $x^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^N$, with the corresponding Lagrange multipliers $\lambda^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^I$.

Assume that both $x^\star(\theta)$ and $\lambda^\star(\theta)$ are differentiable, and that the constraint qualification assumption holds.
Then 
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial \mathcal{L}}{\partial \theta_j} \big(x^\star(\theta),\lambda^\star(\theta),\theta\big),
\forall j
$$
%
where $\mathcal{L}(x,\lambda,\theta)$ is the Lagrangian of the problem.
```

````{admonition} Proof
:class: dropdown

Here is a version of proof. The Lagrangian is

$$
\mathcal{L}(x^\star(\theta),\lambda^\star(\theta), \theta) = 
f(x^\star(\theta),\theta)- \sum_{i=1}^{I}\lambda_{i}^\star(\theta) g_{i}(x^\star(\theta), \theta )
$$

Start by exploring its partial derivatives with respect to $\theta_{j}$, $j=1,\dots,K$ 

$$
\frac{\partial\mathcal{L}(x^\star(\theta), \lambda^\star(\theta), \theta)}{\partial{\theta_{j}}}
&= 
\frac{\partial f(x^\star(\theta), \theta)}{\partial{\theta_{j}}} 
- \sum_{i=1}^{I} \left[ \frac{\partial\lambda_{i}^\star(\theta)}{\partial \theta_{j}}g_{i}(x^\star(\theta), \theta) + \lambda_{i}^\star(\theta) \frac{\partial g_{i}(x^\star(\theta), \theta) }{\partial \theta_{j}}\right] \\
&= \frac{\partial f(x^\star(\theta), \theta)}{\partial{\theta_{j}}} 
- \sum_{i=1}^{I} \lambda_{i}^\star(\theta) \frac{\partial g_{i}(x^\star(\theta), \theta) }{\partial \theta_{j}} 
$$

We use the fact that the equality constraints $g_{i}(x^\star(\theta), \theta)=0$ hold for all $i$.

Now we differentiate the value function w.r.t. $\theta_{j}$ and show that it is equal to the same expression as above.

$$
\frac{\partial V(\theta)}{\partial{\theta_{j}}} = 
\frac{d f(x^\star(\theta),\theta)}{d \theta_{j}} =
\sum_{n=1}^{N} \frac{\partial f(x^\star(\theta), \theta)}{\partial x_{n}} \frac{\partial x_n(\theta)}{\partial \theta_{j}} 
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}} 
=
$$

Here we use the fact that first order conditions hold at $(x^\star,\lambda^\star)$ and we have

$$
\begin{array}{rl}
0 = \frac{\partial\mathcal{L}(x^\star(\theta), \lambda^\star(\theta), \theta)}{\partial{x_n}}
&= 
\frac{\partial f(x^\star(\theta), \theta)}{\partial{x_n}} 
- \sum_{i=1}^{I} \lambda_{i}^\star(\theta) \frac{\partial g_{i}(x^\star(\theta), \theta) }{\partial x_{n}}, \\
\end{array}
$$

Continuing the main line

$$
= \sum_{n=1}^{N} \left(\sum_{i=1}^{I}\lambda_{i} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial x_{n}} \right)\frac{\partial x_{n}(\theta)}{ \partial \theta_{j}} 
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}} 
=\\=
\sum_{i=1}^{I}\lambda_{i} \sum_{n=1}^{N} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial x_{n}}\frac{\partial x_{n}(\theta)}{\partial\theta_{j}} 
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}} 
=
$$

Here we use the fact that the constraints hold, and thus differentiating both side we have

$$
\begin{array}{rl}
g_{i}(x^\star(\theta), \theta) &=0 \\
\frac{\partial}{\partial \theta_{j}} 
g_{i}(x^\star(\theta), \theta) &= \frac{\partial}{\partial \theta_{j}} 0 \\
\sum_{n=1}^{N} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial x_{n}}\frac{\partial x_{n}(\theta)}{ \partial\theta_{j}} 
+ \frac{\partial g_i(x^\star(\theta), \theta)}{\partial \theta_{j}} &= 0
\end{array}
$$

Continuing the main line

$$
=
- \sum_{i=1}^{I}\lambda_{i} \frac{\partial g_i(x^\star(\theta), \theta)}{\partial \theta_j}
+\frac{\partial f(x^\star(\theta), \theta) }{\partial \theta_{j}}
=\\=
\frac{\partial\mathcal{L}(x^\star(\theta), \lambda^\star(\theta), \theta)}{\partial{\theta_{j}}}
$$

$\blacksquare$
````



```{note}
What about the inequality constraints? 

Well, if the solution is interior and none of the constrains are binding, the unconstrained version of the envelope theorem applies.
If any of the constrains are binding, their combination can be represented as a set of equality constraint, and the constrained version of the envelope theorem applies.
*Care needs to be taken* to avoid the changes in the parameter that lead to a switch in the binding constraints. Such points are most likely non-differentiable, and the envelope theorem does not apply there at all!
```




```{admonition} Example
:class: tip

Back to the log utility case
%
$$ 
u(x_1, x_2) = \alpha \log(x_1) + \beta \log(x_2) \to \max_{x_1, x_2} \\
\text{ subject to} \\
p_1 x_1 + p_2 x_2 = m
$$
%
The Lagrangain is
%
$$ 
\mathcal{L}(x_1,x_2,\lambda) = \alpha \log(x_1) + \beta \log(x_2)
- \lambda(p_1 x_1 + p_2 x_2 - m)
$$
%
Solution is 
%
$$
x_1^\star(p_1,p_2,m) = \frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} \\
x_2^\star(p_1,p_2,m) = \frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2} \\
\lambda^\star(p_1,p_2,m) = \frac{\alpha + \beta}{m}
$$
%

Value function is
%
$$
V(p_1,p_2,m) = \alpha \log\left(
\frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} 
\right) + \beta \log\left(
\frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2}
\right)
$$
%

We can verify the Envelope theorem by noting that direct differentiation gives
%
$$
\frac{\partial V}{\partial p_1} = 
-\frac{\alpha}{p_1}, \quad
\frac{\partial V}{\partial p_2} = 
-\frac{\beta}{p_2}, \quad
\frac{\partial V}{\partial m} = 
\frac{\alpha+\beta}{m}
$$
%
And applying the envelope theorem we have
%
$$
\frac{\partial V}{\partial p_1} = 
\frac{\partial \mathcal{L}}{\partial p_1}(x_1^\star,x_2^\star,\lambda^\star) =
-\lambda^\star x_1^\star = -\frac{\alpha}{p_1}
$$
%
$$
\frac{\partial V}{\partial p_2} = 
\frac{\partial \mathcal{L}}{\partial p_2}(x_1^\star,x_2^\star,\lambda^\star) =
-\lambda^\star x_2^\star = -\frac{\beta}{p_2}
$$
%
$$
\frac{\partial V}{\partial m} = 
\frac{\partial \mathcal{L}}{\partial m}(x_1^\star,x_2^\star,\lambda^\star) =
\lambda^\star = \frac{\alpha+\beta}{m}
$$

```


## Lagrange multipliers as shadow prices

In the equality constrained optimization problem, the Lagrange multiplier $\lambda_i$ can be interpreted as the shadow price of the constraint $g_i(x,\theta) = a$, i.e. the change in the value function resulting from a change in parameter $a$, in other words *relaxing* the constraint $g_i(x,\theta) = a$.

**Exercise:** prove this statement



## References and reading

```{dropdown} References
- {cite:ps}`simon1994`: 19.1, 19.2
- {cite:ps}`sundaram1996`: chapter 9, 5.2.3
```

```{dropdown} Further reading and self-learning

- [Mark Walker's video lecture on envelope theorem](https://youtu.be/DiRwRERgglw?si=Wzn-mJirHjbT1l4p)

```
