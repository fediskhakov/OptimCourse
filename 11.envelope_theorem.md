---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---


# üìñ Envelope theorem

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>


Next step after continuity -- differentiability of the value function $V(\theta)$ and the marginal effect of relaxing the constraint (*derivatives!*)

Let's start with an unconstrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

```{admonition} Envelope theorem for unconstrained problems
:class: important
:name: envelope-unconstrained

Let $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ be a differentiable function, and $x^\star(\theta)$ be the maximizer of $f(x,\theta)$ for every $\theta$.
Suppose that $x^\star(\theta)$ is differentiable function itself.
Then the value function of the problem $V(\theta) = f\big(x^\star(\theta),\theta)$ is differentiable w.r.t. $\theta$ and
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial f}{\partial \theta_j} \big(x^\star(\theta),\theta\big),
\forall j
$$

```

In other words, the marginal changes in the value function are given by the partial derivative of the objective function evaluated at the maximizer.


````{admonition} Proof
:class: dropdown

**Exercise**

````

```{note}
When $K=1$, so that $\theta$ is a scalar, the envelope theorem can be written as
%
$$
\frac{d f\big(x^\star(\theta),\theta)}{d \theta} =
\frac{\partial f}{\partial \theta} \big(x^\star(\theta),\theta\big)
$$
% 
so that the meaning is carried only by the derivative sign change
```

```{admonition} Example
:class: tip

Consider $f(x,a) = -x^2 +2ax +4a^2 \to \max_x$. \
What is the (approximate) effect of a unit increase in $a$ on the attained maximum?

FOC: $-2x+2a=0$, giving $x^\star(a) = a$.

So, $V(a) = f(a,a) = 5a^2$, and $V'(a)=10a$. The value increases at a rate of $10a$ per unit increase in $a$.

Using the envelope theorem, we could go directly to
%
$$
V'(a) = \frac{\partial f}{\partial a} (x^\star(a),a) = 2x + 8a \Big|_{x=a} = 10a
$$

```

```{admonition} Envelope theorem for constrained problems
:class: important
:name: envelope-constrained

Consider an equality constrained optimization problem 
%
$$
V(\theta) 
= \max_{x} f(x,\theta)
\\
\text {subject to}
\\
g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}\\
$$
%
where:
- $f(x,\theta) \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$ is an objective function
- $x \in \mathbb{R}^N$ are decision/choice variables
- $\theta \in \mathbb{R}^K$ are parameters
- $g_i(x,\theta) = 0, \; i\in\{1,\dots,I\}$ where $g_i \colon \mathbb{R}^N \times \mathbb{R}^K \to \mathbb{R}$, are equality constraints
- $V(\theta) \colon \mathbb{R}^K \to \mathbb{R}$ is a value function

Assume that the maximizer correspondence $\mathcal{D}^\star(\theta) = \mathrm{arg}\max f(x,\theta)$ is *single-valued* and can be represented by the function $x^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^N$, with the corresponding Lagrange multipliers $\lambda^\star(\theta) \colon \mathbb{R}^K \to \mathbb{R}^K$.

Assume that both $x^\star(\theta)$ and $\lambda^\star(\theta)$ are differentiable, and that the constraint qualification assumption holds.
Then 
%
$$
\frac{\partial V}{\partial \theta_j} =
\frac{\partial \mathcal{L}}{\partial \theta_j} \big(x^\star(\theta),\lambda^\star(\theta),\theta\big),
\forall j
$$
%
where $\mathcal{L}(x,\lambda,\theta)$ is the Lagrangian of the problem.
```

````{admonition} Proof
:class: dropdown

**Exercise**

````

```{note}
What about the inequality constraints? 

Well, if the solution is interior and none of the constrains are binding, the unconstrained version of the envelope theorem applies.
If any of the constrains are binding, their combination can be represented as a set of equality constraint, and the constrained version of the envelope theorem applies.
*Care needs to be taken* to avoid the changes in the parameter that lead to a switch in the binding constraints. Such points are most likely non-differentiable, and the envelope theorem does not apply there at all!
```




```{admonition} Example
:class: tip

Back to the log utility case
%
$$ 
u(x_1, x_2) = \alpha \log(x_1) + \beta \log(x_2) \to \max_{x_1, x_2} \\
\text{ subject to} \\
p_1 x_1 + p_2 x_2 = m
$$
%
The Lagrangain is
%
$$ 
\mathcal{L}(x_1,x_2,\lambda) = \alpha \log(x_1) + \beta \log(x_2)
- \lambda(p_1 x_1 + p_2 x_2 - m)
$$
%
Solution is 
%
$$
x_1^\star(p_1,p_2,m) = \frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} \\
x_2^\star(p_1,p_2,m) = \frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2} \\
\lambda^\star(p_1,p_2,m) = \frac{\alpha + \beta}{m}
$$
%

Value function is
%
$$
V(p_1,p_2,m) = \alpha \log\left(
\frac{\alpha}{\alpha + \beta} \cdot \frac{m}{p_1} 
\right) + \beta \log\left(
\frac{\beta}{\alpha+\beta} \cdot \frac{m}{p_2}
\right)
$$
%

```

We can verify the Envelope theorem by noting that direct differentiation gives
%
$$
\frac{\partial V}{\partial p_1} = 
-\frac{\alpha}{p_1}, \quad
\frac{\partial V}{\partial p_2} = 
-\frac{\beta}{p_2}, \quad
\frac{\partial V}{\partial m} = 
\frac{\alpha+\beta}{m}
$$
%
And applying the envelope theorem we have
%
$$
\frac{\partial V}{\partial p_1} = 
\frac{\partial \mathcal{L}}{\partial p_1}(x_1^\star,x_2^\star,\lambda^\star) =
-\lambda^\star x_1^\star = -\frac{\alpha}{p_1}
$$
%
$$
\frac{\partial V}{\partial p_2} = 
\frac{\partial \mathcal{L}}{\partial p_2}(x_1^\star,x_2^\star,\lambda^\star) =
-\lambda^\star x_2^\star = -\frac{\beta}{p_2}
$$
%
$$
\frac{\partial V}{\partial m} = 
\frac{\partial \mathcal{L}}{\partial m}(x_1^\star,x_2^\star,\lambda^\star) =
\lambda^\star = \frac{\alpha+\beta}{m}
$$


## Lagrange multiplyers as shadow prices

In the equality constrained optimization problem, the Lagrange multiplier $\lambda_i$ can be interpreted as the shadow price of the constraint $g_i(x,\theta) = a$, i.e. the change in the value function resulting from a change in parameter $a$, in other words *relaxing* the constraint $g_i(x,\theta) = a$.

**Exercise:** prove this statement



## References and reading

```{dropdown} References
- {cite:ps}`simon1994`: 19.1, 19.2
- {cite:ps}`sundaram1996`: chapter 9, 5.2.3
```

```{dropdown} Further reading and self-learning

- [Mark Walker's video lecture on envelope theorem](https://youtu.be/DiRwRERgglw?si=Wzn-mJirHjbT1l4p)

```
